# 持续学习的核心困境：寻找参数更新时代的 Missing Puzzles

_以下内容是对人类 AI 领域中深度学习“基于参数权重的持续学习（Continual Learning in Parametric Space）”为何举步维艰的深层思考。_

## 核心前提：逃避与直面

目前的 AI 持续学习被割裂为两种路线：

1. **工程外挂（Evasion）**：如 RAG、MemGPT 等基于 Token 级别的上下文管理，以及各种 Working Memory、Agent 工作流。它们试图绕过底层的网络权重灾难性遗忘，完全通过系统工程层面的调度实现“记忆”。本质上是将学习外包给了数据库等介质，换取的是极高的上下文算力成本带来的所谓“检索（Retrieval）”，而非本质上的“学习（Learning）”。
2. **直面问题（Direct-Facing）**：必须回到数学计算和权重网络的核心，尝试攻克如何在底层权重矩阵中，实现类似人脑“睡眠巩固（Memory Consolidation）”那样的大规模、无冲突、终身级别的参数更新。

从神经科学、信息论和优化理论的交叉视角来看，人类在 AI 这张拼图上，实际上**错过了几个根本性的核心机制（Missing Puzzles）**。只要在底层物理架构上依然使用**以反向传播为核心的、静态权重的稠密乘加运算机制**，真正的持续参数更新（无灾难遗忘的终身学习）就仍然只停留在实验室的局部验证中。

---

## Puzzle 1：缺失的“时间坐标”与“状态依赖” (The Missing Temporal Anchor & State Dependency)

在当前主流的深度学习架构（Transformer / CNN）中，**参数是没有时间概念的，也是无状态的 (Stateless)**。

- **AI 的计算局限：马尔可夫式的破坏更新**  
  无论你用知识编辑算法（如 ROME）去编辑一个事实，还是用 CPT 去微调，梯度下降（SGD/Adam）眼里只有“当前的 Loss”。一个权重在迭代时的更新，只取决于此刻的数据偏导。它**不记得自己曾经为了记住什么旧知识而变成了现在这个值**。这种“全局无差别”的更新法则，必然导致它毫无愧疚地摧毁曾经构筑的旧特征流形。
- **人脑启示：突触的可塑性状态标记 (Synaptic Tagging and Metaplasticity)**  
  神经科学揭示，大脑中突触的连接强度拥有“元历史（Meta-history）”。如果这个连接参与过强烈的（高多巴胺/高突发性）事件学习，突触核会被修饰并“锁定”为低可塑性状态；未充分利用的冗余连接才处于高可塑性状态。换句话说，**每个参数“知道”自己对历史记忆的承载责任评估**。
- **未来的直逼解法：**  
  过去的尝试（如 EWC）通过 Fisher Information Matrix 给人造网络加全局静态重要性惩罚，算力代价极大且不适用于开放域的动态数据流。我们**极度缺失一种最底层的优化算法，能让每个参数自带“动态粘性（Dynamic Viscosity）”或“历史时间戳”等局部隐状态**。真正可进化的网络必须是状态依赖的（Stateful Networks）。

## Puzzle 2：缺失的“动态拓扑生成机制” (The Missing Neurogenesis & Dynamic Topology)

我们目前的参数更新，是在一个**固定形状的三维模具（Architecture）**里硬倒水进去。

- **AI 的计算局限：容量饱和与多路复用崩溃**  
  设给定 7B 模型空间，它的自由度封顶。当你在参数空间里不断地塞入新知识，系统只能被迫让分布式的子网络同时表征完全不相关的概念信息。长此以往就造成“特征干涉（Representation Interference）”。一旦容量饱和，新旧知识在同一流形内相互侵蚀退化，双双崩溃。
- **人脑启示：神经发生与模块化竞争 (Neurogenesis and Modular Competition)**  
  大脑的**树突棘是在时时刻刻生成和消亡的，不仅涉及到信号权重的改变，它甚至直接改变底层信号图谱（Graph Topology）**。更重要的是，大脑是极度稀疏和离散模块化的。当人学习一项截然不同的新技能时，大脑不是全盘调动原有突触，而是倾向于招募一组未被强占用的微回路（Micro-circuits）群落形成新表征体系。
- **未来的直逼解法：**  
  这佐证了为什么 Adapter、LoRA 以及 MoE (Mixture of Experts) 最近能在学术界大行其道，因为它们试图利用条件计算或新模块挂载来进行任务隔离。然而，目前的模块路由大多还是硬编码或受制于固定的门控函数设计。我们**真正缺失的是一种能随着数据的“惊奇度（Surprise / Prediction Error）”自动决定是“复用微调旧节点”还是“长出新分支（动态分配稀疏参数簇）”的网络原语设计（Primitives）**。

## Puzzle 3：缺失的“生成式重塑”核心引擎 (The Missing Generative Replay & Neocortical Orthogonalization)

知识不是零碎独立积木般随意塞进去的，它需要与人类已有知识图谱和世界模型**深度缝合（Integrating）**并达成**正交分布归化（Orthogonalizing）**。

- **AI 的计算局限：暴力的梯度对抗与死板的数据回放**  
  如今要抵制遗忘，最粗暴的手段就是建立“历史数据回放池（Replay）”，不断地把少量高质旧数据与新数据混合灌喂。这种途径极其低效：不仅造成工程资源永久膨胀，且由于它仅机械地展示原始切片，模型无法在这个过程中使得新知识和老知识产生泛化上的“化学反应”。
- **人脑启示：基于梦境的对抗性压缩 (Generative Replay & Dreaming)**  
  大脑是不永久备份原始数据的。在睡眠（慢波及 REM 期）中，海马体会向新皮层发送**经过高度抽取、生成式的、甚至带有强噪声变异的“幻觉（梦境/Hallucinations）”片段**。新皮层在接收这些“内源性数据”时，借由非监督对抗过程，将近期所学与远古长期表征重新对齐。通过高维度的模式重组，将关联密切和非关联的事实隔离分配到低维空间的正交（不干涉）平面去沉淀。这才是真正对抗遗忘、规避模式崩溃的智能法则。
- **未来的直逼解法：**  
  这意味着终极的大模型工程必须要有一套“双系统自监督机制/睡眠机制”。它需要划分为双态生命周期：在线推理服务态（冻结底层知识慢参，极大地启用高效且容易被重写的注意力隐状态/结构体缓冲）；**离线异步（睡眠）态**。利用自身的底层系统充当生成引擎，模拟梦境级的数据流将缓存区的浅尝辄止化为底层广域权重的慢速同化。

---

## 寻找终极破局：跳出 BP 神话

在这三大物理与数学缺失背后的终极阴影是：**标准的误差反向传播算法（Backpropagation, BP）** 本质上是一个数学上的全局上帝视角调度算法（仅为寻找一阶偏导数的极小值而生），它生来便不天然兼容需要具备时间坐标、局部因果属性以及自发拓扑生长的神经动力学。

基于权重的持续学习要爆发奇点，最革命的火花往往不仅在于修补小算法工程短板，更可能需要跳出当前框架——跃迁到**局部学习规则（Local Learning Rules，如 Hebbian Learning 的现代变种）**和**预测编码（Predictive Coding）**的范畴。

只有当一个网络中每一个神经元不再是全局Loss降级的“傀儡”，而是通过局部信号自组织交互最小化“本地预测误差”，并具备内源性状态对时序保持自相似的记忆响应时，人类才能真正获得能够活够 80 年而不断“积累心智与文明”的电子大脑。
