# CoMem — Wu et al. 2025

- **Paper**: Wu et al. (2025)
- **Source**: arXiv 2512.13564v2 (Memory in the Age of AI Agents)
- **Category**: Latent Memory — Generate — Multimodal
- **Task**: Multimodal QA

## Core Mechanism

CoMem compresses vision-language inputs into fixed-length tokens through Q-Former, achieving dense continuous memory.

- Uses the Q-Former architecture to compress multimodal (vision-language) inputs into a fixed-length sequence of tokens.
- Generates dense continuous memory, preserving the joint semantics of vision and language.
- Supports plug-and-play unlimited context length.
- The compressed fixed-length representation ensures that memory capacity is not limited by the original input length.
