# Swallow

Source: Continual Learning of Large Language Model

Core Mechanism:
Cross-Lingual Continual Pre-Training (CPT). It expands the model's vocabulary (Vocabulary Expansion) and continuously injects high-quality target language (Japanese) corpora for training. As the number of tokens increases, the target language performance improves steadily.
