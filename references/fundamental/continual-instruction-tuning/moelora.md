# MoELoRA

来源: Continual Learning of Large Language Models: A Comprehensive Survey

核心机制:
MoE-based Parameter-Efficient Fine-Tuning 用于多模态模型的 Continual Learning。引入 Mixture-of-Experts 结合 LoRA 进行微调，为处理各种特征带来细粒度的任务或模态间无干扰的持续学习效果。
