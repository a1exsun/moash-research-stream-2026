# Meta SMF (Sparse Memory Finetuning) — Jessy Lin et al.

**论文：** Jessy Lin et al. (2025)
**来源：** 机器之心Pro Week 07（提及，详见Pro 2025 Week 46）
**类别：** LLM Memory — 稀疏记忆池

---

## 问题

全量微调导致灾难性遗忘（遗忘率89%）。

## 方法

在1.3B基础模型上附加1B参数的稀疏记忆池，每次只更新被高度激活的记忆槽位。冻结所有密集层，仅更新记忆池。

## 结果

遗忘率从全量微调的89%降至11%。

## 局限

- 冻结所有密集层，模型核心推理能力无法改进
- 仅测试事实记忆（TriviaQA），未涉及能力/技能层面的持续学习
- 本质上是外挂可学习向量，类似可训练的RAG
