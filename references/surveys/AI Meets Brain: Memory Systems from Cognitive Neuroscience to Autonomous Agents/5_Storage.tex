\section{Memory Storage}
\label{sec:Storage}
Memory storage is a core function of cognitive systems, determining how information is retained and organized. 
This section systematically examines two key dimensions of memory storage, namely storage location and storage format, from the perspectives of both cognitive neuroscience (\S\ref{ssec:Memory Storage in Cognitive Neuroscience}) and agents (\S\ref{ssec:Memory Storage in Agents}). 
Comparing the similarities and differences between these two types of systems provides deeper insights into the fundamental mechanisms of memory storage.

\subsection{Memory Storage in Cognitive Neuroscience}
\label{ssec:Memory Storage in Cognitive Neuroscience}
Memory storage in the human brain is not a unitary process but rather a dynamic interplay between multiple systems operating at different timescales. 
Understanding how the brain retains information requires distinguishing between short-term memory, which holds information temporarily for immediate use, and long-term memory, which preserves experiences and knowledge over extended periods.
Although these two systems differ in their temporal scope and neural substrates, they are interconnected with respect to memory storage.
Short-term representations can be consolidated into long-term traces, and long-term memories can be reactivated into short-term working states. 
In the following sections, we examine the neural architecture and representational formats underlying short-term memory (\S\ref{ssec:Short-term Memory Storage}) and long-term memory (\S\ref{sssec:Long-term Memory Storage}), drawing on converging evidence from neuroimaging, electrophysiology, and lesion studies in humans.

\subsubsection{Short-term Memory Storage}
\label{ssec:Short-term Memory Storage}
Short-term memory, as a temporary information storage system, primarily processes a small amount of information within a limited time window. 
To deeply understand how short-term memory supports flexible cognition, however, it is necessary to consider where in the brain short-term information is stored and in what form it is represented and maintained. 
A large body of neuroimaging and electrophysiological evidence has begun to outline the neural basis of short-term memory. 
On the one hand, it appears to rely on a distributed storage scheme spanning sensory cortices and frontoparietal network (i.e, sensory–frontoparietal network). 
On the other hand, at the cellular and circuit levels, information can be held via mechanisms such as persistent neural firing and activity-silent synaptic connection states. 
In this section, we briefly review recent work on short-term memory along two dimensions, namely its storage location and its underlying storage format.

\textbf{Storage Location.} 
Human neuroimaging studies consistently show that short-term memory is supported by a distributed brain system. 
After the stimulus disappears, information about the remembered items remains retained in the sensory cortices, posterior parietal cortex, and prefrontal regions. 
During the maintenance period, these areas display persistent activity or decodable patterns that match the memory content. 
This means short-term memory is not stored in one single region, instead, it relies on sensory–frontoparietal network.
Specifically, perceptual cortices tend to retain fine-grained sensory details, while the prefrontal cortex mainly regulates this storage process by setting priorities, allocating limited memory resources across items~\cite{brain_STM_storage1}, and recoding information into formats that better meet upcoming behavioral demands~\cite{brain_STM_storage2}.
In addition, frontoparietal network also supports cross-modal representations, allowing information from different sensory channels to be linked and manipulated in a shared representational space~\cite{brain_STM_storage3}.

\textbf{Storage Format.} 
At the cellular and circuit levels, short-term memory can be maintained by more than one format, which include: Persistent activity and Synaptic connection weights.

\begin{itemize}
	\item \textbf{Persistent activity} refers to a classic proposal in which neurons that encode the memory maintain a high level of firing activity even after the external stimuli are removed. 
    Single-neuron recordings in epilepsy patients provide direct human evidence for this idea, showing that neurons in the medial frontal and medial temporal lobes remain active after stimulus offset and their firing tracks the remembered content~\cite{brain_STM_storage4}.
    However, persistent activity is energy-demanding and may be vulnerable to interference.
	\item \textbf{Synaptic connection weights} refer to an alternative format for memory storage, where information can be temporarily maintained without sustained neural activity. 
    In this framework, population firing of neurons can drop back to baseline while the memory enters an “activity-silent” state that standard recordings cannot easily decode~\cite{brain_STM_storage5}. 
\end{itemize}

A more widely accepted view today is that both of the formats coexist, with the brain switching between them. 
High-priority items within the focus of attention are often kept in an active, persistently firing state, while items outside attention but still potentially useful can be stored silently and reactivated when needed~\cite{brain_STM_storage6}. 
To support this dual-mechanism hypothesis, \citet{brain_STM_storage7} showed that electroencephalography (EEG) can decode memory content during maintenance, but not during the inter-trial interval, demonstrating how the representation becomes latent and later re-emerges.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/storage_long_short.pdf}
    \caption{Overview of memory storage mechanisms in cognitive neuroscience, including storage locations and storage formats of short- and long-term memory.}
    \label{fig:storage_long_short}
\end{figure*}

\subsubsection{Long-term Memory Storage}
\label{sssec:Long-term Memory Storage}
Compared with the temporary maintenance of information by short-term memory, the characteristic of long-term memory that it can preserve information for a long time implies that it has a relatively more complex neural mechanism. 
Rather than preserving experiences in their original details, the brain gradually transforms and reorganizes them across hippocampal–neocortical systems, giving rise to more structured and abstract representations. 
In this section, we will introduce where in the brain long-term memories are stored, and in what structural form they are represented.
Specifically, we first summarize evidence on the division of labor between the hippocampus and neocortex in long-term memory storage, and then discuss how experiences are organized into structured units, such as event-based unit and cognitive map.

\textbf{Storage Location.}
Long-term memory storage depends on the coordinated activity of two key brain regions, the hippocampus and the neocortex.
When new information is acquired, it is initially encoded and maintained in distributed neocortical regions, whose signals then converge in the hippocampus for integrated processing. 
Rather than serving as a storage warehouse, the hippocampus functions as an index that reactivates these distributed memory traces.
Consequently, newly formed memories depend heavily on hippocampal support, but through systems consolidation, they gradually become sustained by neocortical networks~\cite{brain_LTM_storage1}.
Multiple findings support this view. 
For example, \citet{brain_LTM_storage2} demonstrated that hippocampal activity during encoding predicts how precisely details will be stored, but later retrieval success depended more on prefrontal activity and the strength of neocortical representations. 
In another study, \citet{brain_LTM_storage3} found that rehearsal strengthened memories mainly in neocortical and hippocampal–neocortical interaction regions, rather than in hippocampal activity alone. 
Moreover, when people experience events with overlapping contexts, the hippocampus and relevant neocortical regions show coordinated reactivation during rest. 
This replay helps write cross-event structures into medial prefrontal cortex (mPFC) representations~\cite{brain_LTM_storage4}.These cross-event structures can then serve as contextual knowledge or priors that shape the encoding of information in short-term memory~\cite{brain_class6,brain_class7}.

\textbf{Storage Format.}
It is an obvious fact that memories cannot be preserved in their original details. 
During consolidation, the hippocampus integrates information and develops more abstract, structured representations, allowing memories to be stored in complex forms rather than as raw sensory data.
Here we focus on two such forms: event-based units and cognitive maps.

\begin{itemize}
	\item \textbf{Event-based unit} refers to the discrete memory representations that result from the brain's segmentation of continuous, rich everyday experience into distinct episodes, allowing us to remember life as a series of separate events rather than an unbroken stream. 
    An fMRI study showed strong hippocampal responses at boundaries between experimentally defined events, suggesting that the brain automatically detects event transitions~\cite{brain_LTM_storage5}.
    However, this segmentation does not produce entirely isolated memories. 
    Even when plots, scenes, or locations change, events that occur close in time are often recalled as a coherent stream. 
    This continuity is facilitated by hippocampal-neocortical replay at the end of an event, which reinstates what just happened and helps integrate information across boundaries~\cite{brain_LTM_storage6}.
    Beyond temporal integration, a complete event also appears to have a holistic representation. 
    Although its constituent elements are distributed across neocortical areas, the hippocampus can bind them into a unified memory trace~\cite{brain_LTM_storage7}.
    Consistent with this, human hippocampus contains event-specific neurons that fire strongly during encoding and retrieval of particular episodes. 
    Their activity reflects the event as a whole rather than isolated features~\cite{brain_LTM_storage8}.
	\item \textbf{Cognitive map} refers to the mental representations built by the hippocampus–entorhinal circuit, which not only tracks locations and routes during physical navigation but also serves a broader function in organizing abstract knowledge, concepts, and experiences. 
    Within such maps, past experiences are represented as points in a multidimensional space.
    The construction of cognitive maps involves several key processes. 
    First, the brain establishes an internal coordinate system for abstract concepts that remarkably resembles the one it uses for physical space.
    For instance, during memory for two-dimensional conceptual spaces, entorhinal cortex and ventromedial prefrontal cortex (vmPFC) show hexagonally symmetric signals, similar to grid-cell coding in navigation~\cite{brain_LTM_storage9}. 
    Related coordinate-like representations have also been found for social relationships~\cite{brain_LTM_storage10}. 
    Second, the hippocampus–entorhinal system can encode distances between knowledge units. 
    Neural signal strength reflects relational or path-like distance in the cognitive space~\cite{brain_LTM_storage11}. 
    This principle extends to event memories, which can be arranged into temporal maps where distances correspond to transition probabilities over time~\cite{brain_LTM_storage12}.   
    Third, through consolidation, these maps are gradually transferred to and maintained in neocortical regions. 
    It has been recorded that medial prefrontal cortex stores learned abstract relations~\cite{brain_LTM_storage13}, while other frontal and cingulate regions encode social and functional attributes that organize long-term knowledge~\cite{brain_LTM_storage14}.
\end{itemize}

\subsection{Memory Storage in Agents}
\label{ssec:Memory Storage in Agents}
Unlike the human brain with its complex structures and rich biological information, memory storage in agents is fundamentally based on natural language symbols. Therefore, rather than discussing storage by memory type, we directly introduce two key dimensions of the storage: 
(1) Storage location (\S\ref{sssec:Storage Location}) 
and (2) Storage format (\S\ref{sssec:Storage Format}).

\subsubsection{Storage Location}
\label{sssec:Storage Location}

From the perspective of storage mechanisms, memory in agents can be organized in two primary ways. 
The context window serves as a container that dynamically folds and updates information within a single reasoning trajectory. 
In contrast, memory banks function as external repositories that persistently store accumulated information. 

\textbf{Context Window.}
The context window can serve as a container for memory storage.
Information placed within the context, including user inputs, tool invocation result, and intermediate generation steps, can be transformed into inside-trail memory that the agent directly accesses during reasoning. 
This process is defined as memory folding~\cite{method_mem1,method_deepagent,method_agentfold,method_foldgrpo,method_resum,method_memory_as_action,dy_memory_mem,method_memtool}, which is triggered during the reasoning process, enabling the agent to dynamically update its state of knowledge within its trajectory.
For example, \citet{method_resum} proposed periodically calling a summarization tool to compress accumulated interaction history into a structured summary, allowing the agent to resume reasoning from the compressed state and thus mitigating truncation issues caused by context window limitation.
Building upon this concept, some works~\cite{method_memory_as_action,method_deepagent} have further proposed active folding strategies. 
For example, \citet{method_deepagent} proposed to dynamically determine whether to initiate memory folding during reasoning by generating a special trigger token, thereby providing greater flexibility and adaptability for memory management.


\textbf{Memory Bank.}
Beyond context-based storage, agent systems commonly incorporate external memory modules as dedicated repositories.
Unlike the context window, such memory module~\cite{method_agentkb,method_rmm,method_aios,method_reasoningbank,method_reactree,method_memory_sharing} possess theoretically unbounded capacity and can persistently retain the agent's cross-trail memory, which includes experiential data and domain knowledge accumulated across multiple sessions and tasks.
This type of memory storage mode is typically triggered at the end of a trajectory, granting memory reusability, ensuring that previously acquired useful information remains accessible and retrievable even after conversation termination or system restart.
Specifically, some memory banks are designed to persistently store user preferences, knowledge, and conversation history. 
For example, \citet{method_rmm} introduced a framework that organize the memory bank in a topic-based manner, enabling cross-session tracking of users’ health conditions, allergies, and personal preferences for continuous personalized service.
Other memory banks focus on distilling reusable strategies and reasoning patterns from agents’ historical interactions. 
For instance, \citet{method_reasoningbank} suggest to extract high-level reasoning strategies from both successful and failed experiences, storing abstracted decision principles rather than raw trajectories.
Building upon these approaches, cross-agent shared memory banks transcend the boundaries of individual agents, enabling experience sharing across different frameworks. 
\citet{method_agentkb} propose plug-and-play knowledge sharing by abstracting execution trajectories from various agent systems into unified structured experience units, accessible through lightweight APIs. 
This design addresses the isolation problem in current agent systems, allowing solutions discovered in one framework to be directly reused by others and avoiding redundant trial and error.

\subsubsection{Storage Format}
\label{sssec:Storage Format}
Memory representations in agents are typically divided into four categories, including text stored in conventional natural language, graphs that emphasize preserving structured relationships, parametric memory internalized in model weights, and latent representations in vector form.

\textbf{Text.}
Natural language text is the most common storage format for agent memory. Memories are stored as either raw text or summarized text, encompassing both experiences and information~\cite{method_ace,method_hiagent,method_mms,method_apc,method_eapo,method_maple,method_avatar}. 
This format offers high interpretability, ease of manipulation, and direct compatibility with language model architectures. 
Although stored in textual form, conversational or task-related processes are typically not preserved verbatim but undergo a certain degree of abstraction and summarization. 
For example, \citet{method_hiagent} proposed decomposing tasks into multiple sub-goals, with each sub-goal completion triggering a consolidation of multiple steps into a summary that is then stored in the memory container.
In contrast, \citet{method_ace} argued that excessive abstraction and compression of memory information causes agents to suffer from brevity bias, leading to degraded performance in specialized domains. 
They therefore proposed the playbook memory storage mechanism, which minimizes memory compression and preserves detailed information to the greatest extent possible.
Overall, text-format memory facilitates retrieval and combination while fully leveraging the language processing capabilities of large language models for reasoning.

\textbf{Graph.}
Graph-based memory storage organizes memory into a structured network composed of entities and relationships~\cite{method_fes,method_sgmem,method_flexibly,method_from_isolated,method_arigraph,method_towards_lifelong,method_crafting}. 
In this format, experiences or information are decomposed into nodes (e.g., concepts, objects, events, or steps) and edges (e.g., relationships). 
The graph structure excels at supporting complex reasoning tasks by allowing the system to traverse edges between nodes and concatenate multiple episodes into the most effective memory as experience.
For instance, \citet{method_mem0} constructed a relational graph to explicitly model temporal and logical relationships between entities, demonstrating strong performance in tasks requiring multi-step reasoning by effectively tracking event sequences and character interactions. 
\citet{method_gmemory} employed a three-tier graph hierarchy to manage the lengthy interaction history of multi-agent systems, retrieving high-level, generalizable insights and fine-grained collaboration trajectories through bi-directional memory traversal.
Furthermore, the graph structure inherently supports relationship extraction and pattern discovery, enabling the identification of implicit connections between nodes and thereby assisting complex logical information queries. 
For example, \citet{method_dsmart} constructed the dialogue history into a knowledge graph and executes multi-step graph traversal searches, making the reasoning process traceable and faithful to the dialogic facts. 
\citet{agent_memory_see} proposed an entity-centric multimodal graph to store memories of faces, voices, and text, connecting nodes across different modalities through identity equivalence detection to maintain long-term consistency.
Overall, graph-structured memory enhances reasoning performance and consistency in complex scenarios through its inherent relational representation capabilities.

\textbf{Parameters.}
Parametric storage embeds memories directly within model weights, integrating experience and knowledge into the neural architecture~\cite{method_pretraining_context}. 
This approach solidifies information through persistent modifications of connection strengths, thereby emulating synaptic plasticity in biological brains. 
Its primary advantages include exceptional access efficiency and deep knowledge integration, as memory is automatically activated during forward propagation, which eliminates retrieval latency and cross-modal alignment costs.
This internalization of experience into neural weights is primarily driven by imitation and reinforcement learning. In terms of imitation learning, agents utilize supervised fine-tuning to shape initial parametric representations. 
For instance, \citet{method_memverse} employed distillation to solidify long-term knowledge within weights, while \citet{method_seagent} distilled expert trajectories to transform operational skills into parametric instincts. Conversely, reinforcement learning achieves experience-driven learning. Under this paradigm, agents treat sampled trajectories as episodic memories. Specifically, \citet{method_digirl, method_webrl, method_webagenrr1} successfully internalized explored strategies into parameters by extracting performance advantages from these samples. 
Furthermore, \citet{method_early_experience} introduced an intermediate paradigm that enables agents to learn from self-interaction without explicit rewards by using future states of exploratory actions as supervision signals to internalize experience.
In summary, by internalizing experiences into model weights, parametric storage eliminates retrieval latency and enhances decision stability, thereby enabling rapid and consistent agent responses.


\textbf{Latent Representation.}
Beyond token-space memory structures that explicitly represent information and parametric memory, several studies~\cite{method_cam, method_r3mem} have explored latent memory, which stores information as high-dimensional vectors in embedding space. 
Compared to token-level memory, latent memory offers several key advantages:
(1) Efficient compression: high-dimensional continuous vectors can encode information more compactly than discrete tokens, thereby reducing storage and computational overhead~\cite{cog_memory_memllm,method_rf}. 
(2) End-to-end trainability: as continuous representations, latent memories can directly participate in gradient-based optimization, enabling updates and refinements during training~\cite{method_em2,method_icae}.
(3) Alignment with human cognition: as noted in~\cite{method_nature,method_coconut}, human reasoning relies on integrated representations that transcend discrete symbols, latent space representations effectively embody this principle.
Representative methods include MemoryLLM~\cite{cog_memory_memllm} and M+~\cite{method_m+}, which allocated a fixed set of latent vectors as “internal memory” at each layer of LLMs. 
As the context evolves, these latent memories are iteratively retrieved and updated, then concatenated with the hidden states at each layer and integrated into the model’s forward computation, thereby enabling the retention and utilization of information from long documents.
Building upon this, \citet{method_memgen} further extended this approach to the domain of agent memory. 
It fed the hidden state of the agent's current action into a trigger module to determine whether latent memory needs to be invoked. 
When triggered, the agent leveraged the current action's hidden state to retrieve relevant latent memories from Weaver's parameters, dynamically steering the model's subsequent actions and enabling continuous self-evolving behavior.
