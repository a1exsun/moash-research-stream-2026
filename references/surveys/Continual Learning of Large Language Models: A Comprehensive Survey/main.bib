
% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

%Entries
% Surveys
@article{mai2022online,
    title = {Online continual learning in image classification: An empirical survey},
    journal = {Neurocomputing},
    volume = {469},
    pages = {28-51},
    year = {2022},
    issn = {0925-2312},
    doi = {https://doi.org/10.1016/j.neucom.2021.10.021},
    url = {https://www.sciencedirect.com/science/article/pii/S0925231221014995},
    author = {Zheda Mai and Ruiwen Li and Jihwan Jeong and David Quispe and Hyunwoo Kim and Scott Sanner}
}
@article{de2021continual,
  title={A continual learning survey: Defying forgetting in classification tasks},
  author={De Lange, Matthias and Aljundi, Rahaf and Masana, Marc and Parisot, Sarah and Jia, Xu and Leonardis, Ale{\v{s}} and Slabaugh, Gregory and Tuytelaars, Tinne},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={44},
  number={7},
  pages={3366--3385},
  year={2021},
  publisher={IEEE}
}
@book{kandel2000principles,
  title={Principles of neural science},
  author={Kandel, Eric R and Schwartz, James H and Jessell, Thomas M and Siegelbaum, Steven and Hudspeth, A James and Mack, Sarah and others},
  volume={4},
  year={2000},
  publisher={McGraw-hill New York}
}
@book{chen2018lifelong,
  title={Lifelong machine learning},
  author={Chen, Zhiyuan and Liu, Bing},
  volume={1},
  publisher={Springer}
}
@misc{wu2024continual,
      title={Continual Learning for Large Language Models: A Survey}, 
      author={Tongtong Wu and Linhao Luo and Yuan-Fang Li and Shirui Pan and Thuy-Trang Vu and Gholamreza Haffari},
      year={2024},
      eprint={2402.01364},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{xu2024survey,
      title={A Survey on Knowledge Distillation of Large Language Models}, 
      author={Xiaohan Xu and Ming Li and Chongyang Tao and Tao Shen and Reynold Cheng and Jinyang Li and Can Xu and Dacheng Tao and Tianyi Zhou},
      year={2024},
      eprint={2402.13116},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@ARTICLE{wang2024comprehensive,
  author={Wang, Liyuan and Zhang, Xingxing and Su, Hang and Zhu, Jun},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A Comprehensive Survey of Continual Learning: Theory, Method and Application}, 
  year={2024},
  volume={},
  number={},
  pages={1-20},
  keywords={Task analysis;Training;Surveys;Testing;Complexity theory;Stability analysis;Visualization;Continual learning;incremental learning;lifelong learning;catastrophic forgetting},
  doi={10.1109/TPAMI.2024.3367329}
}
@phdthesis{pentina2016theoretical,
  title={Theoretical foundations of multi-task lifelong learning},
  author={Pentina, Anastasia},
  year={2016}
}
@inproceedings{biesialska2020continual,
    title = "Continual Lifelong Learning in Natural Language Processing: A Survey",
    author = "Biesialska, Magdalena  and
      Biesialska, Katarzyna  and
      Costa-juss{\`a}, Marta R.",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.574",
    doi = "10.18653/v1/2020.coling-main.574",
    pages = "6523--6541",
    abstract = "Continual learning (CL) aims to enable information systems to learn from a continuous data stream across time. However, it is difficult for existing deep learning architectures to learn a new task without largely forgetting previously acquired knowledge. Furthermore, CL is particularly challenging for language learning, as natural language is ambiguous: it is discrete, compositional, and its meaning is context-dependent. In this work, we look at the problem of CL through the lens of various NLP tasks. Our survey discusses major challenges in CL and current methods applied in neural network models. We also provide a critical review of the existing CL evaluation methods and datasets in NLP. Finally, we present our outlook on future research directions.",
}
@misc{ke2023continual,
      title={Continual Learning of Natural Language Processing Tasks: A Survey}, 
      author={Zixuan Ke and Bing Liu},
      year={2023},
      eprint={2211.12701},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{van2022three,
  title={Three types of incremental learning},
  author={Van de Ven, Gido M and Tuytelaars, Tinne and Tolias, Andreas S},
  journal={Nature Machine Intelligence},
  volume={4},
  number={12},
  pages={1185--1197},
  year={2022},
  publisher={Nature Publishing Group UK London}
}
@inproceedings{kim2022theoretical,
 author = {Kim, Gyuhak and Xiao, Changnan and Konishi, Tatsuya and Ke, Zixuan and Liu, Bing},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {5065--5079},
 publisher = {Curran Associates, Inc.},
 title = {A Theoretical Study on Solving Continual Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/20f44da80080d76bbc35bca0027f14e6-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}


% Neuro Scinece
@article{mcclelland1995there,
  title={Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory.},
  author={McClelland, James L and McNaughton, Bruce L and O'Reilly, Randall C},
  journal={Psychological review},
  volume={102},
  number={3},
  pages={419},
  year={1995},
  publisher={American Psychological Association}
}
@article{yang2009stably,
  title={Stably maintained dendritic spines are associated with lifelong memories},
  author={Yang, Guang and Pan, Feng and Gan, Wen-Biao},
  journal={Nature},
  volume={462},
  number={7275},
  pages={920--924},
  year={2009},
  publisher={Nature Publishing Group UK London}
}
@article{pallier2003brain,
  title={Brain imaging of language plasticity in adopted adults: Can a second language replace the first?},
  author={Pallier, Christophe and Dehaene, Stanislas and Poline, J-B and LeBihan, Denis and Argenti, A-M and Dupoux, Emmanuel and Mehler, Jacques},
  journal={Cerebral cortex},
  volume={13},
  number={2},
  pages={155--161},
  year={2003},
  publisher={Oxford University Press}
}
@article{olafsdottir2018role,
  title={The role of hippocampal replay in memory and planning},
  author={{\'O}lafsd{\'o}ttir, H Freyja and Bush, Daniel and Barry, Caswell},
  journal={Current Biology},
  volume={28},
  number={1},
  pages={R37--R50},
  year={2018},
  publisher={Elsevier}
}
@article{liu2019human,
  title={Human replay spontaneously reorganizes experience},
  author={Liu, Yunzhe and Dolan, Raymond J and Kurth-Nelson, Zeb and Behrens, Timothy EJ},
  journal={Cell},
  volume={178},
  number={3},
  pages={640--652},
  year={2019},
  publisher={Elsevier}
}
@article{constantinescu2016organizing,
  title={Organizing conceptual knowledge in humans with a gridlike code},
  author={Constantinescu, Alexandra O and O’Reilly, Jill X and Behrens, Timothy EJ},
  journal={Science},
  volume={352},
  number={6292},
  pages={1464--1468},
  year={2016},
  publisher={American Association for the Advancement of Science}
}
@article{mccaffary2021towards,
  title={Towards continual task learning in artificial neural networks: current approaches and insights from neuroscience},
  author={McCaffary, David},
  journal={arXiv preprint arXiv:2112.14146},
  year={2021}
}

% CLIPs
@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}
@inproceedings{zheng2023preventing,
  title={Preventing zero-shot transfer degradation in continual learning of vision-language models},
  author={Zheng, Zangwei and Ma, Mingyuan and Wang, Kai and Qin, Ziheng and Yue, Xiangyu and You, Yang},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={19125--19136},
  year={2023}
}
@article{wu2024building,
  title={Building an open-vocabulary video CLIP model with better architectures, optimization and data},
  author={Wu, Zuxuan and Weng, Zejia and Peng, Wujian and Yang, Xitong and Li, Ang and Davis, Larry S and Jiang, Yu-Gang},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024},
  publisher={IEEE}
}
@article{khan2021personalizing,
  title={Personalizing pre-trained models},
  author={Khan, Mina and Srivatsa, P and Rane, Advait and Chenniappa, Shriram and Hazariwala, Asadali and Maes, Pattie},
  journal={arXiv preprint arXiv:2106.01499},
  year={2021}
}
@article{liu2023class,
  title={Class Incremental Learning with Pre-trained Vision-Language Models},
  author={Liu, Xialei and Cao, Xusheng and Lu, Haori and Xiao, Jia-wen and Bagdanov, Andrew D and Cheng, Ming-Ming},
  journal={arXiv preprint arXiv:2310.20348},
  year={2023}
}
@article{jha2024clap4clip,
  title={CLAP4CLIP: Continual Learning with Probabilistic Finetuning for Vision-Language Models},
  author={Jha, Saurav and Gong, Dong and Yao, Lina},
  journal={arXiv preprint arXiv:2403.19137},
  year={2024}
}
@article{li2024coleclip,
  title={CoLeCLIP: Open-Domain Continual Learning via Joint Task Prompt and Vocabulary Learning},
  author={Li, Yukun and Pang, Guansong and Suo, Wei and Jing, Chenchen and Xi, Yuling and Liu, Lingqiao and Chen, Hao and Liang, Guoqiang and Wang, Peng},
  journal={arXiv preprint arXiv:2403.10245},
  year={2024}
}


% Miscs
@article{ebrahimi2019uncertainty,
  title={Uncertainty-guided continual learning with bayesian neural networks},
  author={Ebrahimi, Sayna and Elhoseiny, Mohamed and Darrell, Trevor and Rohrbach, Marcus},
  journal={arXiv preprint arXiv:1906.02425},
  year={2019}
}
@inproceedings{ebrahimi2020adversarial,
  title={Adversarial continual learning},
  author={Ebrahimi, Sayna and Meier, Franziska and Calandra, Roberto and Darrell, Trevor and Rohrbach, Marcus},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XI 16},
  pages={386--402},
  year={2020},
  organization={Springer}
}
@article{wang2022dualprompt,
  title={DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning},
  author={Wang, Zifeng and Zhang, Zizhao and Ebrahimi, Sayna and Sun, Ruoxi and Zhang, Han and Lee, Chen-Yu and Ren, Xiaoqi and Su, Guolong and Perot, Vincent and Dy, Jennifer and others},
  journal={European Conference on Computer Vision},
  year={2022}
}
@inproceedings{wang2022learning,
  title={Learning to prompt for continual learning},
  author={Wang, Zifeng and Zhang, Zizhao and Lee, Chen-Yu and Zhang, Han and Sun, Ruoxi and Ren, Xiaoqi and Su, Guolong and Perot, Vincent and Dy, Jennifer and Pfister, Tomas},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={139--149},
  year={2022}
}
@misc{jiang2024empowering,
      title={Empowering Time Series Analysis with Large Language Models: A Survey}, 
      author={Yushan Jiang and Zijie Pan and Xikun Zhang and Sahil Garg and Anderson Schneider and Yuriy Nevmyvaka and Dongjin Song},
      year={2024},
      eprint={2402.03182},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@InProceedings{garg2023in,
  title = 	 {In- or out-of-distribution detection via dual divergence estimation},
  author =       {Garg, Sahil and Dutta, Sanghamitra and Dalirrooyfard, Mina and Schneider, Anderson and Nevmyvaka, Yuriy},
  booktitle = 	 {Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {635--646},
  year = 	 {2023},
  editor = 	 {Evans, Robin J. and Shpitser, Ilya},
  volume = 	 {216},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {31 Jul--04 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v216/garg23b/garg23b.pdf},
  url = 	 {https://proceedings.mlr.press/v216/garg23b.html},
  abstract = 	 {Detecting out-of-distribution (OOD) samples is a problem of practical importance for a reliable use of deep neural networks (DNNs) in production settings. The corollary to this problem is the detection in-distribution (ID) samples, which is applicable to domain adaptation scenarios for augmenting a train set with ID samples from other data sets, or to continual learning for replay from the past. For both ID or OOD detection, we propose a principled yet simple approach of (empirically) estimating KL-Divergence, in its dual form, for a given test set w.r.t. a known set of ID samples in order to quantify the contribution of each test sample individually towards the divergence measure and accordingly detect it as OOD or ID. Our approach is compute-efficient and enjoys strong theoretical guarantees. For WideResnet101 and ViT-L-16, by considering ImageNet-1k dataset as the ID benchmark, we evaluate the proposed OOD detector on 51 test (OOD) datasets, and observe drastically and consistently lower false positive rates w.r.t. all the competitive methods. Moreover, the proposed ID detector is evaluated, using ECG and stock price datasets, for the task of data augmentation in domain adaptation and continual learning settings, and we observe higher efficacy compared to relevant baselines.}
}

@inproceedings{bengio2009curriculum,
    author = {Bengio, Yoshua and Louradour, J\'{e}r\^{o}me and Collobert, Ronan and Weston, Jason},
    title = {Curriculum learning},
    year = {2009},
    isbn = {9781605585161},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1553374.1553380},
    doi = {10.1145/1553374.1553380},
    abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them "curriculum learning". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
    booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
    pages = {41–48},
    numpages = {8},
    location = {Montreal, Quebec, Canada},
    series = {ICML '09}
}
@misc{prabhu2023online,
      title={Online Continual Learning Without the Storage Constraint}, 
      author={Ameya Prabhu and Zhipeng Cai and Puneet Dokania and Philip Torr and Vladlen Koltun and Ozan Sener},
      year={2023},
      eprint={2305.09253},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@inproceedings{garg2024tic,
  title={TiC-CLIP: Continual Training of CLIP Models},
  author={Garg, Saurabh and Farajtabar, Mehrdad and Pouransari, Hadi and Vemulapalli, Raviteja and Mehta, Sachin and Tuzel, Oncel and Shankar, Vaishaal and Faghri, Fartash},
  booktitle={The Twelfth International Conference on Learning Representations (ICLR)},
  year={2024},
  url={https://openreview.net/forum?id=TLADT8Wrhn}
}
@misc{thengane2022clip,
      title={CLIP model is an Efficient Continual Learner}, 
      author={Vishal Thengane and Salman Khan and Munawar Hayat and Fahad Khan},
      year={2022},
      eprint={2210.03114},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@inproceedings{wu2021pretrained,
  title={Pretrained language model in continual learning: A comparative study},
  author={Wu, Tongtong and Caccia, Massimo and Li, Zhuang and Li, Yuan-Fang and Qi, Guilin and Haffari, Gholamreza},
  booktitle={International conference on learning representations},
  year={2021}
}
@InProceedings{mirzadeh2022wide,
  title = 	 {Wide Neural Networks Forget Less Catastrophically},
  author =       {Mirzadeh, Seyed Iman and Chaudhry, Arslan and Yin, Dong and Hu, Huiyi and Pascanu, Razvan and Gorur, Dilan and Farajtabar, Mehrdad},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {15699--15717},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/mirzadeh22a/mirzadeh22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/mirzadeh22a.html},
  abstract = 	 {A primary focus area in continual learning research is alleviating the "catastrophic forgetting" problem in neural networks by designing new algorithms that are more robust to the distribution shifts. While the recent progress in continual learning literature is encouraging, our understanding of what properties of neural networks contribute to catastrophic forgetting is still limited. To address this, instead of focusing on continual learning algorithms, in this work, we focus on the model itself and study the impact of "width" of the neural network architecture on catastrophic forgetting, and show that width has a surprisingly significant effect on forgetting. To explain this effect, we study the learning dynamics of the network from various perspectives such as gradient orthogonality, sparsity, and lazy training regime. We provide potential explanations that are consistent with the empirical results across different architectures and continual learning benchmarks.}
}

@article{neyshabur2020being,
  title={What is being transferred in transfer learning?},
  author={Neyshabur, Behnam and Sedghi, Hanie and Zhang, Chiyuan},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={512--523},
  year={2020}
}
@inproceedings{hao2019visualizing,
    title = "Visualizing and Understanding the Effectiveness of {BERT}",
    author = "Hao, Yaru  and
      Dong, Li  and
      Wei, Furu  and
      Xu, Ke",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1424",
    doi = "10.18653/v1/D19-1424",
    pages = "4143--4152",
    abstract = "Language model pre-training, such as BERT, has achieved remarkable results in many NLP tasks. However, it is unclear why the pre-training-then-fine-tuning paradigm can improve performance and generalization capability across different tasks. In this paper, we propose to visualize loss landscapes and optimization trajectories of fine-tuning BERT on specific datasets. First, we find that pre-training reaches a good initial point across downstream tasks, which leads to wider optima and easier optimization compared with training from scratch. We also demonstrate that the fine-tuning procedure is robust to overfitting, even though BERT is highly over-parameterized for downstream tasks. Second, the visualization results indicate that fine-tuning BERT tends to generalize better because of the flat and wide optima, and the consistency between the training loss surface and the generalization error surface. Third, the lower layers of BERT are more invariant during fine-tuning, which suggests that the layers that are close to input learn more transferable representations of language.",
}
@article{mehta2023empirical,
  author  = {Sanket Vaibhav Mehta and Darshan Patil and Sarath Chandar and Emma Strubell},
  title   = {An Empirical Investigation of the Role of Pre-training in Lifelong Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {214},
  pages   = {1--50},
  url     = {http://jmlr.org/papers/v24/22-0496.html}
}
@misc{li2023blip2,
      title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models}, 
      author={Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},
      year={2023},
      eprint={2301.12597},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}
@inproceedings{yuan2020revisiting,
  title={Revisiting Knowledge Distillation via Label Smoothing Regularization},
  author={Yuan, Li and Tay, Francis EH and Li, Guilin and Wang, Tao and Feng, Jiashi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3903--3911},
  year={2020}
}
@inproceedings{chen2020recall,
    title = "Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting",
    author = "Chen, Sanyuan  and
      Hou, Yutai  and
      Cui, Yiming  and
      Che, Wanxiang  and
      Liu, Ting  and
      Yu, Xiangzhan",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.634",
    doi = "10.18653/v1/2020.emnlp-main.634",
    pages = "7870--7881",
    abstract = "Deep pretrained language models have achieved great success in the way of pretraining first and then fine-tuning. But such a sequential transfer learning paradigm often confronts the catastrophic forgetting problem and leads to sub-optimal performance. To fine-tune with less forgetting, we propose a recall and learn mechanism, which adopts the idea of multi-task learning and jointly learns pretraining tasks and downstream tasks. Specifically, we introduce a Pretraining Simulation mechanism to recall the knowledge from pretraining tasks without data, and an Objective Shifting mechanism to focus the learning on downstream tasks gradually. Experiments show that our method achieves state-of-the-art performance on the GLUE benchmark. Our method also enables BERT-base to achieve better average performance than directly fine-tuning of BERT-large. Further, we provide the open-source RecAdam optimizer, which integrates the proposed mechanisms into Adam optimizer, to facility the NLP community.",
}
@inproceedings{he2021analyzing,
    title = "Analyzing the Forgetting Problem in Pretrain-Finetuning of Open-domain Dialogue Response Models",
    author = "He, Tianxing  and
      Liu, Jun  and
      Cho, Kyunghyun  and
      Ott, Myle  and
      Liu, Bing  and
      Glass, James  and
      Peng, Fuchun",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.95",
    doi = "10.18653/v1/2021.eacl-main.95",
    pages = "1121--1133",
    abstract = "In this work, we study how the finetuning stage in the pretrain-finetune framework changes the behavior of a pretrained neural language generator. We focus on the transformer encoder-decoder model for the open-domain dialogue response generation task. Our major finding is that after standard finetuning, the model forgets some of the important language generation skills acquired during large-scale pretraining. We demonstrate the forgetting phenomenon through a set of detailed behavior analysis from the perspectives of knowledge transfer, context sensitivity, and function space projection. As a preliminary attempt to alleviate the forgetting problem, we propose an intuitive finetuning strategy named {``}mix-review{''}. We find that mix-review effectively regularizes the finetuning process, and the forgetting problem is alleviated to some extent. Finally, we discuss interesting behavior of the resulting dialogue model and its implications.",
}
@inproceedings{hu2022lora,
    title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
    author={Edward J Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=nZeVKeeFYf9}
}
@inproceedings{wang2021kadapter,
    title = "{K-Adapter}: {I}nfusing {K}nowledge into {P}re-{T}rained {M}odels with {A}dapters",
    author = "Wang, Ruize  and
      Tang, Duyu  and
      Duan, Nan  and
      Wei, Zhongyu  and
      Huang, Xuanjing  and
      Ji, Jianshu  and
      Cao, Guihong  and
      Jiang, Daxin  and
      Zhou, Ming",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.121",
    doi = "10.18653/v1/2021.findings-acl.121",
    pages = "1405--1418",
}
@article{ben2010theory,
  title={A theory of learning from different domains},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
  journal={Machine learning},
  volume={79},
  pages={151--175},
  year={2010},
  publisher={Springer}
}
@article{ganin2016domain,
  title={Domain-adversarial training of neural networks},
  author={Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
  journal={The journal of machine learning research},
  volume={17},
  number={1},
  pages={2096--2030},
  year={2016},
  publisher={JMLR. org}
}
@article{zewdu2022part,
    author = {Zewdu, Alebachew and Yitagesu, Betselot},
    year = {2022},
    month = {01},
    pages = {},
    title = {Part of speech tagging: a systematic review of deep learning and machine learning approaches},
    volume = {9},
    journal = {Journal of Big Data},
    doi = {10.1186/s40537-022-00561-y}
}

%% PEFT
@inproceedings{li2021prefix,
    title = "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    author = "Li, Xiang Lisa  and
      Liang, Percy",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.353",
    doi = "10.18653/v1/2021.acl-long.353",
    pages = "4582--4597",
    abstract = "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were {``}virtual tokens{''}. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1{\%} of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.",
}
@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}
@inproceedings{lester2021power,
    title = "The Power of Scale for Parameter-Efficient Prompt Tuning",
    author = "Lester, Brian  and
      Al-Rfou, Rami  and
      Constant, Noah",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.243",
    doi = "10.18653/v1/2021.emnlp-main.243",
    pages = "3045--3059",
    abstract = "In this work, we explore {``}prompt tuning,{''} a simple yet effective mechanism for learning {``}soft prompts{''} to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3{'}s few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method {``}closes the gap{''} and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed {``}prefix tuning{''} of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient {``}prompt ensembling.{''} We release code and model checkpoints to reproduce our experiments.",
}


% Pure LLMs
@misc{gpt-j,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}
@InProceedings{du2022glam,
  title = 	 {{GL}a{M}: Efficient Scaling of Language Models with Mixture-of-Experts},
  author =       {Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and Zoph, Barret and Fedus, Liam and Bosma, Maarten P and Zhou, Zongwei and Wang, Tao and Wang, Emma and Webster, Kellie and Pellat, Marie and Robinson, Kevin and Meier-Hellstern, Kathleen and Duke, Toju and Dixon, Lucas and Zhang, Kun and Le, Quoc and Wu, Yonghui and Chen, Zhifeng and Cui, Claire},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {5547--5569},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/du22c/du22c.pdf},
  url = 	 {https://proceedings.mlr.press/v162/du22c.html},
  abstract = 	 {Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named \glam (\textbf{G}eneralist \textbf{La}nguage \textbf{M}odel), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest \glam has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall fewshot performance across 29 NLP tasks.}
}

@misc{soldaini2024dolma,
      title={Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research}, 
      author={Luca Soldaini and Rodney Kinney and Akshita Bhagia and Dustin Schwenk and David Atkinson and Russell Authur and Ben Bogin and Khyathi Chandu and Jennifer Dumas and Yanai Elazar and Valentin Hofmann and Ananya Harsh Jha and Sachin Kumar and Li Lucy and Xinxi Lyu and Nathan Lambert and Ian Magnusson and Jacob Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E. Peters and Abhilasha Ravichander and Kyle Richardson and Zejiang Shen and Emma Strubell and Nishant Subramani and Oyvind Tafjord and Pete Walsh and Luke Zettlemoyer and Noah A. Smith and Hannaneh Hajishirzi and Iz Beltagy and Dirk Groeneveld and Jesse Dodge and Kyle Lo},
      year={2024},
      eprint={2402.00159},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{xie2024data,
  title={Data selection for language models via importance resampling},
  author={Xie, Sang Michael and Santurkar, Shibani and Ma, Tengyu and Liang, Percy S},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{li2023quality,
  title={From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning},
  author={Ming Li and Yong Zhang and Zhitao Li and Jiuhai Chen and Lichang Chen and Ning Cheng and Jianzong Wang and Tianyi Zhou and Jing Xiao},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.12032},
  url={https://api.semanticscholar.org/CorpusID:261076515}
}
@article{min2022rethinking,
  title={Rethinking the role of demonstrations: What makes in-context learning work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2202.12837},
  year={2022}
}
@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}
@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}
@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}
@misc{deepseekai2024deepseek,
      title={DeepSeek LLM: Scaling Open-Source Language Models with Longtermism}, 
      author={DeepSeek-AI Team},
      year={2024},
      eprint={2401.02954},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{biderman2023pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle={International Conference on Machine Learning},
  pages={2397--2430},
  year={2023},
  organization={PMLR}
}
@article{gao2020pile,
  title={The {P}ile: An 800GB Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}
@misc{cerebras2023slimpajama,
    author = {Soboleva, Daria and Al-Khateeb, Faisal and Myers, Robert and Steeves, Jacob R and Hestness, Joel and Dey, Nolan},
    title = {{SlimPajama: A 627B token cleaned and deduplicated version of RedPajama}},
    year = {2023},
    howpublished = {\url{https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama}},
    url = {https://huggingface.co/datasets/cerebras/SlimPajama-627B},
}

% Pure CL Fundamental
@article{lopez2017gradient,
  title={Gradient episodic memory for continual learning},
  author={Lopez-Paz, David and Ranzato, Marc'Aurelio},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@inproceedings{wu2019large,
  title={Large scale incremental learning},
  author={Wu, Yue and Chen, Yinpeng and Wang, Lijuan and Ye, Yuancheng and Liu, Zicheng and Guo, Yandong and Fu, Yun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={374--382},
  year={2019}
}
@Inproceedings{wistuba2023,
 author = {Martin Wistuba and Prabhu Teja Sivaprasad and Lukas Balles and Giovanni Zappella},
 title = {Continual learning with low rank adaptation},
 year = {2023},
 url = {https://www.amazon.science/publications/continual-learning-with-low-rank-adaptation},
 booktitle = {NeurIPS 2023 Workshop on Distribution Shifts (DistShifts)},
}
@inproceedings{aljundi2017expert,
  title={Expert gate: Lifelong learning with a network of experts},
  author={Aljundi, Rahaf and Chakravarty, Punarjay and Tuytelaars, Tinne},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3366--3375},
  year={2017}
}
@inproceedings{cai2021online,
  title={Online continual learning with natural distribution shifts: An empirical study with visual data},
  author={Cai, Zhipeng and Sener, Ozan and Koltun, Vladlen},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={8281--8290},
  year={2021}
}
@misc{verwimp2024continual,
      title={Continual Learning: Applications and the Road Forward}, 
      author={Eli Verwimp and Rahaf Aljundi and Shai Ben-David and Matthias Bethge and Andrea Cossu and Alexander Gepperth and Tyler L. Hayes and Eyke Hüllermeier and Christopher Kanan and Dhireesha Kudithipudi and Christoph H. Lampert and Martin Mundt and Razvan Pascanu and Adrian Popescu and Andreas S. Tolias and Joost van de Weijer and Bing Liu and Vincenzo Lomonaco and Tinne Tuytelaars and Gido M. van de Ven},
      year={2024},
      eprint={2311.11908},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

%% Rehearsal-Free
@article{li2017learning,
  title={Learning without forgetting},
  author={Li, Zhizhong and Hoiem, Derek},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={12},
  pages={2935--2947},
  year={2017},
  publisher={IEEE}
}
@misc{smith2023closer,
      title={A Closer Look at Rehearsal-Free Continual Learning}, 
      author={James Seale Smith and Junjiao Tian and Shaunak Halbe and Yen-Chang Hsu and Zsolt Kira},
      year={2023},
      eprint={2203.17269},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{hayes2020lifelong,
      title={Lifelong Machine Learning with Deep Streaming Linear Discriminant Analysis}, 
      author={Tyler L. Hayes and Christopher Kanan},
      year={2020},
      eprint={1909.01520},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{lomonaco2020rehearsalfree,
      title={Rehearsal-Free Continual Learning over Small Non-I.I.D. Batches}, 
      author={Vincenzo Lomonaco and Davide Maltoni and Lorenzo Pellegrini},
      year={2020},
      eprint={1907.03799},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

%% Replay, Rehearsal
@article{chaudhry2019tiny,
  title={On tiny episodic memories in continual learning},
  author={Chaudhry, Arslan and Rohrbach, Marcus and Elhoseiny, Mohamed and Ajanthan, Thalaiyasingam and Dokania, Puneet K and Torr, Philip HS and Ranzato, Marc'Aurelio},
  journal={arXiv preprint arXiv:1902.10486},
  year={2019}
}
@inproceedings{schwarz2018progress,
  title={Progress \& compress: A scalable framework for continual learning},
  author={Schwarz, Jonathan and Czarnecki, Wojciech and Luketina, Jelena and Grabska-Barwinska, Agnieszka and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
  booktitle={International conference on machine learning},
  pages={4528--4537},
  year={2018},
  organization={PMLR}
}
@article{sarfraz2023error,
  title={Error Sensitivity Modulation based Experience Replay: Mitigating Abrupt Representation Drift in Continual Learning},
  author={Sarfraz, Fahad and Arani, Elahe and Zonooz, Bahram},
  journal={arXiv preprint arXiv:2302.11344},
  year={2023}
}
@article{riemer2018learning,
  title={Learning to learn without forgetting by maximizing transfer and minimizing interference},
  author={Riemer, Matthew and Cases, Ignacio and Ajemian, Robert and Liu, Miao and Rish, Irina and Tu, Yuhai and Tesauro, Gerald},
  journal={arXiv preprint arXiv:1810.11910},
  year={2018}
}
@article{buzzega2020dark,
  title={Dark experience for general continual learning: a strong, simple baseline},
  author={Buzzega, Pietro and Boschini, Matteo and Porrello, Angelo and Abati, Davide and Calderara, Simone},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={15920--15930},
  year={2020}
}
@article{shi2024unified,
  title={A Unified Approach to Domain Incremental Learning with Memory: Theory and Algorithm},
  author={Shi, Haizhou and Wang, Hao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@InProceedings{bang2021rainbow,
    author    = {Bang, Jihwan and Kim, Heesu and Yoo, YoungJoon and Ha, Jung-Woo and Choi, Jonghyun},
    title     = {Rainbow Memory: Continual Learning With a Memory of Diverse Samples},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {8218-8227}
}
@ARTICLE{zhao2022memory,
  author={Zhao, Hanbin and Wang, Hui and Fu, Yongjian and Wu, Fei and Li, Xi},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Memory-Efficient Class-Incremental Learning for Image Classification}, 
  year={2022},
  volume={33},
  number={10},
  pages={5966-5977},
  keywords={Feature extraction;Knowledge transfer;Data mining;Adaptation models;Training;Noise measurement;Knowledge engineering;Catastrophic forgetting;class-incremental learning (CIL);classification;exemplar;memory efficient},
  doi={10.1109/TNNLS.2021.3072041}}



%% Param. Reg.
@inproceedings{rebuffi2017icarl,
  title={icarl: Incremental classifier and representation learning},
  author={Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander and Sperl, Georg and Lampert, Christoph H},
  booktitle={Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
  pages={2001--2010},
  year={2017}
}
@article{ritter2018online,
  title={Online structured laplace approximations for overcoming catastrophic forgetting},
  author={Ritter, Hippolyt and Botev, Aleksandar and Barber, David},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
@inproceedings{aljundi2018memory,
  title={Memory aware synapses: Learning what (not) to forget},
  author={Aljundi, Rahaf and Babiloni, Francesca and Elhoseiny, Mohamed and Rohrbach, Marcus and Tuytelaars, Tinne},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={139--154},
  year={2018}
}
@inproceedings{chaudhry2019efficient,
  title={Efficient Lifelong Learning with A-GEM},
  author={Chaudhry, Arslan and Ranzato, Marc’Aurelio and Rohrbach, Marcus and Elhoseiny, Mohamed},
  booktitle={ICLR},
  year={2019}
}
@inproceedings{sprechmann2018memory,
  title={Memory-based Parameter Adaptation},
  author={Sprechmann, Pablo and Jayakumar, Siddhant M and Rae, Jack W and Pritzel, Alexander and Badia, Adria Puigdomenech and Uria, Benigno and Vinyals, Oriol and Hassabis, Demis and Pascanu, Razvan and Blundell, Charles},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@article{caccia2021new,
  title={New insights on reducing abrupt representation change in online continual learning},
  author={Caccia, Lucas and Aljundi, Rahaf and Asadi, Nader and Tuytelaars, Tinne and Pineau, Joelle and Belilovsky, Eugene},
  journal={arXiv preprint arXiv:2104.05025},
  year={2021}
}
%% Arch. Exp.
@article{ramesh2021model,
  title={Model Zoo: A Growing" Brain" That Learns Continually},
  author={Ramesh, Rahul and Chaudhari, Pratik},
  journal={arXiv preprint arXiv:2106.03027},
  year={2021}
}
@inproceedings{wang2022coscl,
  title={CoSCL: Cooperation of Small Continual Learners is Stronger Than a Big One},
  author={Wang, Liyuan and Zhang, Xingxing and Li, Qian and Zhu, Jun and Zhong, Yi},
  booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXVI},
  pages={254--271},
  year={2022},
  organization={Springer}
}

%%% Datasets
@inproceedings{he2016ups,
    author = {He, Ruining and McAuley, Julian},
    title = {Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering},
    year = {2016},
    isbn = {9781450341431},
    publisher = {International World Wide Web Conferences Steering Committee},
    address = {Republic and Canton of Geneva, CHE},
    url = {https://doi.org/10.1145/2872427.2883037},
    doi = {10.1145/2872427.2883037},
    abstract = {Building a successful recommender system depends on understanding both the dimensions of people's preferences as well as their dynamics. In certain domains, such as fashion, modeling such preferences can be incredibly difficult, due to the need to simultaneously model the visual appearance of products as well as their evolution over time. The subtle semantics and non-linear dynamics of fashion evolution raise unique challenges especially considering the sparsity and large scale of the underlying datasets. In this paper we build novel models for the One-Class Collaborative Filtering setting, where our goal is to estimate users' fashion-aware personalized ranking functions based on their past feedback. To uncover the complex and evolving visual factors that people consider when evaluating products, our method combines high-level visual features extracted from a deep convolutional neural network, users' past feedback, as well as evolving trends within the community. Experimentally we evaluate our method on two large real-world datasets from Amazon.com, where we show it to outperform state-of-the-art personalized ranking measures, and also use it to visualize the high-level fashion trends across the 11-year span of our dataset.},
    booktitle = {Proceedings of the 25th International Conference on World Wide Web},
    pages = {507–517},
    numpages = {11},
    keywords = {fashion evolution, personalized ranking, recommender systems, visual dimensions},
    location = {Montr\'{e}al, Qu\'{e}bec, Canada},
    series = {WWW '16}
}
@inproceedings{ni2019justifying,
    title = "Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects",
    author = "Ni, Jianmo  and
      Li, Jiacheng  and
      McAuley, Julian",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1018",
    doi = "10.18653/v1/D19-1018",
    pages = "188--197",
    abstract = "Several recent works have considered the problem of generating reviews (or {`}tips{'}) as a form of explanation as to why a recommendation might match a customer{'}s interests. While promising, we demonstrate that existing approaches struggle (in terms of both quality and content) to generate justifications that are relevant to users{'} decision-making process. We seek to introduce new datasets and methods to address the recommendation justification task. In terms of data, we first propose an {`}extractive{'} approach to identify review segments which justify users{'} intentions; this approach is then used to distantly label massive review corpora and construct large-scale personalized recommendation justification datasets. In terms of generation, we are able to design two personalized generation models with this data: (1) a reference-based Seq2Seq model with aspect-planning which can generate justifications covering different aspects, and (2) an aspect-conditional masked language model which can generate diverse justifications based on templates extracted from justification histories. We conduct experiments on two real-world datasets which show that our model is capable of generating convincing and diverse justifications.",
}
@misc{baumgartner2020pushshift,
      title={The Pushshift Reddit Dataset}, 
      author={Jason Baumgartner and Savvas Zannettou and Brian Keegan and Megan Squire and Jeremy Blackburn},
      year={2020},
      eprint={2001.08435},
      archivePrefix={arXiv},
      primaryClass={cs.SI}
}
@article{zellers2019defending,
    title={Defending against neural fake news},
    author={Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
    journal={Advances in neural information processing systems},
    volume={32},
    year={2019}
}
@misc{gokaslan2019OpenWeb,  
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	url="http://Skylion007.github.io/OpenWebTextCorpus", 
	year={2019}
}
@misc{caselaw2018,
      title = {Caselaw Access Project},
      author ={{Caselaw Access Project}},
      publisher = "Harvard",
      url = "https://case.law/",
      year = {2018}
}
@misc{chelba2014billion,
      title={One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling}, 
      author={Ciprian Chelba and Tomas Mikolov and Mike Schuster and Qi Ge and Thorsten Brants and Phillipp Koehn and Tony Robinson},
      year={2014},
      eprint={1312.3005},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{zhang2015character,
    author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Character-level Convolutional Networks for Text Classification},
    url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},
    volume = {28},
    year = {2015}
}
@misc{xu2019bert,
      title={BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis}, 
      author={Hu Xu and Bing Liu and Lei Shu and Philip S. Yu},
      year={2019},
      eprint={1904.02232},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1904.02232}, 
}

%%%% Miscs
@article{deng2023survey,
  title={A survey on proactive dialogue systems: Problems, methods, and prospects},
  author={Deng, Yang and Lei, Wenqiang and Lam, Wai and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2305.02750},
  year={2023}
}
@inproceedings{kwok2001scaling,
    author = {Kwok, Cody C. T. and Etzioni, Oren and Weld, Daniel S.},
    title = {Scaling question answering to the Web},
    year = {2001},
    isbn = {1581133480},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/371920.371973},
    doi = {10.1145/371920.371973},
    booktitle = {Proceedings of the 10th International Conference on World Wide Web},
    pages = {150–161},
    numpages = {12},
    location = {Hong Kong, Hong Kong},
    series = {WWW '01}
}
@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}
@article{ni2021revisiting,
  title={Revisiting catastrophic forgetting in class incremental learning},
  author={Ni, Zixuan and Shi, Haizhou and Tang, Siliang and Wei, Longhui and Tian, Qi and Zhuang, Yueting},
  journal={arXiv preprint arXiv:2107.12308},
  year={2021}
}

% CL + LLMs (Joint)

% Continual Pre-Training
@inproceedings{petroni2019language,
    title = "Language Models as Knowledge Bases?",
    author = {Petroni, Fabio  and
      Rockt{\"a}schel, Tim  and
      Riedel, Sebastian  and
      Lewis, Patrick  and
      Bakhtin, Anton  and
      Wu, Yuxiang  and
      Miller, Alexander},
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1250",
    doi = "10.18653/v1/D19-1250",
    pages = "2463--2473",
    abstract = "Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as {``}fill-in-the-blank{''} cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at \url{https://github.com/facebookresearch/LAMA}.",
}
@article{sun2020ernie, 
    title={ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding}, 
    volume={34}, 
    url={https://ojs.aaai.org/index.php/AAAI/article/view/6428}, 
    DOI={10.1609/aaai.v34i05.6428}, 
    abstractNote={&lt;p&gt;Recently pre-trained models have achieved state-of-the-art results in various language understanding tasks. Current pre-training procedures usually focus on training the model with several simple tasks to grasp the co-occurrence of words or sentences. However, besides co-occurring information, there exists other valuable lexical, syntactic and semantic information in training corpora, such as named entities, semantic closeness and discourse relations. In order to extract the lexical, syntactic and semantic information from training corpora, we propose a continual pre-training framework named ERNIE 2.0 which incrementally builds pre-training tasks and then learn pre-trained models on these constructed tasks via continual multi-task learning. Based on this framework, we construct several tasks and train the ERNIE 2.0 model to capture lexical, syntactic and semantic aspects of information in the training data. Experimental results demonstrate that ERNIE 2.0 model outperforms BERT and XLNet on 16 tasks including English tasks on GLUE benchmarks and several similar tasks in Chinese. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.&lt;/p&gt;}, 
    number={05}, 
    journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
    author={Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Tian, Hao and Wu, Hua and Wang, Haifeng}, 
    year={2020}, 
    month={Apr.}, 
    pages={8968-8975}
}

@inproceedings{amba2021dynamic,
  title={Dynamic language models for continuously evolving content},
  author={Amba Hombaiah, Spurthi and Chen, Tao and Zhang, Mingyang and Bendersky, Michael and Najork, Marc},
  booktitle={Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  pages={2514--2524},
  year={2021}
}

@article{qin2023recyclable,
  title={Recyclable Tuning for Continual Pre-training},
  author={Qin, Yujia and Qian, Cheng and Han, Xu and Lin, Yankai and Wang, Huadong and Xie, Ruobing and Liu, Zhiyuan and Sun, Maosong and Zhou, Jie},
  journal={arXiv preprint arXiv:2305.08702},
  year={2023}
}

@inproceedings{qin2022elle,
    title = "{ELLE}: Efficient Lifelong Pre-training for Emerging Data",
    author = "Qin, Yujia  and
      Zhang, Jiajie  and
      Lin, Yankai  and
      Liu, Zhiyuan  and
      Li, Peng  and
      Sun, Maosong  and
      Zhou, Jie",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.220",
    doi = "10.18653/v1/2022.findings-acl.220",
    pages = "2789--2810",
    abstract = "Current pre-trained language models (PLM) are typically trained with static data, ignoring that in real-world scenarios, streaming data of various sources may continuously grow. This requires PLMs to integrate the information from all the sources in a lifelong manner. Although this goal could be achieved by exhaustive pre-training on all the existing data, such a process is known to be computationally expensive. To this end, we propose ELLE, aiming at efficient lifelong pre-training for emerging data. Specifically, ELLE consists of (1) function preserved model expansion, which flexibly expands an existing PLM{'}s width and depth to improve the efficiency of knowledge acquisition; and (2) pre-trained domain prompts, which disentangle the versatile knowledge learned during pre-training and stimulate the proper knowledge for downstream tasks. We experiment ELLE with streaming data from 5 domains on BERT and GPT. The results show the superiority of ELLE over various lifelong learning baselines in both pre-training efficiency and downstream performances. The codes are publicly available at \url{https://github.com/thunlp/ELLE}.",
}

@article{gadde2021towards,
  title={Towards Continual Entity Learning in Language Models for Conversational Agents},
  author={Gadde, Ravi Teja and Bulyko, Ivan},
  journal={arXiv preprint arXiv:2108.00082},
  year={2021}
}

@inproceedings{jin2022lifelong,
    title = "Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora",
    author = "Jin, Xisen  and
      Zhang, Dejiao  and
      Zhu, Henghui  and
      Xiao, Wei  and
      Li, Shang-Wen  and
      Wei, Xiaokai  and
      Arnold, Andrew  and
      Ren, Xiang",
    editor = "Fan, Angela  and
      Ilic, Suzana  and
      Wolf, Thomas  and
      Gall{\'e}, Matthias",
    booktitle = "Proceedings of BigScience Episode {\#}5 -- Workshop on Challenges {\&} Perspectives in Creating Large Language Models",
    month = may,
    year = "2022",
    address = "virtual+Dublin",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.bigscience-1.1",
    doi = "10.18653/v1/2022.bigscience-1.1",
    pages = "1--16",
    abstract = "Pretrained language models (PTLMs) are typically learned over a large, static corpus and further fine-tuned for various downstream tasks. However, when deployed in the real world, a PTLM-based model must deal with data distributions that deviates from what the PTLM was initially trained on. In this paper, we study a lifelong language model pretraining challenge where a PTLM is continually updated so as to adapt to emerging data. Over a domain-incremental research paper stream and a chronologically-ordered tweet stream, we incrementally pretrain a PTLM with different continual learning algorithms, and keep track of the downstream task performance (after fine-tuning). We evaluate PTLM{'}s ability to adapt to new corpora while retaining learned knowledge in earlier corpora. Our experiments show distillation-based approaches to be most effective in retaining downstream performance in earlier domains. The algorithms also improve knowledge transfer, allowing models to achieve better downstream performance over latest data, and improve temporal generalization when distribution gaps exist between training and evaluation because of time. We believe our problem formulation, methods, and analysis will inspire future studies towards continual pretraining of language models.",
}

@misc{cossu2022continual,
      title={Continual Pre-Training Mitigates Forgetting in Language and Vision}, 
      author={Andrea Cossu and Tinne Tuytelaars and Antonio Carta and Lucia Passaro and Vincenzo Lomonaco and Davide Bacciu},
      year={2022},
      eprint={2205.09357},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{sun2023improving,
  title={Improving Representational Continuity via Continued Pretraining},
  author={Sun, Michael and Kumar, Ananya and Madaan, Divyam and Liang, Percy},
  journal={arXiv preprint arXiv:2302.13289},
  year={2023}
}

@inproceedings{ke2022continual-pre,
  title={Continual Pre-training of Language Models},
  author={Ke, Zixuan and Shao, Yijia and Lin, Haowei and Konishi, Tatsuya and Kim, Gyuhak and Liu, Bing},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@inproceedings{chen2023lifelong,
  title={Lifelong language pretraining with distribution-specialized experts},
  author={Chen, Wuyang and Zhou, Yanqi and Du, Nan and Huang, Yanping and Laudon, James and Chen, Zhifeng and Cui, Claire},
  booktitle={International Conference on Machine Learning},
  pages={5383--5395},
  year={2023},
  organization={PMLR}
}

@misc{gupta2023continual,
      title={Continual Pre-Training of Large Language Models: How to (re)warm your model?}, 
      author={Kshitij Gupta and Benjamin Thérien and Adam Ibrahim and Mats L. Richter and Quentin Anthony and Eugene Belilovsky and Irina Rish and Timothée Lesort},
      year={2023},
      eprint={2308.04014},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{gururangan2022demix,
    title = "{DEM}ix Layers: Disentangling Domains for Modular Language Modeling",
    author = "Gururangan, Suchin  and
      Lewis, Mike  and
      Holtzman, Ari  and
      Smith, Noah A.  and
      Zettlemoyer, Luke",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.407",
    doi = "10.18653/v1/2022.naacl-main.407",
    pages = "5557--5576",
    abstract = "We introduce a new domain expert mixture (DEMix) layer that enables conditioning a language model (LM) on the domain of the input text. A DEMix layer includes a collection of expert feedforward networks, each specialized to a domain, that makes the LM modular: experts can be mixed, added, or removed after initial training. Extensive experiments with autoregressive transformer LMs (up to 1.3B parameters) show that DEMix layers reduce test-time perplexity (especially for out-of-domain data), increase training efficiency, and enable rapid adaptation. Mixing experts during inference, using a parameter-free weighted ensemble, enables better generalization to heterogeneous or unseen domains. We also show it is possible to add experts to adapt to new domains without forgetting older ones, and remove experts to restrict access to unwanted domains. Overall, these results demonstrate benefits of domain modularity in language models.",
}

@misc{gogoulou2024continual,
      title={Continual Learning Under Language Shift}, 
      author={Evangelia Gogoulou and Timothée Lesort and Magnus Boman and Joakim Nivre},
      year={2024},
      eprint={2311.01200},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wu2024llama,
      title={LLaMA Pro: Progressive LLaMA with Block Expansion}, 
      author={Chengyue Wu and Yukang Gan and Yixiao Ge and Zeyu Lu and Jiahao Wang and Ye Feng and Ping Luo and Ying Shan},
      year={2024},
      eprint={2401.02415},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{li2024examining,
      title={Examining Forgetting in Continual Pre-training of Aligned Large Language Models}, 
      author={Chen-An Li and Hung-Yi Lee},
      year={2024},
      eprint={2401.03129},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{lazaridou2021mind,
  title={Mind the gap: Assessing temporal generalization in neural language models},
  author={Lazaridou, Angeliki and Kuncoro, Adhi and Gribovskaya, Elena and Agrawal, Devang and Liska, Adam and Terzi, Tayfun and Gimenez, Mai and de Masson d'Autume, Cyprien and Kocisky, Tomas and Ruder, Sebastian and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={29348--29363},
  year={2021}
}

@article{dhingra2022time,
  title={Time-aware language models as temporal knowledge bases},
  author={Dhingra, Bhuwan and Cole, Jeremy R and Eisenschlos, Julian Martin and Gillick, Daniel and Eisenstein, Jacob and Cohen, William W},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={257--273},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@inproceedings{jang2022towards,
  title={Towards Continual Knowledge Learning of Language Models},
  author={Jang, Joel and Ye, Seonghyeon and Yang, Sohee and Shin, Joongbo and Han, Janghoon and Kim, Gyeonghun and Choi, Stanley Jungkyu and Seo, Minjoon},
  booktitle={ICLR},
  year={2022}
}

@inproceedings{jang2022temporalwiki,
  title={TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models},
  author={Jang, Joel and Ye, Seonghyeon and Lee, Changho and Yang, Sohee and Shin, Joongbo and Han, Janghoon and Kim, Gyeonghun and Seo, Minjoon},
  journal={EMNLP 2022},
  year={2022}
}
@inproceedings{su2023efficient,
    title = "Efficient Continue Training of Temporal Language Model with Structural Information",
    author = "Su, Zhaochen  and
      Li, Juntao  and
      Zhang, Zikang  and
      Zhou, Zihan  and
      Zhang, Min",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.418",
    doi = "10.18653/v1/2023.findings-emnlp.418",
    pages = "6315--6329",
    abstract = "Current language models are mainly trained on snap-shots of data gathered at a particular time, which decreases their capability to generalize over time and model language change. To model the \textit{time} variable, existing works have explored temporal language models (e.g., TempoBERT) by directly incorporating the timestamp into the training process. While effective to some extent, these methods are limited by the superficial temporal information brought by timestamps, which fails to learn the inherent changes of linguistic components. In this paper, we empirically confirm that the performance of pre-trained language models (PLMs) is closely affiliated with syntactically changed tokens. Based on this observation, we propose a simple yet effective method named \textit{ \textbf{S}yntax-\textbf{G}uided \textbf{T}emporal \textbf{L}anguage \textbf{M}odel} (SG-TLM), which could learn the inherent language changes by capturing an intrinsic relationship between the \textit{time} prefix and the tokens with salient syntactic change. Experiments on two datasets and three tasks demonstrate that our model outperforms existing temporal language models in both memorization and generalization capabilities. Extensive results further confirm the effectiveness of our approach across different model frameworks, including both encoder-only and decoder-only models (e.g., LLaMA). Our code is available at \url{https://github.com/zhaochen0110/TempoLM}.",
}
@inproceedings{tan2023towards,
    title = "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models",
    author = "Tan, Qingyu  and
      Ng, Hwee Tou  and
      Bing, Lidong",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.828",
    doi = "10.18653/v1/2023.acl-long.828",
    pages = "14820--14835",
    abstract = "Reasoning about time is of fundamental importance. Many facts are time-dependent. For example, athletes change teams from time to time, and different government officials are elected periodically. Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of time spans or question types. In this paper, we introduce a comprehensive probing dataset TempReason to evaluate the temporal reasoning capability of large language models. Our dataset includes questions of three temporal reasoning levels. In addition, we also propose a novel learning framework to improve the temporal reasoning capability of large language models, based on temporal span extraction and time-sensitive reinforcement learning. We conducted experiments in closed book QA, open book QA, and reasoning QA settings and demonstrated the effectiveness of our approach.",
}
@inproceedings{rosin2022time,
    author = {Rosin, Guy D. and Guy, Ido and Radinsky, Kira},
    title = {Time Masking for Temporal Language Models},
    year = {2022},
    isbn = {9781450391320},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3488560.3498529},
    doi = {10.1145/3488560.3498529},
    abstract = {Our world is constantly evolving, and so is the content on the web. Consequently, our languages, often said to mirror the world, are dynamic in nature. However, most current contextual language models are static and cannot adapt to changes over time. In this work, we propose a temporal contextual language model called TempoBERT, which uses time as an additional context of texts. Our technique is based on modifying texts with temporal information and performing time masking - specific masking for the supplementary time information. We leverage our approach for the tasks of semantic change detection and sentence time prediction, experimenting on diverse datasets in terms of time, size, genre, and language. Our extensive evaluation shows that both tasks benefit from exploiting time masking.},
    booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
    pages = {833–841},
    numpages = {9},
    keywords = {temporal semantics, semantic change detection, language models},
    location = {Virtual Event, AZ, USA},
    series = {WSDM '22}
}
@inproceedings{loureiro2022timelms,
    title = "{T}ime{LM}s: Diachronic Language Models from {T}witter",
    author = "Loureiro, Daniel  and
      Barbieri, Francesco  and
      Neves, Leonardo  and
      Espinosa Anke, Luis  and
      Camacho-collados, Jose",
    editor = "Basile, Valerio  and
      Kozareva, Zornitsa  and
      Stajner, Sanja",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-demo.25",
    doi = "10.18653/v1/2022.acl-demo.25",
    pages = "251--260",
    abstract = "Despite its importance, the time variable has been largely neglected in the NLP and language model literature. In this paper, we present TimeLMs, a set of language models specialized on diachronic Twitter data. We show that a continual learning strategy contributes to enhancing Twitter-based language models{'} capacity to deal with future and out-of-distribution tweets, while making them competitive with standardized and more monolithic benchmarks. We also perform a number of qualitative analyses showing how they cope with trends and peaks in activity involving specific named entities or concept drift. TimeLMs is available at github.com/cardiffnlp/timelms.",
}
@misc{attanasio2023worth,
      title={Is It Worth the (Environmental) Cost? Limited Evidence for Temporal Adaptation via Continuous Training}, 
      author={Giuseppe Attanasio and Debora Nozza and Federico Bianchi and Dirk Hovy},
      year={2023},
      eprint={2210.07365},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
%%%Benchmark for Temporal LMs


% Domain-Adaptive Pre-Training 
@INPROCEEDINGS {yan2023af,
    author = {Y. Yan and K. Xue and X. Shi and Q. Ye and J. Liu and T. Ruan},
    booktitle = {2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
    title = {AF Adapter: Continual Pretraining for Building Chinese Biomedical Language Model},
    year = {2023},
    volume = {},
    issn = {},
    pages = {953-957},
    abstract = {Continual pretraining is a popular way of building a domain-specific pretrained language model from a general-domain language model. In spite of its high efficiency, continual pretraining suffers from catastrophic forgetting, which may harm the model’s performance in downstream tasks. To alleviate the issue, in this paper, we propose a continual pretraining method for the BERT-based model, named Attention-FFN Adapter. Its main idea is to introduce a small number of attention heads and hidden units inside each self-attention layer and feed-forward network. Furthermore, we train a domain-specific language model named AF Adapter based RoBERTa for the Chinese biomedical domain. In experiments, models are applied to downstream tasks for evaluation. The results demonstrate that with only about 17% of model parameters trained, AF Adapter achieves 0.6%, 2% gain in performance on average, compared to strong baselines. Further experimental results show that our method alleviates the catastrophic forgetting problem by 11% compared to the fine-tuning method. Code is available at https://github.com/yanyongyu/AF-Adapter.},
    keywords = {adaptation models;head;biological system modeling;buildings;natural languages;stability analysis;task analysis},
    doi = {10.1109/BIBM58861.2023.10385733},
    url = {https://doi.ieeecomputersociety.org/10.1109/BIBM58861.2023.10385733},
    publisher = {IEEE Computer Society},
    address = {Los Alamitos, CA, USA},
    month = {dec}
}

@misc{rongali2021continual,
      title={Continual Domain-Tuning for Pretrained Language Models}, 
      author={Subendhu Rongali and Abhyuday Jagannatha and Bhanu Pratap Singh Rawat and Hong Yu},
      year={2021},
      eprint={2004.02288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{gu2022ppt,
    title = "{PPT}: Pre-trained Prompt Tuning for Few-shot Learning",
    author = "Gu, Yuxian  and
      Han, Xu  and
      Liu, Zhiyuan  and
      Huang, Minlie",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.576",
    doi = "10.18653/v1/2022.acl-long.576",
    pages = "8410--8423",
    abstract = "Prompts for pre-trained language models (PLMs) have shown remarkable performance by bridging the gap between pre-training tasks and various downstream tasks. Among these methods, prompt tuning, which freezes PLMs and only tunes soft prompts, provides an efficient and effective solution for adapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to be fully explored. In our pilot experiments, we find that prompt tuning performs comparably with conventional full-model tuning when downstream data are sufficient, whereas it is much worse under few-shot learning settings, which may hinder the application of prompt tuning. We attribute this low performance to the manner of initializing soft prompts. Therefore, in this work, we propose to pre-train prompts by adding soft prompts into the pre-training stage to obtain a better initialization. We name this Pre-trained Prompt Tuning framework {``}PPT{''}. To ensure the generalization of PPT, we formulate similar classification tasks into a unified task form and pre-train soft prompts for this unified task. Extensive experiments show that tuning pre-trained prompts for downstream tasks can reach or even outperform full-model fine-tuning under both full-data and few-shot settings. Our approach is effective and efficient for using large-scale PLMs in practice.",
}
@inproceedings{ke2022continual-train,
    title = "Continual Training of Language Models for Few-Shot Learning",
    author = "Ke, Zixuan  and
      Lin, Haowei  and
      Shao, Yijia  and
      Xu, Hu  and
      Shu, Lei  and
      Liu, Bing",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.695",
    doi = "10.18653/v1/2022.emnlp-main.695",
    pages = "10205--10216",
    abstract = "Recent work on applying large language models (LMs) achieves impressive performance in many NLP applications. Adapting or posttraining an LM using an unlabeled domain corpus can produce even better performance for end-tasks in the domain. This paper proposes the problem of continually extending an LM by incrementally post-train the LM with a sequence of unlabeled domain corpora to expand its knowledge without forgetting its previous skills. The goal is to improve the few-shot end-task learning in these domains. The resulting system is called CPT (Continual PostTraining), which to our knowledge, is the first continual post-training system. Experimental results verify its effectiveness.",
}

@misc{guo2023continuous,
      title={Continuous Training and Fine-tuning for Domain-Specific Language Models in Medical Question Answering}, 
      author={Zhen Guo and Yining Hua},
      year={2023},
      eprint={2311.00204},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{gururangan2020dont,
    title = "Don{'}t Stop Pretraining: Adapt Language Models to Domains and Tasks",
    author = "Gururangan, Suchin  and
      Marasovi{\'c}, Ana  and
      Swayamdipta, Swabha  and
      Lo, Kyle  and
      Beltagy, Iz  and
      Downey, Doug  and
      Smith, Noah A.",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.740",
    doi = "10.18653/v1/2020.acl-main.740",
    pages = "8342--8360",
    abstract = "Language models pretrained on text from a wide variety of sources form the foundation of today{'}s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task{'}s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.",
}

@misc{ma2023ecomgptct,
      title={EcomGPT-CT: Continual Pre-training of E-commerce Large Language Models with Semi-structured Data}, 
      author={Shirong Ma and Shen Huang and Shulin Huang and Xiaobin Wang and Yangning Li and Hai-Tao Zheng and Pengjun Xie and Fei Huang and Yong Jiang},
      year={2023},
      eprint={2312.15696},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{han2021econet,
    title = "{ECONET}: Effective Continual Pretraining of Language Models for Event Temporal Reasoning",
    author = "Han, Rujun  and
      Ren, Xiang  and
      Peng, Nanyun",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.436",
    doi = "10.18653/v1/2021.emnlp-main.436",
    pages = "5367--5380",
    abstract = "While pre-trained language models (PTLMs) have achieved noticeable success on many NLP tasks, they still struggle for tasks that require event temporal reasoning, which is essential for event-centric applications. We present a continual pre-training approach that equips PTLMs with targeted knowledge about event temporal relations. We design self-supervised learning objectives to recover masked-out event and temporal indicators and to discriminate sentences from their corrupted counterparts (where event or temporal indicators got replaced). By further pre-training a PTLM with these objectives jointly, we reinforce its attention to event and temporal information, yielding enhanced capability on event temporal reasoning. This **E**ffective **CON**tinual pre-training framework for **E**vent **T**emporal reasoning (ECONET) improves the PTLMs{'} fine-tuning performances across five relation extraction and question answering tasks and achieves new or on-par state-of-the-art performances in most of our downstream tasks.",
}

@misc{xie2023efficient,
      title={Efficient Continual Pre-training for Building Domain Specific Large Language Models}, 
      author={Yong Xie and Karan Aggarwal and Aitzaz Ahmad},
      year={2023},
      eprint={2311.08545},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{zhou2020pre,
  title={Pre-training text-to-text transformers for concept-centric common sense},
  author={Zhou, Wangchunshu and Lee, Dong-Ho and Selvam, Ravi Kiran and Lee, Seyeon and Lin, Bill Yuchen and Ren, Xiang},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{xie2023quert,
    author = {Xie, Jian and Liang, Yidan and Liu, Jingping and Xiao, Yanghua and Wu, Baohua and Ni, Shenghua},
    title = {QUERT: Continual Pre-training of Language Model for Query Understanding in Travel Domain Search},
    year = {2023},
    isbn = {9798400701030},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3580305.3599891},
    doi = {10.1145/3580305.3599891},
    abstract = {In light of the success of the pre-trained language models (PLMs), continual pre-training of generic PLMs has been the paradigm of domain adaption. In this paper, we propose QUERT, A Continual Pre-trained Language Model for QUERy Understanding in Travel Domain Search. QUERT is jointly trained on four tailored pre-training tasks to the characteristics of query in travel domain search: Geography-aware Mask Prediction, Geohash Code Prediction, User Click Behavior Learning, and Phrase and Token Order Prediction. Performance improvement of downstream tasks and ablation experiment demonstrate the effectiveness of our proposed pre-training tasks. To be specific, the average performance of downstream tasks increases by 2.02\% and 30.93\% in supervised and unsupervised settings, respectively. To check on the improvement of QUERT to online business, we deploy QUERT and perform A/B testing on Fliggy APP. The feedback results show that QUERT increases the Unique Click-Through Rate and Page Click-Through Rate by 0.89\% and 1.03\% when applying QUERT as the encoder. Resources are available at https://github.com/hsaest/QUERT},
    booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
    pages = {5282–5291},
    numpages = {10},
    keywords = {continual pre-training, query understanding, travel domain search},
    location = {<conf-loc>, <city>Long Beach</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
    series = {KDD '23}
}

@inproceedings{zhang2023revisit,
    title = "Revisit Few-shot Intent Classification with {PLM}s: Direct Fine-tuning vs. Continual Pre-training",
    author = "Zhang, Haode  and
      Liang, Haowen  and
      Zhan, Li-Ming  and
      Wu, Xiao-Ming  and
      Lam, Albert Y.S.",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.706",
    doi = "10.18653/v1/2023.findings-acl.706",
    pages = "11105--11121",
    abstract = "We consider the task of few-shot intent detection, which involves training a deep learning model to classify utterances based on their underlying intents using only a small amount of labeled data. The current approach to address this problem is through continual pre-training, i.e., fine-tuning pre-trained language models (PLMs) on external resources (e.g., conversational corpora, public intent detection datasets, or natural language understanding datasets) before using them as utterance encoders for training an intent classifier. In this paper, we show that continual pre-training may not be essential, since the overfitting problem of PLMs on this task may not be as serious as expected. Specifically, we find that directly fine-tuning PLMs on only a handful of labeled examples already yields decent results compared to methods that employ continual pre-training, and the performance gap diminishes rapidly as the number of labeled data increases. To maximize the utilization of the limited available data, we propose a context augmentation method and leverage sequential self-distillation to boost performance. Comprehensive experiments on real-world benchmarks show that given only two or more labeled samples per class, direct fine-tuning outperforms many strong baselines that utilize external data sources for continual pre-training. The code can be found at \url{https://github.com/hdzhang-code/DFTPlus}.",
}


% domain-specific LLMs
%% overlapping references in paper "A Survey on Knowledge Distillation of Large Language Models", 
%% https://arxiv.org/abs/2402.13116
@misc{savelka2023explaining,
      title={Explaining Legal Concepts with Augmented Large Language Models (GPT-4)}, 
      author={Jaromir Savelka and Kevin D. Ashley and Morgan A. Gray and Hannes Westermann and Huihui Xu},
      year={2023},
      eprint={2306.09525},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{lai2023large,
  title={Large Language Models in Law: A Survey},
  author={Lai, Jinqi and Gan, Wensheng and Wu, Jiayang and Qi, Zhenlian and Yu, Philip S},
  journal={arXiv preprint arXiv:2312.03718},
  year={2023}
}
@article{yue2023disc,
  title={Disc-lawllm: Fine-tuning large language models for intelligent legal services},
  author={Yue, Shengbin and Chen, Wei and Wang, Siyuan and Li, Bingxuan and Shen, Chenchen and Liu, Shujun and Zhou, Yuxuan and Xiao, Yao and Yun, Song and Lin, Wei and others},
  journal={arXiv preprint arXiv:2309.11325},
  year={2023}
}
@misc{xiao2021lawformer,
      title={Lawformer: A Pre-trained Language Model for Chinese Legal Long Documents}, 
      author={Chaojun Xiao and Xueyu Hu and Zhiyuan Liu and Cunchao Tu and Maosong Sun},
      year={2021},
      eprint={2105.03887},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{lawyerllam-git,
  title={Lawyer Llama},
  author={Huang, Quzhe and Tao, Mingxu and Zhang, Chen and An, Zhenwei and Jiang, Cong and Chen, Zhibin and Wu, Zirui and Feng, Yansong},
  year={2023},
  publisher={GitHub},
  journal={GitHub repository},
  howpublished={\url{https://github.com/AndrewZhe/lawyer-llama}},
}
@inproceedings{koehn2005europarl,
    title = "{E}uroparl: A Parallel Corpus for Statistical Machine Translation",
    author = "Koehn, Philipp",
    booktitle = "Proceedings of Machine Translation Summit X: Papers",
    month = sep # " 13-15",
    year = "2005",
    address = "Phuket, Thailand",
    url = "https://aclanthology.org/2005.mtsummit-papers.11",
    pages = "79--86",
    abstract = "We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web. This corpus has found widespread use in the NLP community. Here, we focus on its acquisition and its application as training data for statistical machine translation (SMT). We trained SMT systems for 110 language pairs, which reveal interesting clues into the challenges ahead.",
}
@article{huang2023lawyer,
  title={Lawyer LLaMA Technical Report},
  author={Huang, Quzhe and Tao, Mingxu and An, Zhenwei and Zhang, Chen and Jiang, Cong and Chen, Zhibin and Wu, Zirui and Feng, Yansong},
  journal={arXiv preprint arXiv:2305.15062},
  year={2023}
}

@inproceedings{zhong2020jec,
  title={JEC-QA: a legal-domain question answering dataset},
  author={Zhong, Haoxi and Xiao, Chaojun and Tu, Cunchao and Zhang, Tianyang and Liu, Zhiyuan and Sun, Maosong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={9701--9708},
  year={2020}
}

@misc{colombo2024saullm7b,
      title={SaulLM-7B: A pioneering Large Language Model for Law}, 
      author={Pierre Colombo and Telmo Pessoa Pires and Malik Boudiaf and Dominic Culver and Rui Melo and Caio Corro and Andre F. T. Martins and Fabrizio Esposito and Vera Lúcia Raposo and Sofia Morgado and Michael Desa},
      year={2024},
      eprint={2403.03883},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{martin2024better,
      title={Better Call GPT, Comparing Large Language Models Against Lawyers}, 
      author={Lauren Martin and Nick Whitehouse and Stephanie Yiu and Lizzie Catterson and Rivindu Perera},
      year={2024},
      eprint={2401.16212},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@article{gu2021domain,
   title={Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing},
   volume={3},
   ISSN={2637-8051},
   url={http://dx.doi.org/10.1145/3458754},
   DOI={10.1145/3458754},
   number={1},
   journal={ACM Transactions on Computing for Healthcare},
   publisher={Association for Computing Machinery (ACM)},
   author={Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
   year={2021},
   month=oct, pages={1–23}
}
@article{luo2022biogpt,
   title={BioGPT: generative pre-trained transformer for biomedical text generation and mining},
   volume={23},
   ISSN={1477-4054},
   url={http://dx.doi.org/10.1093/bib/bbac409},
   DOI={10.1093/bib/bbac409},
   number={6},
   journal={Briefings in Bioinformatics},
   publisher={Oxford University Press (OUP)},
   author={Luo, Renqian and Sun, Liai and Xia, Yingce and Qin, Tao and Zhang, Sheng and Poon, Hoifung and Liu, Tie-Yan},
   year={2022},
   month=sep 
}

@article{singhal2023large,
    title={Large language models encode clinical knowledge},
    author={Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and others},
    journal={Nature},
    volume={620},
    number={7972},
    pages={172--180},
    year={2023},
    publisher={Nature Publishing Group UK London}
}
@article{Singhal2023MedPaLM2,
  author       = {Karan Singhal and
                  Tao Tu and
                  Juraj Gottweis and
                  Rory Sayres and
                  Ellery Wulczyn and
                  Le Hou and
                  Kevin Clark and
                  Stephen Pfohl and
                  Heather Cole{-}Lewis and
                  Darlene Neal and
                  Mike Schaekermann and
                  Amy Wang and
                  Mohamed Amin and
                  Sami Lachgar and
                  Philip Andrew Mansfield and
                  Sushant Prakash and
                  Bradley Green and
                  Ewa Dominowska and
                  Blaise Ag{\"{u}}era y Arcas and
                  Nenad Tomasev and
                  Yun Liu and
                  Renee Wong and
                  Christopher Semturs and
                  S. Sara Mahdavi and
                  Joelle K. Barral and
                  Dale R. Webster and
                  Gregory S. Corrado and
                  Yossi Matias and
                  Shekoofeh Azizi and
                  Alan Karthikesalingam and
                  Vivek Natarajan},
  title        = {Towards Expert-Level Medical Question Answering with Large Language
                  Models},
  journal      = {CoRR},
  volume       = {abs/2305.09617},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.09617},
  doi          = {10.48550/ARXIV.2305.09617},
  eprinttype    = {arXiv},
  eprint       = {2305.09617},
  timestamp    = {Tue, 11 Jul 2023 17:14:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2305-09617.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{bao2023discmedllm,
      title={DISC-MedLLM: Bridging General Large Language Models and Real-World Medical Consultation}, 
      author={Zhijie Bao and Wei Chen and Shengze Xiao and Kuang Ren and Jiaao Wu and Cheng Zhong and Jiajie Peng and Xuanjing Huang and Zhongyu Wei},
      year={2023},
      eprint={2308.14346},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{jeblick2022chatgpt,
      title={ChatGPT Makes Medicine Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports}, 
      author={Katharina Jeblick and Balthasar Schachtner and Jakob Dexl and Andreas Mittermeier and Anna Theresa Stüber and Johanna Topalis and Tobias Weber and Philipp Wesp and Bastian Sabel and Jens Ricke and Michael Ingrisch},
      year={2022},
      eprint={2212.14882},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article {chen2023utility,
	author = {Shan Chen and Benjamin H. Kann and Michael B. Foote and Hugo JWL Aerts and Guergana K. Savova and Raymond H. Mak and Danielle S. Bitterman},
	title = {The utility of ChatGPT for cancer treatment information},
	elocation-id = {2023.03.16.23287316},
	year = {2023},
	doi = {10.1101/2023.03.16.23287316},
	publisher = {Cold Spring Harbor Laboratory Press},
	abstract = {The use of large language models (LLMs) such as ChatGPT for medical question-answering is becoming increasingly popular. However, there are concerns that these models may generate and amplify medical misinformation. Because cancer patients frequently seek to educate themselves through online resources, some individuals will likely use ChatGPT to obtain cancer treatment information. This study evaluated the performance and robustness of ChatGPT in providing breast, prostate, and lung cancer treatment recommendations that align with National Comprehensive Cancer Network (NCCN) guidelines. Four prompt templates were created to explore how differences in how the query is posed impacts response. ChatGPT output was scored by 3 oncologists and a 4th oncologist adjudicated in cases of disagreement. ChatGPT provided at least one NCCN-concordant recommendation for 102/104 (98\%) prompts. However, 35/102 (34.3\%) of these also included a recommendation that was at least partially non-concordant with NCCN guidelines. Responses varied based on prompt type. In conclusion, ChatGPT did not perform well at reliably and robustly providing cancer treatment recommendations. Patients and clinicians should be aware of the limitations of ChatGPT and similar technologies for self-education.Competing Interest StatementThe authors have declared no competing interest.Funding StatementThe authors thank the Woods Foundation for their generous support of this work.Author DeclarationsI confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.YesI confirm that all necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived, and that any patient/participant/sample identifiers included were not known to anyone (e.g., hospital staff, patients or participants themselves) outside the research group so cannot be used to identify individuals.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).YesI have followed all appropriate research reporting guidelines, such as any relevant EQUATOR Network research reporting checklist(s) and other pertinent material, if applicable.YesAll data produced are available online at https://github.com/AIM-Harvard/ChatGPT_NCCN https://github.com/AIM-Harvard/ChatGPT_NCCN},
	URL = {https://www.medrxiv.org/content/early/2023/03/23/2023.03.16.23287316},
	eprint = {https://www.medrxiv.org/content/early/2023/03/23/2023.03.16.23287316.full.pdf},
	journal = {medRxiv}
}
@inproceedings{lo2020s2orc,
    title = "{S}2{ORC}: The Semantic Scholar Open Research Corpus",
    author = "Lo, Kyle  and
      Wang, Lucy Lu  and
      Neumann, Mark  and
      Kinney, Rodney  and
      Weld, Daniel",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.447",
    doi = "10.18653/v1/2020.acl-main.447",
    pages = "4969--4983",
    abstract = "We introduce S2ORC, a large corpus of 81.1M English-language academic papers spanning many academic disciplines. The corpus consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers. Full text is annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. In S2ORC, we aggregate papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date. We hope this resource will facilitate research and development of tools and tasks for text mining over academic text.",
}
@article{hendryckstest2021,
  title={Measuring Massive Multitask Language Understanding},
  author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
  journal={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2021}
}
@article{li2023cmmlu,
  title={Cmmlu: Measuring massive multitask language understanding in chinese},
  author={Li, Haonan and Zhang, Yixuan and Koto, Fajri and Yang, Yifei and Zhao, Hai and Gong, Yeyun and Duan, Nan and Baldwin, Timothy},
  journal={arXiv preprint arXiv:2306.09212},
  year={2023}
}
@misc{du2023chinesellama2,
  author = {Zefeng Du, Minghao Wu, Longyue Wang},
  title = {Chinese-Llama-2},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/longyuewangdcu/Chinese-Llama-2}}
}
@misc{liu2023benchmarking,
      title={Benchmarking Large Language Models on CMExam -- A Comprehensive Chinese Medical Exam Dataset}, 
      author={Junling Liu and Peilin Zhou and Yining Hua and Dading Chong and Zhongyu Tian and Andrew Liu and Helin Wang and Chenyu You and Zhenhua Guo and Lei Zhu and Michael Lingzhi Li},
      year={2023},
      eprint={2306.03030},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{wang2023cmb,
  title={Cmb: A comprehensive medical benchmark in chinese},
  author={Wang, Xidong and Chen, Guiming Hardy and Song, Dingjie and Zhang, Zhiyi and Chen, Zhihong and Xiao, Qingying and Jiang, Feng and Li, Jianquan and Wan, Xiang and Wang, Benyou and others},
  journal={arXiv preprint arXiv:2308.08833},
  year={2023}
}
@article{sha2021wudao,
    author = {Yuan, Sha and Zhao, Hanyu and Du, Zhengxiao and Ding, Ming and Liu, Xiao and Cen, Yukuo and Zou, Xu and Yang, Zhilin and Tang, Jie},
    year = {2021},
    month = {06},
    pages = {},
    title = {WuDaoCorpora: A Super Large-scale Chinese Corpora for Pre-training Language Models},
    volume = {2},
    journal = {AI Open},
    doi = {10.1016/j.aiopen.2021.06.001}
}

@article{zhu2023promptcblue,
  title={PromptCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain},
  author={Zhu, Wei and Wang, Xiaoling and Zheng, Huanran and Chen, Mosha and Tang, Buzhou},
  journal={arXiv preprint arXiv:2310.14151},
  year={2023}
}


@article{wu2023pmc,
  title={Pmc-llama: Towards building open-source language models for medicine},
  author={Wu, Chaoyi and Lin, Weixiong and Zhang, Xiaoman and Zhang, Ya and Wang, Yanfeng and Xie, Weidi},
  journal={arXiv preprint arXiv:2305.10415},
  volume={6},
  year={2023}
}
@misc{together2023redpajama,
  author = {Together Computer},
  title = {RedPajama: an Open Dataset for Training Large Language Models},
  year = {2023},
  url = {https://github.com/togethercomputer/RedPajama-Data}
}


@article{luo2023biomedgpt,
  title={Biomedgpt: Open multimodal generative pre-trained transformer for biomedicine},
  author={Luo, Yizhen and Zhang, Jiahuan and Fan, Siqi and Yang, Kai and Wu, Yushuai and Qiao, Mu and Nie, Zaiqing},
  journal={arXiv preprint arXiv:2308.09442},
  year={2023}
}


@inproceedings{zhang2023huatuogpt,
    title = "{H}uatuo{GPT}, Towards Taming Language Model to Be a Doctor",
    author = "Zhang, Hongbo  and
      Chen, Junying  and
      Jiang, Feng  and
      Yu, Fei  and
      Chen, Zhihong  and
      Chen, Guiming  and
      Li, Jianquan  and
      Wu, Xiangbo  and
      Zhiyi, Zhang  and
      Xiao, Qingying  and
      Wan, Xiang  and
      Wang, Benyou  and
      Li, Haizhou",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.725",
    doi = "10.18653/v1/2023.findings-emnlp.725",
    pages = "10859--10885",
    abstract = "In this paper, we present HuatuoGPT, a Large Language Model (LLM) for medical consultation. The core recipe of HuatuoGPT is to leverage both distilled data from **ChatGPT** and real-world data from **doctors** in the supervised fine-tuning stage. This is not only because purely using **ChatGPT**-distilled data might cause {`}model collapse{'}, but also because real-world data from **doctors** would be complementary to **ChatGPT**-distilled data. The responses from ChatGPT are usually detailed, well-presented, fluent, and instruction-followed, but it cannot perform like a doctor in many aspects, e.g. for interactive diagnosis. Therefore, the extra doctors{'} data could tame a distilled language model to perform like doctors. To synergize the strengths of both data sources, we introduce RLMF (Reinforcement Learning from Mixed Feedback) where a reward model is trained to align the language model with the merits that both sources (ChatGPT and doctors) bring. Experimental results (in GPT-4 evaluation, human evaluation, and medical benchmark datasets) demonstrate that HuatuoGPT achieves state-of-the-art results in performing medical consultation among open-source LLMs. It is worth noting that by using additional real-world data and RLMF, the distilled language model (i.e., HuatuoGPT) outperforms its teacher model (i.e., ChatGPT) in most cases.",
}


@article{xiong2023doctorglm,
  title={Doctorglm: Fine-tuning your chinese doctor is not a herculean task},
  author={Xiong, Honglin and Wang, Sheng and Zhu, Yitao and Zhao, Zihao and Liu, Yuxiao and Wang, Qian and Shen, Dinggang},
  journal={arXiv preprint arXiv:2304.01097},
  year={2023}
}


@article{zhang2023alpacare,
  title={Alpacare: Instruction-tuned large language models for medical application},
  author={Zhang, Xinlu and Tian, Chenxin and Yang, Xianjun and Chen, Lichang and Li, Zekun and Petzold, Linda Ruth},
  journal={arXiv preprint arXiv:2310.14558},
  year={2023}
}


@article{wang2023huatuo,
  title={Huatuo: Tuning llama model with chinese medical knowledge},
  author={Wang, Haochun and Liu, Chi and Xi, Nuwa and Qiang, Zewen and Zhao, Sendong and Qin, Bing and Liu, Ting},
  journal={arXiv preprint arXiv:2304.06975},
  year={2023}
}


@article{li2023chatdoctor,
  title={ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge},
  author={Li, Yunxiang and Li, Zihan and Zhang, Kai and Dan, Ruilong and Jiang, Steve and Zhang, You},
  journal={Cureus},
  volume={15},
  number={6},
  year={2023},
  publisher={Cureus}
}


@article{Han2023MedAlpaca,
  author       = {Tianyu Han and
                  Lisa C. Adams and
                  Jens{-}Michalis Papaioannou and
                  Paul Grundmann and
                  Tom Oberhauser and
                  Alexander L{\"{o}}ser and
                  Daniel Truhn and
                  Keno K. Bressem},
  title        = {MedAlpaca - An Open-Source Collection of Medical Conversational {AI}
                  Models and Training Data},
  journal      = {CoRR},
  volume       = {abs/2304.08247},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2304.08247},
  doi          = {10.48550/ARXIV.2304.08247},
  eprinttype    = {arXiv},
  eprint       = {2304.08247},
  timestamp    = {Fri, 21 Apr 2023 11:01:56 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2304-08247.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{Chen2023HuatuoGPTII,
  author       = {Junying Chen and
                  Xidong Wang and
                  Anningzhe Gao and
                  Feng Jiang and
                  Shunian Chen and
                  Hongbo Zhang and
                  Dingjie Song and
                  Wenya Xie and
                  Chuyi Kong and
                  Jianquan Li and
                  Xiang Wan and
                  Haizhou Li and
                  Benyou Wang},
  title        = {HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs},
  journal      = {CoRR},
  volume       = {abs/2311.09774},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2311.09774},
  doi          = {10.48550/ARXIV.2311.09774},
  eprinttype    = {arXiv},
  eprint       = {2311.09774},
  timestamp    = {Tue, 21 Nov 2023 13:55:21 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2311-09774.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{shah2023zero,
      title={Zero is Not Hero Yet: Benchmarking Zero-Shot Performance of LLMs for Financial Tasks}, 
      author={Agam Shah and Sudheer Chava},
      year={2023},
      eprint={2305.16633},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Xue2023WeaverBird,
  author       = {Siqiao Xue and
                  Fan Zhou and
                  Yi Xu and
                  Hongyu Zhao and
                  Shuo Xie and
                  Qingyang Dai and
                  Caigao Jiang and
                  James Zhang and
                  Jun Zhou and
                  Dacheng Xiu and
                  Hongyuan Mei},
  title        = {WeaverBird: Empowering Financial Decision-Making with Large Language Model, Knowledge Base, and Search Engine},
  journal      = {CoRR},
  volume       = {abs/2308.05361},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2308.05361},
  doi          = {10.48550/ARXIV.2308.05361},
  eprinttype    = {arXiv},
  eprint       = {2308.05361},
  timestamp    = {Tue, 17 Oct 2023 15:35:32 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2308-05361.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{dettmers2023qlora,
  title={QLoRA: Efficient Finetuning of Quantized LLMs},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2305.14314},
  year={2023}
}
@misc{araci2019finbert,
      title={FinBERT: Financial Sentiment Analysis with Pre-trained Language Models}, 
      author={Dogu Araci},
      year={2019},
      eprint={1908.10063},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{Wu2023BloombergGPT,
  author       = {Shijie Wu and
                  Ozan Irsoy and
                  Steven Lu and
                  Vadim Dabravolski and
                  Mark Dredze and
                  Sebastian Gehrmann and
                  Prabhanjan Kambadur and
                  David S. Rosenberg and
                  Gideon Mann},
  title        = {BloombergGPT: {A} Large Language Model for Finance},
  journal      = {CoRR},
  volume       = {abs/2303.17564},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2303.17564},
  doi          = {10.48550/ARXIV.2303.17564},
  eprinttype    = {arXiv},
  eprint       = {2303.17564},
  timestamp    = {Fri, 06 Oct 2023 19:24:15 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2303-17564.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{chen2023disc,
  title={Disc-finllm: A chinese financial large language model based on multiple experts fine-tuning},
  author={Chen, Wei and Wang, Qiushi and Long, Zefei and Zhang, Xianyin and Lu, Zhongtian and Li, Bingxuan and Wang, Siyuan and Xu, Jiarong and Bai, Xiang and Huang, Xuanjing and others},
  journal={arXiv preprint arXiv:2310.15205},
  year={2023}
}

@article{Lu2023BBTFin,
  author       = {Dakuan Lu and
                  Hengkui Wu and
                  Jiaqing Liang and
                  Yipei Xu and
                  Qianyu He and
                  Yipeng Geng and
                  Mengkun Han and
                  Yingsi Xin and
                  Yanghua Xiao},
  title        = {BBT-Fin: Comprehensive Construction of Chinese Financial Domain Pre-trained
                  Language Model, Corpus and Benchmark},
  journal      = {CoRR},
  volume       = {abs/2302.09432},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2302.09432},
  doi          = {10.48550/ARXIV.2302.09432},
  eprinttype    = {arXiv},
  eprint       = {2302.09432},
  timestamp    = {Tue, 28 Feb 2023 08:39:45 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2302-09432.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{Yang2023InvestLM,
  author       = {Yi Yang and
                  Yixuan Tang and
                  Kar Yan Tam},
  title        = {InvestLM: {A} Large Language Model for Investment using Financial
                  Domain Instruction Tuning},
  journal      = {CoRR},
  volume       = {abs/2309.13064},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.13064},
  doi          = {10.48550/ARXIV.2309.13064},
  eprinttype    = {arXiv},
  eprint       = {2309.13064},
  timestamp    = {Mon, 16 Oct 2023 18:00:27 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2309-13064.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{Xie2023PIXIU,
  author       = {Qianqian Xie and
                  Weiguang Han and
                  Xiao Zhang and
                  Yanzhao Lai and
                  Min Peng and
                  Alejandro Lopez{-}Lira and
                  Jimin Huang},
  title        = {{PIXIU:} {A} Large Language Model, Instruction Data and Evaluation
                  Benchmark for Finance},
  journal      = {CoRR},
  volume       = {abs/2306.05443},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2306.05443},
  doi          = {10.48550/ARXIV.2306.05443},
  eprinttype    = {arXiv},
  eprint       = {2306.05443},
  timestamp    = {Wed, 14 Jun 2023 13:17:00 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2306-05443.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{Wang2023FinGPT,
  author       = {Neng Wang and
                  Hongyang Yang and
                  Christina Dan Wang},
  title        = {FinGPT: Instruction Tuning Benchmark for Open-Source Large Language
                  Models in Financial Datasets},
  journal      = {CoRR},
  volume       = {abs/2310.04793},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.04793},
  doi          = {10.48550/ARXIV.2310.04793},
  eprinttype    = {arXiv},
  eprint       = {2310.04793},
  timestamp    = {Fri, 20 Oct 2023 12:04:38 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2310-04793.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Zhang2023xuanyuan,
    author = {Zhang, Xuanyu and Yang, Qing},
    title = {XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters},
    year = {2023},
    isbn = {9798400701245},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3583780.3615285},
    doi = {10.1145/3583780.3615285},
    abstract = {Recently, with the popularity of ChatGPT, large-scale language models have experienced rapid development. However, there is a scarcity of open-sourced chat models specifically designed for the Chinese language, especially in the field of Chinese finance, at the scale of hundreds of billions. To address this gap, we introduce XuanYuan 2.0, the largest Chinese chat model to date, built upon the BLOOM-176B architecture. Additionally, we propose a novel training method called hybrid-tuning to mitigate catastrophic forgetting. By integrating general and domain-specific knowledge, as well as combining the stages of pre-training and fine-tuning, XuanYuan 2.0 is capable of providing accurate and contextually appropriate responses in the Chinese financial domain.},
    booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
    pages = {4435–4439},
    numpages = {5},
    keywords = {ChatGPT, Chinese, finance, large language model},
    location = {<conf-loc>, <city>Birmingham</city>, <country>United Kingdom</country>, </conf-loc>},
    series = {CIKM '23}
}

@misc{li2023cfgpt,
      title={CFGPT: Chinese Financial Assistant with Large Language Model}, 
      author={Jiangtong Li and Yuxuan Bian and Guoxuan Wang and Yang Lei and Dawei Cheng and Zhijun Ding and Changjun Jiang},
      year={2023},
      eprint={2309.10654},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Taylor2022Galactica,
  author       = {Ross Taylor and
                  Marcin Kardas and
                  Guillem Cucurull and
                  Thomas Scialom and
                  Anthony Hartshorn and
                  Elvis Saravia and
                  Andrew Poulton and
                  Viktor Kerkez and
                  Robert Stojnic},
  title        = {Galactica: {A} Large Language Model for Science},
  journal      = {CoRR},
  volume       = {abs/2211.09085},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2211.09085},
  doi          = {10.48550/ARXIV.2211.09085},
  eprinttype    = {arXiv},
  eprint       = {2211.09085},
  timestamp    = {Wed, 23 Nov 2022 18:03:56 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2211-09085.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Yang2023PLLaMa,
  author       = {Xianjun Yang and
                  Junfeng Gao and
                  Wenxin Xue and
                  Erik Alexandersson},
  title        = {PLLaMa: An Open-source Large Language Model for Plant Science},
  journal      = {CoRR},
  volume       = {abs/2401.01600},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2401.01600},
  doi          = {10.48550/ARXIV.2401.01600},
  eprinttype    = {arXiv},
  eprint       = {2401.01600},
  timestamp    = {Mon, 15 Jan 2024 16:37:16 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2401-01600.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{Yin2023FORGE,
  author       = {Junqi Yin and
                  Sajal Dash and
                  Feiyi Wang and
                  Mallikarjun Shankar},
  editor       = {Dorian Arnold and
                  Rosa M. Badia and
                  Kathryn M. Mohror},
  title        = {{FORGE:} Pre-Training Open Foundation Models for Science},
  booktitle    = {Proceedings of the International Conference for High Performance Computing,
                  Networking, Storage and Analysis, {SC} 2023, Denver, CO, USA, November
                  12-17, 2023},
  pages        = {81:1--81:13},
  publisher    = {{ACM}},
  year         = {2023},
  url          = {https://doi.org/10.1145/3581784.3613215},
  doi          = {10.1145/3581784.3613215},
  timestamp    = {Tue, 28 Nov 2023 20:06:11 +0100},
  biburl       = {https://dblp.org/rec/conf/sc/YinDWS23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{Xie2023DARWIN,
  author       = {Tong Xie and
                  Yuwei Wan and
                  Wei Huang and
                  Zhenyu Yin and
                  Yixuan Liu and
                  Shaozhou Wang and
                  Qingyuan Linghu and
                  Chunyu Kit and
                  Clara Grazian and
                  Wenjie Zhang and
                  Imran Razzak and
                  Bram Hoex},
  title        = {{DARWIN} Series: Domain Specific Large Language Models for Natural
                  Science},
  journal      = {CoRR},
  volume       = {abs/2308.13565},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2308.13565},
  doi          = {10.48550/ARXIV.2308.13565},
  eprinttype    = {arXiv},
  eprint       = {2308.13565},
  timestamp    = {Fri, 01 Sep 2023 14:25:01 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2308-13565.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Zhang2024SciGLM,
  author       = {Dan Zhang and
                  Ziniu Hu and
                  Sining Zhoubian and
                  Zhengxiao Du and
                  Kaiyu Yang and
                  Zihan Wang and
                  Yisong Yue and
                  Yuxiao Dong and
                  Jie Tang},
  title        = {SciGLM: Training Scientific Language Models with Self-Reflective Instruction
                  Annotation and Tuning},
  journal      = {CoRR},
  volume       = {abs/2401.07950},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2401.07950},
  doi          = {10.48550/ARXIV.2401.07950},
  eprinttype    = {arXiv},
  eprint       = {2401.07950},
  timestamp    = {Thu, 01 Feb 2024 15:35:36 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2401-07950.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{Azerbayev2023LLEMMA,
  author       = {Zhangir Azerbayev and
                  Hailey Schoelkopf and
                  Keiran Paster and
                  Marco Dos Santos and
                  Stephen McAleer and
                  Albert Q. Jiang and
                  Jia Deng and
                  Stella Biderman and
                  Sean Welleck},
  title        = {Llemma: An Open Language Model For Mathematics},
  journal      = {CoRR},
  volume       = {abs/2310.10631},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.10631},
  doi          = {10.48550/ARXIV.2310.10631},
  eprinttype    = {arXiv},
  eprint       = {2310.10631},
  timestamp    = {Wed, 25 Oct 2023 17:11:46 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2310-10631.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{Yu2023Outcome,
  author       = {Fei Yu and
                  Anningzhe Gao and
                  Benyou Wang},
  title        = {Outcome-supervised Verifiers for Planning in Mathematical Reasoning},
  journal      = {CoRR},
  volume       = {abs/2311.09724},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2311.09724},
  doi          = {10.48550/ARXIV.2311.09724},
  eprinttype    = {arXiv},
  eprint       = {2311.09724},
  timestamp    = {Tue, 21 Nov 2023 13:55:21 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2311-09724.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{luo2023wizardmath,
  title={Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct},
  author={Luo, Haipeng and Sun, Qingfeng and Xu, Can and Zhao, Pu and Lou, Jianguang and Tao, Chongyang and Geng, Xiubo and Lin, Qingwei and Chen, Shifeng and Zhang, Dongmei},
  journal={arXiv preprint arXiv:2308.09583},
  year={2023}
}


@article{yue2023mammoth,
  title={Mammoth: Building math generalist models through hybrid instruction tuning},
  author={Yue, Xiang and Qu, Xingwei and Zhang, Ge and Fu, Yao and Huang, Wenhao and Sun, Huan and Su, Yu and Chen, Wenhu},
  journal={arXiv preprint arXiv:2309.05653},
  year={2023}
}


@inproceedings{
Gou2023ToRA,
title={To{RA}: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving},
author={Zhibin Gou and Zhihong Shao and Yeyun Gong and yelong shen and Yujiu Yang and Minlie Huang and Nan Duan and Weizhu Chen},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=Ep0TtjVoap}
}


@article{Gao2023GLLAVA,
  author       = {Jiahui Gao and
                  Renjie Pi and
                  Jipeng Zhang and
                  Jiacheng Ye and
                  Wanjun Zhong and
                  Yufei Wang and
                  Lanqing Hong and
                  Jianhua Han and
                  Hang Xu and
                  Zhenguo Li and
                  Lingpeng Kong},
  title        = {G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language
                  Model},
  journal      = {CoRR},
  volume       = {abs/2312.11370},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2312.11370},
  doi          = {10.48550/ARXIV.2312.11370},
  eprinttype    = {arXiv},
  eprint       = {2312.11370},
  timestamp    = {Tue, 16 Jan 2024 11:57:42 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2312-11370.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{Nguyen2023AstroLLaMA,
  author       = {Tuan Dung Nguyen and
                  Yuan{-}Sen Ting and
                  Ioana Ciuca and
                  Charlie O'Neill and
                  Zechang Sun and
                  Maja Jablonska and
                  Sandor Kruk and
                  Ernest Perkowski and
                  Jack W. Miller and
                  Jason Li and
                  Josh Peek and
                  Kartheik Iyer and
                  Tomasz R{\'{o}}zanski and
                  Pranav Khetarpal and
                  Sharaf Zaman and
                  David Brodrick and
                  Sergio J. Rodr{\'{\i}}guez M{\'{e}}ndez and
                  Thang Bui and
                  Alyssa Goodman and
                  Alberto Accomazzi and
                  Jill P. Naiman and
                  Jesse Cranney and
                  Kevin Schawinski and
                  UniverseTBD},
  title        = {AstroLLaMA: Towards Specialized Foundation Models in Astronomy},
  journal      = {CoRR},
  volume       = {abs/2309.06126},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.06126},
  doi          = {10.48550/ARXIV.2309.06126},
  eprinttype    = {arXiv},
  eprint       = {2309.06126},
  timestamp    = {Thu, 25 Jan 2024 15:27:20 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2309-06126.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{Perkowski2024AstroLLaMAChat,
  author       = {Ernest Perkowski and
                  Rui Pan and
                  Tuan Dung Nguyen and
                  Yuan{-}Sen Ting and
                  Sandor Kruk and
                  Tong Zhang and
                  Charlie O'Neill and
                  Maja Jablonska and
                  Zechang Sun and
                  Michael J. Smith and
                  Huiling Liu and
                  Kevin Schawinski and
                  Kartheik Iyer and
                  Ioana Ciuca and
                  UniverseTBD},
  title        = {AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse Datasets},
  journal      = {CoRR},
  volume       = {abs/2401.01916},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2401.01916},
  doi          = {10.48550/ARXIV.2401.01916},
  eprinttype    = {arXiv},
  eprint       = {2401.01916},
  timestamp    = {Mon, 29 Jan 2024 17:03:26 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2401-01916.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{
Zhao2023GIMLET,
title={{GIMLET}: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning},
author={Haiteng Zhao and Shengchao Liu and Chang Ma and Hannan Xu and Jie Fu and Zhi-Hong Deng and Lingpeng Kong and Qi Liu},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=Tt6DrRCgJV}
}


@article{Rubungo2023LLM-Prop,
  author       = {Andre Niyongabo Rubungo and
                  Craig Arnold and
                  Barry P. Rand and
                  Adji Bousso Dieng},
  title        = {LLM-Prop: Predicting Physical And Electronic Properties Of Crystalline
                  Solids From Their Text Descriptions},
  journal      = {CoRR},
  volume       = {abs/2310.14029},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.14029},
  doi          = {10.48550/ARXIV.2310.14029},
  eprinttype    = {arXiv},
  eprint       = {2310.14029},
  timestamp    = {Fri, 27 Oct 2023 15:01:27 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2310-14029.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{Cao2023InstructMol,
  author       = {He Cao and
                  Zijing Liu and
                  Xingyu Lu and
                  Yuan Yao and
                  Yu Li},
  title        = {InstructMol: Multi-Modal Integration for Building a Versatile and
                  Reliable Molecular Assistant in Drug Discovery},
  journal      = {CoRR},
  volume       = {abs/2311.16208},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2311.16208},
  doi          = {10.48550/ARXIV.2311.16208},
  eprinttype    = {arXiv},
  eprint       = {2311.16208},
  timestamp    = {Wed, 29 Nov 2023 17:42:08 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2311-16208.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{
Abdine2023Prot2Text,
title={Prot2Text: Multimodal Protein{\textquoteright}s Function Generation with {GNN}s and Transformers},
author={Hadi Abdine and Michail Chatzianastasis and Costas Bouyioukos and Michalis Vazirgiannis},
booktitle={Deep Generative Models for Health Workshop NeurIPS 2023},
year={2023},
url={https://openreview.net/forum?id=EJ7YNgWYFj}
}


@article{xTrimoPGLM2024Chen,
  author       = {Bo Chen and
                  Xingyi Cheng and
                  Pan Li and
                  Yangli{-}ao Geng and
                  Jing Gong and
                  Shen Li and
                  Zhilei Bei and
                  Xu Tan and
                  Boyan Wang and
                  Xin Zeng and
                  Chiming Liu and
                  Aohan Zeng and
                  Yuxiao Dong and
                  Jie Tang and
                  Le Song},
  title        = {xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering
                  the Language of Protein},
  journal      = {CoRR},
  volume       = {abs/2401.06199},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2401.06199},
  doi          = {10.48550/ARXIV.2401.06199},
  eprinttype    = {arXiv},
  eprint       = {2401.06199},
  timestamp    = {Fri, 26 Jan 2024 12:45:12 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2401-06199.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{roberts2023gpt4geo,
      title={GPT4GEO: How a Language Model Sees the World's Geography},
      author={Jonathan Roberts and Timo Lüddecke and Sowmen Das and Kai Han and Samuel Albanie},
      year={2023},
      eprint={2306.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{lin2023geogalactica,
      title={GeoGalactica: A Scientific Large Language Model in Geoscience},
      author={Zhouhan Lin and Cheng Deng and Le Zhou and Tianhang Zhang and Yi Xu and Yutong Xu and Zhongmou He and Yuanyuan Shi and Beiya Dai and Yunchong Song and Boyi Zeng and Qiyuan Chen and Tao Shi and Tianyu Huang and Yiwei Xu and Shu Wang and Luoyi Fu and Weinan Zhang and Junxian He and Chao Ma and Yunqiang Zhu and Xinbing Wang and Chenghu Zhou},
      year={2023},
      eprint={2401.00434},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{wang2023nearrealtime,
      title={Near-real-time Earthquake-induced Fatality Estimation using Crowdsourced Data and Large-Language Models},
      author={Chenguang Wang and Davis Engler and Xuechun Li and James Hou and David J. Wald and Kishor Jaiswal and Susu Xu},
      year={2023},
      eprint={2312.03755},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{Bi2023OCEANGPT,
  author       = {Zhen Bi and
                  Ningyu Zhang and
                  Yida Xue and
                  Yixin Ou and
                  Daxiong Ji and
                  Guozhou Zheng and
                  Huajun Chen},
  title        = {OceanGPT: {A} Large Language Model for Ocean Science Tasks},
  journal      = {CoRR},
  volume       = {abs/2310.02031},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.02031},
  doi          = {10.48550/ARXIV.2310.02031},
  eprinttype    = {arXiv},
  eprint       = {2310.02031},
  timestamp    = {Thu, 19 Oct 2023 13:12:52 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2310-02031.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{Zheng2023MarineGPT,
  author       = {Ziqiang Zheng and
                  Jipeng Zhang and
                  Tuan{-}Anh Vu and
                  Shizhe Diao and
                  Yue Him Wong Tim and
                  Sai{-}Kit Yeung},
  title        = {MarineGPT: Unlocking Secrets of Ocean to the Public},
  journal      = {CoRR},
  volume       = {abs/2310.13596},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.13596},
  doi          = {10.48550/ARXIV.2310.13596},
  eprinttype    = {arXiv},
  eprint       = {2310.13596},
  timestamp    = {Fri, 27 Oct 2023 12:21:19 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2310-13596.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{liu2024radiologygpt,
      title={Radiology-GPT: A Large Language Model for Radiology}, 
      author={Zhengliang Liu and Aoxiao Zhong and Yiwei Li and Longtao Yang and Chao Ju and Zihao Wu and Chong Ma and Peng Shu and Cheng Chen and Sekeun Kim and Haixing Dai and Lin Zhao and Lichao Sun and Dajiang Zhu and Jun Liu and Wei Liu and Dinggang Shen and Xiang Li and Quanzheng Li and Tianming Liu},
      year={2024},
      eprint={2306.08666},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{deng2023learning,
      title={K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization}, 
      author={Cheng Deng and Tianhang Zhang and Zhongmou He and Yi Xu and Qiyuan Chen and Yuanyuan Shi and Luoyi Fu and Weinan Zhang and Xinbing Wang and Chenghu Zhou and Zhouhan Lin and Junxian He},
      year={2023},
      eprint={2306.05064},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

%%% Vertical LLM for Code 
@misc{sun2024survey,
      title={A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond}, 
      author={Qiushi Sun and Zhirui Chen and Fangzhi Xu and Kanzhi Cheng and Chang Ma and Zhangyue Yin and Jianing Wang and Chengcheng Han and Renyu Zhu and Shuai Yuan and Qipeng Guo and Xipeng Qiu and Pengcheng Yin and Xiaoli Li and Fei Yuan and Lingpeng Kong and Xiang Li and Zhiyong Wu},
      year={2024},
      eprint={2403.14734},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}
@misc{chen2021evaluating,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{chai2023erniecode,
    title = "{ERNIE}-Code: Beyond {E}nglish-Centric Cross-lingual Pretraining for Programming Languages",
    author = "Chai, Yekun  and
      Wang, Shuohuan  and
      Pang, Chao  and
      Sun, Yu  and
      Tian, Hao  and
      Wu, Hua",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.676",
    doi = "10.18653/v1/2023.findings-acl.676",
    pages = "10628--10650",
    abstract = "Software engineers working with the same programming language (PL) may speak different natural languages (NLs) and vice versa, erecting huge barriers to communication and working efficiency. Recent studies have demonstrated the effectiveness of generative pre-training in computer programs, yet they are always English-centric. In this work, we step towards bridging the gap between multilingual NLs and multilingual PLs for large language models (LLMs). We release ERNIE-Code, a unified pre-trained language model for 116 NLs and 6 PLs. We employ two methods for universal cross-lingual pre-training: span-corruption language modeling that learns patterns from monolingual NL or PL; and pivot-based translation language modeling that relies on parallel data of many NLs and PLs. Extensive results show that ERNIE-Code outperforms previous multilingual LLMs for PL or NL across a wide range of end tasks of code intelligence, including multilingual code-to-text, text-to-code, code-to-code, and text-to-text generation. We further show its advantage of zero-shot prompting on multilingual code summarization and text-to-text translation. We release our code and pre-trained checkpoints.",
}
@inproceedings{kanade2020learning,
  title={Learning and evaluating contextual embedding of source code},
  author={Kanade, Aditya and Maniatis, Petros and Balakrishnan, Gogul and Shi, Kensen},
  booktitle={International conference on machine learning},
  pages={5110--5121},
  year={2020},
  organization={PMLR}
}
@article{feng2020codebert,
  title={Codebert: A pre-trained model for programming and natural languages},
  author={Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others},
  journal={arXiv preprint arXiv:2002.08155},
  year={2020}
}
@article{moradidakhel2023github,
    title = {GitHub Copilot AI pair programmer: Asset or Liability?},
    journal = {Journal of Systems and Software},
    volume = {203},
    pages = {111734},
    year = {2023},
    issn = {0164-1212},
    doi = {https://doi.org/10.1016/j.jss.2023.111734},
    url = {https://www.sciencedirect.com/science/article/pii/S0164121223001292},
    author = {Arghavan {Moradi Dakhel} and Vahid Majdinasab and Amin Nikanjam and Foutse Khomh and Michel C. Desmarais and Zhen Ming (Jack) Jiang},
    keywords = {Code completion, Language model, GitHub copilot, Testing},
    abstract = {Automatic program synthesis is a long-lasting dream in software engineering. Recently, a promising Deep Learning (DL) based solution, called Copilot, has been proposed by OpenAI and Microsoft as an industrial product. Although some studies evaluate the correctness of Copilot solutions and report its issues, more empirical evaluations are necessary to understand how developers can benefit from it effectively. In this paper, we study the capabilities of Copilot in two different programming tasks: (i) generating (and reproducing) correct and efficient solutions for fundamental algorithmic problems, and (ii) comparing Copilot’s proposed solutions with those of human programmers on a set of programming tasks. For the former, we assess the performance and functionality of Copilot in solving selected fundamental problems in computer science, like sorting and implementing data structures. In the latter, a dataset of programming problems with human-provided solutions is used. The results show that Copilot is capable of providing solutions for almost all fundamental algorithmic problems, however, some solutions are buggy and non-reproducible. Moreover, Copilot has some difficulties in combining multiple methods to generate a solution. Comparing Copilot to humans, our results show that the correct ratio of humans’ solutions is greater than Copilot’s suggestions, while the buggy solutions generated by Copilot require less effort to be repaired. Based on our findings, if Copilot is used by expert developers in software projects, it can become an asset since its suggestions could be comparable to humans’ contributions in terms of quality. However, Copilot can become a liability if it is used by novice developers who may fail to filter its buggy or non-optimal solutions due to a lack of expertise.}
}
@article{nijkamp2022codegen,
  title={CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis},
  author={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  journal={ICLR},
  year={2023}
}
@article{nijkamp2023codegen2,
  title={CodeGen2: Lessons for Training LLMs on Programming and Natural Languages},
  author={Nijkamp, Erik and Hayashi, Hiroaki and Xiong, Caiming and Savarese, Silvio and Zhou, Yingbo},
  journal={ICLR},
  year={2023}
}

@misc{rozière2024code,
      title={Code Llama: Open Foundation Models for Code}, 
      author={Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Romain Sauvestre and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
      year={2024},
      eprint={2308.12950},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{li2023starcoder,
      title={StarCoder: may the source be with you!}, 
      author={StarCode Team},
      year={2023},
      eprint={2305.06161},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chen2023teaching,
      title={Teaching Large Language Models to Self-Debug}, 
      author={Xinyun Chen and Maxwell Lin and Nathanael Schärli and Denny Zhou},
      year={2023},
      eprint={2304.05128},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{luo2023wizardcoder,
      title={WizardCoder: Empowering Code Large Language Models with Evol-Instruct}, 
      author={Ziyang Luo and Can Xu and Pu Zhao and Qingfeng Sun and Xiubo Geng and Wenxiang Hu and Chongyang Tao and Jing Ma and Qingwei Lin and Daxin Jiang},
      year={2023},
      eprint={2306.08568},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{wang2021codet5,
    title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation}, 
    author={Yue Wang and Weishi Wang and Shafiq Joty and Steven C.H. Hoi},
    booktitle={EMNLP},
    year={2021},
}

@misc{wang2023codet5plus,
      title={CodeT5+: Open Code Large Language Models for Code Understanding and Generation}, 
      author={Yue Wang and Hung Le and Akhilesh Deepak Gotmare and Nghi D. Q. Bui and Junnan Li and Steven C. H. Hoi},
      year={2023},
      eprint={2305.07922},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{guo2024deepseekcoder,
      title={DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence}, 
      author={Daya Guo and Qihao Zhu and Dejian Yang and Zhenda Xie and Kai Dong and Wentao Zhang and Guanting Chen and Xiao Bi and Y. Wu and Y. K. Li and Fuli Luo and Yingfei Xiong and Wenfeng Liang},
      year={2024},
      eprint={2401.14196},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}
@misc{gunasekar2023textbooks,
      title={Textbooks Are All You Need}, 
      author={Suriya Gunasekar and Yi Zhang and Jyoti Aneja and Caio César Teodoro Mendes and Allie Del Giorno and Sivakanth Gopi and Mojan Javaheripi and Piero Kauffmann and Gustavo de Rosa and Olli Saarikivi and Adil Salim and Shital Shah and Harkirat Singh Behl and Xin Wang and Sébastien Bubeck and Ronen Eldan and Adam Tauman Kalai and Yin Tat Lee and Yuanzhi Li},
      year={2023},
      eprint={2306.11644},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chen2022codet,
      title={CodeT: Code Generation with Generated Tests}, 
      author={Bei Chen and Fengji Zhang and Anh Nguyen and Daoguang Zan and Zeqi Lin and Jian-Guang Lou and Weizhu Chen},
      year={2022},
      eprint={2207.10397},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{muennighoff2024octopack,
      title={OctoPack: Instruction Tuning Code Large Language Models}, 
      author={Niklas Muennighoff and Qian Liu and Armel Zebaze and Qinkai Zheng and Binyuan Hui and Terry Yue Zhuo and Swayam Singh and Xiangru Tang and Leandro von Werra and Shayne Longpre},
      year={2024},
      eprint={2308.07124},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{jiang2023selfplanning,
      title={Self-planning Code Generation with Large Language Models}, 
      author={Xue Jiang and Yihong Dong and Lecheng Wang and Zheng Fang and Qiwei Shang and Ge Li and Zhi Jin and Wenpin Jiao},
      year={2023},
      eprint={2303.06689},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}
@misc{shen2023pangucoder2,
      title={PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback}, 
      author={Bo Shen and Jiaxin Zhang and Taihong Chen and Daoguang Zan and Bing Geng and An Fu and Muhan Zeng and Ailun Yu and Jichuan Ji and Jingyang Zhao and Yuenan Guo and Qianxiang Wang},
      year={2023},
      eprint={2307.14936},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{shypula2023learning,
      title={Learning Performance-Improving Code Edits}, 
      author={Alexander Shypula and Aman Madaan and Yimeng Zeng and Uri Alon and Jacob Gardner and Milad Hashemi and Graham Neubig and Parthasarathy Ranganathan and Osbert Bastani and Amir Yazdanbakhsh},
      year={2023},
      eprint={2302.07867},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}
@misc{jiang2023selfevolve,
      title={SelfEvolve: A Code Evolution Framework via Large Language Models}, 
      author={Shuyang Jiang and Yuhao Wang and Yu Wang},
      year={2023},
      eprint={2306.02907},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{wei2023magicoder,
      title={Magicoder: Source Code Is All You Need}, 
      author={Yuxiang Wei and Zhe Wang and Jiawei Liu and Yifeng Ding and Lingming Zhang},
      year={2023},
      eprint={2312.02120},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{tipirneni2024structcoder,
    author = {Tipirneni, Sindhu and Zhu, Ming and Reddy, Chandan K.},
    title = {StructCoder: Structure-Aware Transformer for Code Generation},
    year = {2024},
    issue_date = {April 2024},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {18},
    number = {3},
    issn = {1556-4681},
    url = {https://doi.org/10.1145/3636430},
    doi = {10.1145/3636430},
    abstract = {There has been a recent surge of interest in automating software engineering tasks using deep learning. This article addresses the problem of code generation, in which the goal is to generate target code given source code in a different language or a natural language description. Most state-of-the-art deep learning models for code generation use training strategies primarily designed for natural language. However, understanding and generating code requires a more rigorous comprehension of the code syntax and semantics. With this motivation, we develop an encoder-decoder Transformer model in which both the encoder and decoder are explicitly trained to recognize the syntax and dataflow in the source and target codes, respectively. We not only make the encoder structure aware by leveraging the source code’s syntax tree and dataflow graph, but we also support the decoder in preserving the syntax and dataflow of the target code by introducing two novel auxiliary tasks: Abstract Syntax Tree (AST) path prediction and dataflow prediction. To the best of our knowledge, this is the first work to introduce a structure-aware Transformer decoder that models both syntax and dataflow to enhance the quality of generated code. The proposed StructCoder model achieves state-of-the-art performance on code translation and text-to-code generation tasks in the CodeXGLUE benchmark and improves over baselines of similar size on the APPS code generation benchmark. Our code is publicly available at .},
    journal = {ACM Trans. Knowl. Discov. Data},
    month = {jan},
    articleno = {70},
    numpages = {20},
    keywords = {Deep learning, language models, code generation, Transformer}
}
@misc{zhuo2024astraios,
      title={Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models}, 
      author={Terry Yue Zhuo and Armel Zebaze and Nitchakarn Suppattarachai and Leandro von Werra and Harm de Vries and Qian Liu and Niklas Muennighoff},
      year={2024},
      eprint={2401.00788},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{weyssow2024exploring,
      title={Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models}, 
      author={Martin Weyssow and Xin Zhou and Kisub Kim and David Lo and Houari Sahraoui},
      year={2024},
      eprint={2308.10462},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}
@misc{zheng2024survey,
      title={A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends}, 
      author={Zibin Zheng and Kaiwen Ning and Yanlin Wang and Jingwen Zhang and Dewu Zheng and Mingxi Ye and Jiachi Chen},
      year={2024},
      eprint={2311.10372},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}
@misc{nijkamp2023xgen7b,
      title={XGen-7B Technical Report}, 
      author={Erik Nijkamp and Tian Xie and Hiroaki Hayashi and Bo Pang and Congying Xia and Chen Xing and Jesse Vig and Semih Yavuz and Philippe Laban and Ben Krause and Senthil Purushwalkam and Tong Niu and Wojciech Kryściński and Lidiya Murakhovs'ka and Prafulla Kumar Choubey and Alex Fabbri and Ye Liu and Rui Meng and Lifu Tu and Meghana Bhat and Chien-Sheng Wu and Silvio Savarese and Yingbo Zhou and Shafiq Joty and Caiming Xiong},
      year={2023},
      eprint={2309.03450},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{di2023codefuse,
  title={Codefuse-13b: A pretrained multi-lingual code large language model},
  author={Di, Peng and Li, Jianguo and Yu, Hang and Jiang, Wei and Cai, Wenting and Cao, Yang and Chen, Chaoyu and Chen, Dajun and Chen, Hongwei and Chen, Liang and others},
  journal={arXiv preprint arXiv:2310.06266},
  year={2023}
}
@misc{li2024instructcoder,
      title={InstructCoder: Instruction Tuning Large Language Models for Code Editing}, 
      author={Kaixin Li and Qisheng Hu and Xu Zhao and Hui Chen and Yuxi Xie and Tiedong Liu and Qizhe Xie and Junxian He},
      year={2024},
      eprint={2310.20329},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{yadav2023exploring,
      title={Exploring Continual Learning for Code Generation Models}, 
      author={Prateek Yadav and Qing Sun and Hantian Ding and Xiaopeng Li and Dejiao Zhang and Ming Tan and Xiaofei Ma and Parminder Bhatia and Ramesh Nallapati and Murali Krishna Ramanathan and Mohit Bansal and Bing Xiang},
      year={2023},
      eprint={2307.02435},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{lozhkov2024starcoder,
      title={StarCoder 2 and The Stack v2: The Next Generation}, 
      author={StarCoder2 Team},
      year={2024},
      eprint={2402.19173},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}
@misc{paul2024ircoder,
      title={IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators}, 
      author={Indraneil Paul and Jun Luo and Goran Glavaš and Iryna Gurevych},
      year={2024},
      eprint={2403.03894},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@misc{ma2024llamoco,
      title={LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation}, 
      author={Zeyuan Ma and Hongshu Guo and Jiacheng Chen and Guojun Peng and Zhiguang Cao and Yining Ma and Yue-Jiao Gong},
      year={2024},
      eprint={2403.01131},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}
@misc{agarwal2024structured,
      title={Structured Code Representations Enable Data-Efficient Adaptation of Code Language Models}, 
      author={Mayank Agarwal and Yikang Shen and Bailin Wang and Yoon Kim and Jie Chen},
      year={2024},
      eprint={2401.10716},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{song2024code,
      title={Code Needs Comments: Enhancing Code LLMs with Comment Augmentation}, 
      author={Demin Song and Honglin Guo and Yunhua Zhou and Shuhao Xing and Yudong Wang and Zifan Song and Wenwei Zhang and Qipeng Guo and Hang Yan and Xipeng Qiu and Dahua Lin},
      year={2024},
      eprint={2402.13013},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{he2024instruction,
      title={Instruction Tuning for Secure Code Generation}, 
      author={Jingxuan He and Mark Vero and Gabriela Krasnopolska and Martin Vechev},
      year={2024},
      eprint={2402.09497},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}
@misc{gong2024astt5,
      title={AST-T5: Structure-Aware Pretraining for Code Generation and Understanding}, 
      author={Linyuan Gong and Mostafa Elhoushi and Alvin Cheung},
      year={2024},
      eprint={2401.03003},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}
@misc{huang2024knowledgeaware,
      title={Knowledge-Aware Code Generation with Large Language Models}, 
      author={Tao Huang and Zhihong Sun and Zhi Jin and Ge Li and Chen Lyu},
      year={2024},
      eprint={2401.15940},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}
@misc{weyssow2023codell,
      title={CodeLL: A Lifelong Learning Dataset to Support the Co-Evolution of Data and Language Models of Code}, 
      author={Martin Weyssow and Claudio Di Sipio and Davide Di Ruscio and Houari Sahraoui},
      year={2023},
      eprint={2312.12492},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}
@inproceedings{chai2023ernie,
    title = "{ERNIE}-Code: Beyond {E}nglish-Centric Cross-lingual Pretraining for Programming Languages",
    author = "Chai, Yekun  and
      Wang, Shuohuan  and
      Pang, Chao  and
      Sun, Yu  and
      Tian, Hao  and
      Wu, Hua",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.676",
    doi = "10.18653/v1/2023.findings-acl.676",
    pages = "10628--10650",
    abstract = "Software engineers working with the same programming language (PL) may speak different natural languages (NLs) and vice versa, erecting huge barriers to communication and working efficiency. Recent studies have demonstrated the effectiveness of generative pre-training in computer programs, yet they are always English-centric. In this work, we step towards bridging the gap between multilingual NLs and multilingual PLs for large language models (LLMs). We release ERNIE-Code, a unified pre-trained language model for 116 NLs and 6 PLs. We employ two methods for universal cross-lingual pre-training: span-corruption language modeling that learns patterns from monolingual NL or PL; and pivot-based translation language modeling that relies on parallel data of many NLs and PLs. Extensive results show that ERNIE-Code outperforms previous multilingual LLMs for PL or NL across a wide range of end tasks of code intelligence, including multilingual code-to-text, text-to-code, code-to-code, and text-to-text generation. We further show its advantage of zero-shot prompting on multilingual code summarization and text-to-text translation. We release our code and pre-trained checkpoints.",
}
@misc{allal2023santacoder,
      title={SantaCoder: don't reach for the stars!}, 
      author={Loubna Ben Allal and Raymond Li and Denis Kocetkov and Chenghao Mou and Christopher Akiki and Carlos Munoz Ferrandis and Niklas Muennighoff and Mayank Mishra and Alex Gu and Manan Dey and Logesh Kumar Umapathi and Carolyn Jane Anderson and Yangtian Zi and Joel Lamy Poirier and Hailey Schoelkopf and Sergey Troshin and Dmitry Abulkhanov and Manuel Romero and Michael Lappert and Francesco De Toni and Bernardo García del Río and Qian Liu and Shamik Bose and Urvashi Bhattacharyya and Terry Yue Zhuo and Ian Yu and Paulo Villegas and Marco Zocca and Sourab Mangrulkar and David Lansky and Huu Nguyen and Danish Contractor and Luis Villa and Jia Li and Dzmitry Bahdanau and Yacine Jernite and Sean Hughes and Daniel Fried and Arjun Guha and Harm de Vries and Leandro von Werra},
      year={2023},
      eprint={2301.03988},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}
@misc{zheng2023codegeex,
      title={CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X}, 
      author={Qinkai Zheng and Xiao Xia and Xu Zou and Yuxiao Dong and Shan Wang and Yufei Xue and Zihan Wang and Lei Shen and Andi Wang and Yang Li and Teng Su and Zhilin Yang and Jie Tang},
      year={2023},
      eprint={2303.17568},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{cheng2024adapting,
      title={Adapting Large Language Models via Reading Comprehension}, 
      author={Daixuan Cheng and Shaohan Huang and Furu Wei},
      year={2024},
      eprint={2309.09530},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{vandekar2022dont,
    title = "Don{'}t Prompt, Search! Mining-based Zero-Shot Learning with Language Models",
    author = "van de Kar, Mozes  and
      Xia, Mengzhou  and
      Chen, Danqi  and
      Artetxe, Mikel",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.509",
    doi = "10.18653/v1/2022.emnlp-main.509",
    pages = "7508--7520",
    abstract = "Masked language models like BERT can perform text classification in a zero-shot fashion by reformulating downstream tasks as text infilling. However, this approach is highly sensitive to the template used to prompt the model, yet practitioners are blind when designing them in strict zero-shot settings. In this paper, we propose an alternative mining-based approach for zero-shot learning. Instead of prompting language models, we use regular expressions to mine labeled examples from unlabeled corpora, which can optionally be filtered through prompting, and used to finetune a pretrained model. Our method is more flexible and interpretable than prompting, and outperforms it on a wide range of tasks when using comparable templates. Our results suggest that the success of prompting can partly be explained by the model being exposed to similar examples during pretraining, which can be directly retrieved through regular expressions.",
}

% Continual Fine-Tuning
@misc{liu2020exploring,
      title={Exploring Fine-tuning Techniques for Pre-trained Cross-lingual Models via Continual Learning}, 
      author={Zihan Liu and Genta Indra Winata and Andrea Madotto and Pascale Fung},
      year={2020},
      eprint={2004.14218},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{ke2021achieve,
      title={Achieving Forgetting Prevention and Knowledge Transfer in Continual Learning},
      author={Ke, Zixuan and Liu, Bing and Ma, Nianzu and Xu, Hu and Lei, Shu},
      booktitle={NeurIPS},
      year={2021}
}
@inproceedings{ke2021adapting,
    title = "Adapting {BERT} for Continual Learning of a Sequence of Aspect Sentiment Classification Tasks",
    author = "Ke, Zixuan  and
      Xu, Hu  and
      Liu, Bing",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.378",
    doi = "10.18653/v1/2021.naacl-main.378",
    pages = "4746--4755",
    abstract = "This paper studies continual learning (CL) of a sequence of aspect sentiment classification (ASC) tasks. Although some CL techniques have been proposed for document sentiment classification, we are not aware of any CL work on ASC. A CL system that incrementally learns a sequence of ASC tasks should address the following two issues: (1) transfer knowledge learned from previous tasks to the new task to help it learn a better model, and (2) maintain the performance of the models for previous tasks so that they are not forgotten. This paper proposes a novel capsule network based model called B-CL to address these issues. B-CL markedly improves the ASC performance on both the new task and the old tasks via forward and backward knowledge transfer. The effectiveness of B-CL is demonstrated through extensive experiments.",
}
@inproceedings{ni2023continual,
  title={Continual vision-language representation learning with off-diagonal information},
  author={Ni, Zixuan and Wei, Longhui and Tang, Siliang and Zhuang, Yueting and Tian, Qi},
  booktitle={Proceedings of the 40th International Conference on Machine Learning},
  pages={26129--26149},
  year={2023}
}
@misc{zheng2024antiforgetting,
      title={Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with Positive Forward Transfer}, 
      author={Junhao Zheng and Qianli Ma and Zhen Liu and Binquan Wu and Huawen Feng},
      year={2024},
      eprint={2401.09181},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{tao2022can,
  title={Can bert refrain from forgetting on sequential tasks? a probing study},
  author={Tao, Mingxu and Feng, Yansong and Zhao, Dongyan},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}
@inproceedings{wei2022circle,
    author = {Yuan, Wei and Zhang, Quanjun and He, Tieke and Fang, Chunrong and Hung, Nguyen Quoc Viet and Hao, Xiaodong and Yin, Hongzhi},
    title = {CIRCLE: continual repair across programming languages},
    year = {2022},
    isbn = {9781450393799},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3533767.3534219},
    doi = {10.1145/3533767.3534219},
    abstract = {Automatic Program Repair (APR) aims at fixing buggy source code with less manual debugging efforts, which plays a vital role in improving software reliability and development productivity. Recent APR works have achieved remarkable progress via applying deep learning (DL), particularly neural machine translation (NMT) techniques. However, we observe that existing DL-based APR models suffer from at least two severe drawbacks: (1) Most of them can only generate patches for a single programming language, as a result, to repair multiple languages, we have to build and train many repairing models. (2) Most of them are developed offline. Therefore, they won’t function when there are new-coming requirements. To address the above problems, a T5-based APR framework equipped with continual learning ability across multiple programming languages is proposed, namely ContInual Repair aCross Programming LanguagEs (CIRCLE). Specifically, (1) CIRCLE utilizes a prompting function to narrow the gap between natural language processing (NLP) pre-trained tasks and APR. (2) CIRCLE adopts a difficulty-based rehearsal strategy to achieve lifelong learning for APR without access to the full historical data. (3) An elastic regularization method is employed to strengthen CIRCLE’s continual learning ability further, preventing it from catastrophic forgetting. (4) CIRCLE applies a simple but effective re-repairing method to revise generated errors caused by crossing multiple programming languages. We train CIRCLE for four languages (i.e., C, JAVA, JavaScript, and Python) and evaluate it on five commonly used benchmarks. The experimental results demonstrate that CIRCLE not only effectively and efficiently repairs multiple programming languages in continual learning settings, but also achieves state-of-the-art performance (e.g., fixes 64 Defects4J bugs) with a single repair model.},
    booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
    pages = {678–690},
    numpages = {13},
    keywords = {Neural Machine Translation, Lifelong Learning, Automatic Program Repair, AI and Software Engineering},
    location = {<conf-loc>, <city>Virtual</city>, <country>South Korea</country>, </conf-loc>},
    series = {ISSTA 2022}
}
@misc{song2023conpet,
      title={ConPET: Continual Parameter-Efficient Tuning for Large Language Models}, 
      author={Chenyang Song and Xu Han and Zheni Zeng and Kuai Li and Chen Chen and Zhiyuan Liu and Maosong Sun and Tao Yang},
      year={2023},
      eprint={2309.14763},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{kane2022continual,
      title={Continual VQA for Disaster Response Systems}, 
      author={Aditya Kane and V Manushree and Sahil Khose},
      year={2022},
      eprint={2209.10320},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{raghavan2023engineering,
      title={Engineering flexible machine learning systems by traversing functionally-invariant paths}, 
      author={Guruprasad Raghavan and Bahey Tharwat and Surya Narayanan Hari and Dhruvil Satani and Matt Thomson},
      year={2023},
      eprint={2205.00334},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{bai2023enhancing,
      title={Enhancing Continual Learning with Global Prototypes: Counteracting Negative Representation Drift}, 
      author={Xueying Bai and Jinghuan Shang and Yifan Sun and Niranjan Balasubramanian},
      year={2023},
      eprint={2205.12186},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{scialom2022fine,
    title = "Fine-tuned Language Models are Continual Learners",
    author = "Scialom, Thomas  and
      Chakrabarty, Tuhin  and
      Muresan, Smaranda",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.410",
    doi = "10.18653/v1/2022.emnlp-main.410",
    pages = "6107--6122",
    abstract = "Recent work on large language models relies on the intuition that most natural language processing tasks can be described via natural language instructions and that models trained on these instructions show strong zero-shot performance on several standard datasets. However, these models even though impressive still perform poorly on a wide range of tasks outside of their respective training and evaluation sets. To address this limitation, we argue that a model should be able to keep extending its knowledge and abilities, without forgetting previous skills. In spite of the limited success of Continual Learning, we show that \textit{Fine-tuned Language Models can be continual learners}.We empirically investigate the reason for this success and conclude that Continual Learning emerges from self-supervision pre-training. Our resulting model Continual-T0 (CT0) is able to learn 8 new diverse language generation tasks, while still maintaining good performance on previous tasks, spanning in total of 70 datasets. Finally, we show that CT0 is able to combine instructions in ways it was never trained for, demonstrating some level of instruction compositionality.",
}
@inproceedings{huang2022fpt,
    title = "{FPT}: Improving Prompt Tuning Efficiency via Progressive Training",
    author = "Huang, Yufei  and
      Qin, Yujia  and
      Wang, Huadong  and
      Yin, Yichun  and
      Sun, Maosong  and
      Liu, Zhiyuan  and
      Liu, Qun",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.511",
    doi = "10.18653/v1/2022.findings-emnlp.511",
    pages = "6877--6887",
    abstract = "Recently, prompt tuning (PT) has gained increasing attention as a parameter-efficient way of tuning pre-trained language models (PLMs). Despite extensively reducing the number of tunable parameters and achieving satisfying performance, PT is training-inefficient due to its slow convergence. To improve PT{'}s training efficiency, we first make some novel observations about the prompt transferability of {``}partial PLMs{''}, which are defined by compressing a PLM in depth or width. We observe that the soft prompts learned by different partial PLMs of various sizes are similar in the parameter space, implying that these soft prompts could potentially be transferred among partial PLMs. Inspired by these observations, we propose Fast Prompt Tuning (FPT), which starts by conducting PT using a small-scale partial PLM, and then progressively expands its depth and width until the full-model size. After each expansion, we recycle the previously learned soft prompts as initialization for the enlarged partial PLM and then proceed PT. We demonstrate the feasibility of FPT on 5 tasks and show that FPT could save over 30{\%} training computations while achieving comparable performance. The codes are publicly available at \url{https://github.com/thunlp/FastPromptTuning}.",
}
@misc{luo2023investigating,
      title={Investigating Forgetting in Pre-Trained Representations Through Continual Learning}, 
      author={Yun Luo and Zhen Yang and Xuefeng Bai and Fandong Meng and Jie Zhou and Yue Zhang},
      year={2023},
      eprint={2305.05968},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{shiri2023l3,
      title={L3 Ensembles: Lifelong Learning Approach for Ensemble of Foundational Language Models}, 
      author={Aidin Shiri and Kaushik Roy and Amit Sheth and Manas Gaur},
      year={2023},
      eprint={2311.06493},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{zheng2023learn,
      title={Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models}, 
      author={Junhao Zheng and Shengjie Qiu and Qianli Ma},
      year={2023},
      eprint={2312.07887},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{qin2021lfpt5,
  title={LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5},
  author={Qin, Chengwei and Joty, Shafiq},
  booktitle={International Conference on Learning Representations},
  year={2021}
}
@misc{lin2024mitigating,
      title={Mitigating the Alignment Tax of RLHF}, 
      author={Yong Lin and Hangyu Lin and Wei Xiong and Shizhe Diao and Jianmeng Liu and Jipeng Zhang and Rui Pan and Haoxiang Wang and Wenbin Hu and Hanning Zhang and Hanze Dong and Renjie Pi and Han Zhao and Nan Jiang and Heng Ji and Yuan Yao and Tong Zhang},
      year={2024},
      eprint={2309.06256},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}
@article{dua2019drop,
  title={DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs},
  author={Dua, Dheeru and Wang, Yizhong and Dasigi, Pradeep and Stanovsky, Gabriel and Singh, Sameer and Gardner, Matt},
  journal={arXiv preprint arXiv:1903.00161},
  year={2019}
}
@inproceedings{bojar2014findings,
  title={Findings of the 2014 workshop on statistical machine translation},
  author={Bojar, Ond{\v{r}}ej and Buck, Christian and Federmann, Christian and Haddow, Barry and Koehn, Philipp and Leveling, Johannes and Monz, Christof and Pecina, Pavel and Post, Matt and Saint-Amand, Herve and others},
  booktitle={Proceedings of the ninth workshop on statistical machine translation},
  pages={12--58},
  year={2014}
}
@article{rajpurkar2018know,
  title={Know what you don't know: Unanswerable questions for SQuAD},
  author={Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  journal={arXiv preprint arXiv:1806.03822},
  year={2018}
}
@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={7432--7439},
  year={2020}
}
@article{lai2017race,
  title={Race: Large-scale reading comprehension dataset from examinations},
  author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
  journal={arXiv preprint arXiv:1704.04683},
  year={2017}
}
@inproceedings{volske2017tl,
  title={Tl; dr: Mining reddit to learn automatic summarization},
  author={V{\"o}lske, Michael and Potthast, Martin and Syed, Shahbaz and Stein, Benno},
  booktitle={Proceedings of the Workshop on New Frontiers in Summarization},
  pages={59--63},
  year={2017}
}
@inproceedings{weyssow2023usage,
    author = {Weyssow, Martin and Zhou, Xin and Kim, Kisub and Lo, David and Sahraoui, Houari},
    title = {On the Usage of Continual Learning for Out-of-Distribution Generalization in Pre-trained Language Models of Code},
    year = {2023},
    isbn = {9798400703270},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3611643.3616244},
    doi = {10.1145/3611643.3616244},
    abstract = {Pre-trained language models (PLMs) have become a prevalent technique in deep learning for code, utilizing a two-stage pre-training and fine-tuning procedure to acquire general knowledge about code and specialize in a variety of downstream tasks. However, the dynamic nature of software codebases poses a challenge to the effectiveness and robustness of PLMs. In particular, world-realistic scenarios potentially lead to significant differences between the distribution of the pre-training and test data, i.e., distribution shift, resulting in a degradation of the PLM's performance on downstream tasks. In this paper, we stress the need for adapting PLMs of code to software data whose distribution changes over time, a crucial problem that has been overlooked in previous works. The motivation of this work is to consider the PLM in a non-stationary environment, where fine-tuning data evolves over time according to a software evolution scenario. Specifically, we design a scenario where the model needs to learn from a stream of programs containing new, unseen APIs over time. We study two widely used PLM architectures, i.e., a GPT2 decoder and a RoBERTa encoder, on two downstream tasks, API call and API usage prediction. We demonstrate that the most commonly used fine-tuning technique from prior work is not robust enough to handle the dynamic nature of APIs, leading to the loss of previously acquired knowledge i.e., catastrophic forgetting. To address these issues, we implement five continual learning approaches, including replay-based and regularization-based methods. Our findings demonstrate that utilizing these straightforward methods effectively mitigates catastrophic forgetting in PLMs across both downstream tasks while achieving comparable or superior performance.},
    booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
    pages = {1470–1482},
    numpages = {13},
    keywords = {pre-trained language models, out-of-distribution generalization, deep learning for code, continual learning},
    location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
    series = {ESEC/FSE 2023}
}
@inproceedings{winata2023overcoming,
    title = "Overcoming Catastrophic Forgetting in Massively Multilingual Continual Learning",
    author = "Winata, Genta  and
      Xie, Lingjue  and
      Radhakrishnan, Karthik  and
      Wu, Shijie  and
      Jin, Xisen  and
      Cheng, Pengxiang  and
      Kulkarni, Mayank  and
      Preotiuc-Pietro, Daniel",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.48",
    doi = "10.18653/v1/2023.findings-acl.48",
    pages = "768--777",
    abstract = "Real-life multilingual systems should be able to efficiently incorporate new languages as data distributions fed to the system evolve and shift over time. To do this, systems need to handle the issue of catastrophic forgetting, where the model performance drops for languages or tasks seen further in its past. In this paper, we study catastrophic forgetting, as well as methods to minimize this, in a massively multilingual continual learning framework involving up to 51 languages and covering both classification and sequence labeling tasks. We present LR ADJUST, a learning rate scheduling method that is simple, yet effective in preserving new information without strongly overwriting past knowledge. Furthermore, we show that this method is effective across multiple continual learning approaches. Finally, we provide further insights into the dynamics of catastrophic forgetting in this massively multilingual setup.",
}
@article{chen2024parameterizing,
  title={Parameterizing Context: Unleashing the Power of Parameter-Efficient Fine-Tuning and In-Context Tuning for Continual Table Semantic Parsing},
  author={Chen, Yongrui and Zhang, Shenyu and Qi, Guilin and Guo, Xinnan},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{zhong2022proqa,
    title = "{P}ro{QA}: Structural Prompt-based Pre-training for Unified Question Answering",
    author = "Zhong, Wanjun  and
      Gao, Yifan  and
      Ding, Ning  and
      Qin, Yujia  and
      Liu, Zhiyuan  and
      Zhou, Ming  and
      Wang, Jiahai  and
      Yin, Jian  and
      Duan, Nan",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.313",
    doi = "10.18653/v1/2022.naacl-main.313",
    pages = "4230--4243",
    abstract = "Question Answering (QA) is a longstanding challenge in natural language processing. Existing QA works mostly focus on specific question types, knowledge domains, or reasoning skills. The specialty in QA research hinders systems from modeling commonalities between tasks and generalization for wider applications. To address this issue, we present ProQA, a unified QA paradigm that solves various tasks through a single model. ProQA takes a unified structural prompt as the bridge and improves the QA-centric ability by structural prompt-based pre-training. Through a structurally designed prompt-based input schema, ProQA concurrently models the knowledge generalization for all QA tasks while keeping the knowledge customization for every specific QA task. Furthermore, ProQA is pre-trained with structural prompt-formatted large-scale synthesized corpus, which empowers the model with the commonly-required QA ability. Experimental results on 11 QA benchmarks demonstrate that ProQA consistently boosts performance on both full data fine-tuning, few-shot learning, and zero-shot testing scenarios. Furthermore, ProQA exhibits strong ability in both continual learning and transfer learning by taking the advantages of the structural prompt.",
}
@inproceedings{panigrahi2023task,
    author = {Panigrahi, Abhishek and Saunshi, Nikunj and Zhao, Haoyu and Arora, Sanjeev},
    title = {Task-specific skill localization in fine-tuned language models},
    year = {2023},
    publisher = {JMLR.org},
    abstract = {Pre-trained language models can be fine-tuned to solve diverse NLP tasks, including in few-shot settings. Thus fine-tuning allows the model to quickly pick up task-specific "skills," but there has been limited study of where these newly-learnt skills reside inside the massive model. This paper introduces the term skill localization for this problem and proposes a solution. Given the downstream task and a model fine-tuned on that task, a simple optimization is used to identify a very small subset of parameters (∼ 0.01\% of model parameters) responsible for (> 95\%) of the model's performance, in the sense that grafting the fine-tuned values for just this tiny subset onto the pre-trained model gives a performance almost as well as the fine-tuned model. While reminiscent of recent works on parameter-efficient fine-tuning, the novel aspects here are that: (i) No further re-training is needed on the subset (unlike, say, with lottery tickets). (ii) Notable improvements are seen over vanilla fine-tuning with respect to calibration of predictions in-distribution (40-90\% error reduction) as well as the quality of predictions out-of-distribution (OOD). In models trained on multiple tasks, a stronger notion of skill localization is observed, where the sparse regions corresponding to different tasks are almost disjoint, and their overlap (when it happens) is a proxy for task similarity. Experiments suggest that localization via grafting can assist certain forms of continual learning. Our code is available at Skill-Localization-by-grafting.},
    booktitle = {Proceedings of the 40th International Conference on Machine Learning},
    articleno = {1125},
    numpages = {23},
    location = {<conf-loc>, <city>Honolulu</city>, <state>Hawaii</state>, <country>USA</country>, </conf-loc>},
    series = {ICML'23}
}


% Continual Model Alignment, by Hengyi Wang
@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}
@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}
@inproceedings{banerjee2005meteor,
  title={METEOR: An automatic metric for MT evaluation with improved correlation with human judgments},
  author={Banerjee, Satanjeev and Lavie, Alon},
  booktitle={Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization},
  pages={65--72},
  year={2005}
}
@misc{hendrycks2023aligning,
      title={Aligning AI With Shared Human Values}, 
      author={Dan Hendrycks and Collin Burns and Steven Basart and Andrew Critch and Jerry Li and Dawn Song and Jacob Steinhardt},
      year={2023},
      eprint={2008.02275},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}
@misc{schulman2017proximal,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{ji2024ai,
      title={AI Alignment: A Comprehensive Survey}, 
      author={Jiaming Ji and Tianyi Qiu and Boyuan Chen and Borong Zhang and Hantao Lou and Kaile Wang and Yawen Duan and Zhonghao He and Jiayi Zhou and Zhaowei Zhang and Fanzhi Zeng and Kwan Yee Ng and Juntao Dai and Xuehai Pan and Aidan O'Gara and Yingshan Lei and Hua Xu and Brian Tse and Jie Fu and Stephen McAleer and Yaodong Yang and Yizhou Wang and Song-Chun Zhu and Yike Guo and Wen Gao},
      year={2024},
      eprint={2310.19852},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@article{wu2022survey,
  title={A survey of human-in-the-loop for machine learning},
  author={Wu, Xingjiao and Xiao, Luwei and Sun, Yixuan and Zhang, Junhang and Ma, Tianlong and He, Liang},
  journal={Future Generation Computer Systems},
  volume={135},
  pages={364--381},
  year={2022},
  publisher={Elsevier}
}

@article{taori2023alpaca,
  title={Alpaca: A strong, replicable instruction-following model},
  author={Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  journal={Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html},
  volume={3},
  number={6},
  pages={7},
  year={2023}
}

@inproceedings{wang2022selfinstruct,
    title={Self-training Improves Pre-training for Few-shot Learning in Task-oriented Dialog Systems},
    author={Yuxiang Wang and Yilin Chen and Bei Yu and Gokhan Tur and Yun-Nung Chen and Jianfeng Gao},
    booktitle={EMNLP},
    year={2022}
}

@misc{ouyang2022rlhf,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{rafailov2024dpo,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{maas2011learning,
  title={Learning word vectors for sentiment analysis},
  author={Maas, Andrew and Daly, Raymond E and Pham, Peter T and Huang, Dan and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies},
  pages={142--150},
  year={2011}
}
@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}
@misc{stiennon2022learning,
      title={Learning to summarize from human feedback}, 
      author={Nisan Stiennon and Long Ouyang and Jeff Wu and Daniel M. Ziegler and Ryan Lowe and Chelsea Voss and Alec Radford and Dario Amodei and Paul Christiano},
      year={2022},
      eprint={2009.01325},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{zhang2023copf,
  title={Copf: Continual learning human preference through optimal policy fitting},
  author={Zhang, Han and Gui, Lin and Zhai, Yuanzhao and Wang, Hui and Lei, Yu and Xu, Ruifeng},
  journal={arXiv preprint arXiv:2310.15694},
  year={2023}
}

@article{zhangcppo,
  title={CPPO: Continual Learning for Reinforcement Learning with Human Feedback},
  author={Zhang, Han and Lei, Yu and Gui, Lin and Yang, Min and He, Yulan and Wang, Hui and Xu, Ruifeng}
}

@article{puthumanaillam2024moral,
  title={A Moral Imperative: The Need for Continual Superalignment of Large Language Models},
  author={Puthumanaillam, Gokul and Vora, Manav and Thangeda, Pranay and Ornik, Melkior},
  journal={arXiv preprint arXiv:2403.14683},
  year={2024}
}

@inproceedings{houlsby2019parameter,
    title={Parameter-efficient transfer learning for NLP},
    author={Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin De Laroussilhe and Andrea Gesmundo and Mona Attariyan},
    booktitle={Proceedings of the 36th International Conference on Machine Learning},
    year={2019}
}
@article{kirkpatrick2017overcoming,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the national academy of sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
  publisher={National Acad Sciences}
}

@article{rusu2016progressive,
  title={Progressive neural networks},
  author={Rusu, Andrei A and Rabinowitz, Neil C and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
  journal={arXiv preprint arXiv:1606.04671},
  year={2016}
}

@article{robins1995catastrophic,
  title={Catastrophic forgetting, rehearsal and pseudorehearsal},
  author={Robins, Anthony},
  journal={Connection Science},
  volume={7},
  number={2},
  pages={123--146},
  year={1995},
  publisher={Taylor \& Francis}
}
@article{liu2022few,
  title={Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning},
  author={Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1950--1965},
  year={2022}
}
@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

%====== model editing =========

%% ==== dataset ====
@article{PARAREL,
  title={Measuring and improving consistency in pretrained language models},
  author={Elazar, Yanai and Kassner, Nora and Ravfogel, Shauli and Ravichander, Abhilasha and Hovy, Eduard and Sch{\"u}tze, Hinrich and Goldberg, Yoav},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={1012--1031},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{Wikireading,
  title={Wikireading: A novel large-scale language understanding task over wikipedia},
  author={Hewlett, Daniel and Lacoste, Alexandre and Jones, Llion and Polosukhin, Illia and Fandrianto, Andrew and Han, Jay and Kelcey, Matthew and Berthelot, David},
  journal={arXiv preprint arXiv:1608.03542},
  year={2016}
}

@inproceedings{Dbpedia,
  title={Dbpedia abstracts: A large-scale, open, multilingual NLP training corpus},
  author={Br{\"u}mmer, Martin and Dojchinovski, Milan and Hellmann, Sebastian},
  booktitle={Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)},
  pages={3339--3343},
  year={2016}
}

@article{fever,
  title={FEVER: a large-scale dataset for fact extraction and VERification},
  author={Thorne, James and Vlachos, Andreas and Christodoulopoulos, Christos and Mittal, Arpit},
  journal={arXiv preprint arXiv:1803.05355},
  year={2018}
}

@article{zsRE,
  title={Zero-shot relation extraction via reading comprehension},
  author={Levy, Omer and Seo, Minjoon and Choi, Eunsol and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1706.04115},
  year={2017}
}

@article{scotus,
  title={Fairlex: A multilingual benchmark for evaluating fairness in legal text processing},
  author={Chalkidis, Ilias and Pasini, Tommaso and Zhang, Sheng and Tomada, Letizia and Schwemer, Sebastian Felix and S{\o}gaard, Anders},
  journal={arXiv preprint arXiv:2203.07228},
  year={2022}
}

@article{vitaminC,
  title={Get your vitamin C! robust fact verification with contrastive evidence},
  author={Schuster, Tal and Fisch, Adam and Barzilay, Regina},
  journal={arXiv preprint arXiv:2103.08541},
  year={2021}
}

@article{nq,
  title={Natural questions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{T-rex,
  title={T-rex: A large scale alignment of natural language with knowledge base triples},
  author={Elsahar, Hady and Vougiouklis, Pavlos and Remaci, Arslen and Gravier, Christophe and Hare, Jonathon and Laforest, Frederique and Simperl, Elena},
  booktitle={Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
  year={2018}
}


% dataset of CounterFactual 
@article{meng2022locating,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}

@article{li2022large,
  title={Large language models with controllable working memory},
  author={Li, Daliang and Rawat, Ankit Singh and Zaheer, Manzil and Wang, Xin and Lukasik, Michal and Veit, Andreas and Yu, Felix and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:2211.05110},
  year={2022}
}

@article{mazzia2023survey,
  title={A Survey on Knowledge Editing of Neural Networks},
  author={Mazzia, Vittorio and Pedrani, Alessandro and Caciolai, Andrea and Rottmann, Kay and Bernardi, Davide},
  journal={arXiv preprint arXiv:2310.19704},
  year={2023}
}

@article{dong2022calibrating,
  title={Calibrating factual knowledge in pretrained language models},
  author={Dong, Qingxiu and Dai, Damai and Song, Yifan and Xu, Jingjing and Sui, Zhifang and Li, Lei},
  journal={arXiv preprint arXiv:2210.03329},
  year={2022}
}

@article{sinitsin2020editable,
  title={Editable neural networks},
  author={Sinitsin, Anton and Plokhotnyuk, Vsevolod and Pyrkin, Dmitriy and Popov, Sergei and Babenko, Artem},
  journal={arXiv preprint arXiv:2004.00345},
  year={2020}
}


@article{de2021editing,
  title={Editing factual knowledge in language models},
  author={De Cao, Nicola and Aziz, Wilker and Titov, Ivan},
  journal={arXiv preprint arXiv:2104.08164},
  year={2021}
}

@article{fast_edit,
  title={Fast model editing at scale},
  author={Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Finn, Chelsea and Manning, Christopher D},
  journal={arXiv preprint arXiv:2110.11309},
  year={2021}
}

@article{hase2021language,
  title={Do language models have beliefs? methods for detecting, updating, and visualizing model beliefs},
  author={Hase, Peter and Diab, Mona and Celikyilmaz, Asli and Li, Xian and Kozareva, Zornitsa and Stoyanov, Veselin and Bansal, Mohit and Iyer, Srinivasan},
  journal={arXiv preprint arXiv:2111.13654},
  year={2021}
}

@inproceedings{mitchell2022memory,
  title={Memory-based model editing at scale},
  author={Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Manning, Christopher D and Finn, Chelsea},
  booktitle={International Conference on Machine Learning},
  pages={15817--15831},
  year={2022},
  organization={PMLR}
}

@article{hase2023does,
  title={Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs},
  author={Hase, Peter and Bansal, Mohit and Kim, Been and Ghandeharioun, Asma},
  journal={Knowledge Editing in Language Models},
  year={2023}
}

@article{meng2022mass,
  title={Mass-editing memory in a transformer},
  author={Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex and Belinkov, Yonatan and Bau, David},
  journal={arXiv preprint arXiv:2210.07229},
  year={2022}
}

@article{huang2023transformer,
  title={Transformer-patcher: One mistake worth one neuron},
  author={Huang, Zeyu and Shen, Yikang and Zhang, Xiaofeng and Zhou, Jie and Rong, Wenge and Xiong, Zhang},
  journal={arXiv preprint arXiv:2301.09785},
  year={2023}
}

@inproceedings{hartvigsen2023aging,
  title={Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors},
  author={Hartvigsen, Thomas and Sankaranarayanan, Swami and Palangi, Hamid and Kim, Yoon and Ghassemi, Marzyeh},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}
@inproceedings{lin2022continual,
    title = "On Continual Model Refinement in Out-of-Distribution Data Streams",
    author = "Lin, Bill Yuchen  and
      Wang, Sida  and
      Lin, Xi  and
      Jia, Robin  and
      Xiao, Lin  and
      Ren, Xiang  and
      Yih, Scott",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.223",
    doi = "10.18653/v1/2022.acl-long.223",
    pages = "3128--3139",
    abstract = "Real-world natural language processing (NLP) models need to be continually updated to fix the prediction errors in out-of-distribution (OOD) data streams while overcoming catastrophic forgetting. However, existing continual learning (CL) problem setups cannot cover such a realistic and complex scenario. In response to this, we propose a new CL problem formulation dubbed continual model refinement (CMR). Compared to prior CL settings, CMR is more practical and introduces unique challenges (boundary-agnostic and non-stationary distribution shift, diverse mixtures of multiple OOD data clusters, error-centric streams, etc.). We extend several existing CL approaches to the CMR setting and evaluate them extensively. For benchmarking and analysis, we propose a general sampling algorithm to obtain dynamic OOD data streams with controllable non-stationarity, as well as a suite of metrics measuring various aspects of online performance. Our experiments and detailed analysis reveal the promise and challenges of the CMR problem, supporting that studying CMR in dynamic OOD streams can benefit the longevity of deployed NLP models in production.",
}
@misc{hu2024wilke,
      title={WilKE: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing}, 
      author={Chenhui Hu and Pengfei Cao and Yubo Chen and Kang Liu and Jun Zhao},
      year={2024},
      eprint={2402.10987},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{das2024larimar,
  title={Larimar: Large Language Models with Episodic Memory Control},
  author={Das, Payel and Chaudhury, Subhajit and Nelson, Elliot and Melnyk, Igor and Swaminathan, Sarath and Dai, Sihui and Lozano, Aur{\'e}lie and Kollias, Georgios and Chenthamarakshan, Vijil and Dan, Soham and others},
  journal={arXiv preprint arXiv:2403.11901},
  year={2024}
}
@article{yu2023melo,
  title={MELO: Enhancing Model Editing with Neuron-Indexed Dynamic LoRA},
  author={Yu, Lang and Chen, Qin and Zhou, Jie and He, Liang},
  journal={arXiv preprint arXiv:2312.11795},
  year={2023}
}
@misc{li2023continual,
    title={{CONTINUAL} {MODEL} {EVOLVEMENT} {WITH} {INNER}-{PRODUCT} {RESTRICTION}},
    author={Linyang Li and Xipeng Qiu},
    year={2023},
    url={https://openreview.net/forum?id=fn0BQK5T8p}
}
@misc{yang2024moral,
      title={MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning}, 
      author={Shu Yang and Muhammad Asif Ali and Cheng-Long Wang and Lijie Hu and Di Wang},
      year={2024},
      eprint={2402.11260},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

%====== instruction tuning =========
@inproceedings{jang2023exploring,
    author = {Jang, Joel and Kim, Seungone and Ye, Seonghyeon and Kim, Doyoung and Logeswaran, Lajanugen and Lee, Moontae and Lee, Kyungjae and Seo, Minjoon},
    title = {Exploring the benefits of training expert language models over instruction tuning},
    year = {2023},
    publisher = {JMLR.org},
    abstract = {Recently, Language Models (LMs) instruction-tuned on multiple tasks, also known as multitask-prompted fine-tuning (MT), have shown the capability to generalize to unseen tasks. Previous work has shown that scaling the number of training tasks is the key component in making stronger MT LMs. In this work, we report an unexpected finding that an expert LM fine-tuned on just a single task can outperform an MT LM trained with 300+ different tasks on 11 different unseen datasets and on 13 datasets of the BIG-bench benchmark by a mean accuracy of 3.20\% and 1.29\%, respectively. This finding casts doubt on the previously held belief that simply scaling the number of tasks makes stronger MT LMs. Leveraging this finding, we further show that this distributed approach of training a separate expert LM per training task instead of a single MT LM for zero-shot inference possesses many benefits including (1) avoiding negative task transfer that often occurs during instruction tuning, (2) being able to continually learn new tasks without having to re-train on previous tasks to avoid catastrophic forgetting, and (3) showing compositional capabilities when merging individual experts together. The code is available at https://github.com/joeljang/ELM.},
    booktitle = {Proceedings of the 40th International Conference on Machine Learning},
    articleno = {600},
    numpages = {28},
    location = {<conf-loc>, <city>Honolulu</city>, <state>Hawaii</state>, <country>USA</country>, </conf-loc>},
    series = {ICML'23}
}
@misc{wang2023trace,
      title={TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models}, 
      author={Xiao Wang and Yuansen Zhang and Tianze Chen and Songyang Gao and Senjie Jin and Xianjun Yang and Zhiheng Xi and Rui Zheng and Yicheng Zou and Tao Gui and Qi Zhang and Xuanjing Huang},
      year={2023},
      eprint={2310.06762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{mok2023large,
    title = "Large-scale Lifelong Learning of In-context Instructions and How to Tackle It",
    author = "Mok, Jisoo  and
      Do, Jaeyoung  and
      Lee, Sungjin  and
      Taghavi, Tara  and
      Yu, Seunghak  and
      Yoon, Sungroh",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.703",
    doi = "10.18653/v1/2023.acl-long.703",
    pages = "12573--12589",
    abstract = "Jointly fine-tuning a Pre-trained Language Model (PLM) on a pre-defined set of tasks with in-context instructions has been proven to improve its generalization performance, allowing us to build a universal language model that can be deployed across task boundaries. In this work, we explore for the first time whether this attractive property of in-context instruction learning can be extended to a scenario in which tasks are fed to the target PLM in a sequential manner. The primary objective of so-called lifelong in-context instruction learning is to improve the target PLM{'}s instance- and task-level generalization performance as it observes more tasks. DynaInst, the proposed method to lifelong in-context instruction learning, achieves noticeable improvements in both types of generalization, nearly reaching the upper bound performance obtained through joint training.",
}
@inproceedings{zhang2023citb,
    title = "{CITB}: A Benchmark for Continual Instruction Tuning",
    author = "Zhang, Zihan  and
      Fang, Meng  and
      Chen, Ling  and
      Namazi-Rad, Mohammad-Reza",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.633",
    doi = "10.18653/v1/2023.findings-emnlp.633",
    pages = "9443--9455",
    abstract = "Continual learning (CL) is a paradigm that aims to replicate the human ability to learn and accumulate knowledge continually without forgetting previous knowledge and transferring it to new tasks. Recent instruction tuning (IT) involves fine-tuning models to make them more adaptable to solving NLP tasks in general. However, it is still uncertain how instruction tuning works in the context of CL tasks. This challenging yet practical problem is formulated as Continual Instruction Tuning (CIT). In this work, we establish a CIT benchmark consisting of learning and evaluation protocols. We curate two long dialogue task streams of different types, InstrDialog and InstrDialog++, to study various CL methods systematically. Our experiments show that existing CL methods do not effectively leverage the rich natural language instructions, and fine-tuning an instruction-tuned model sequentially can yield similar or better results. We further explore different aspects that might affect the learning of CIT. We hope this benchmark will facilitate more research in this direction.",
}
% MLLM
@misc{chen2024coin,
      title={CoIN: A Benchmark of Continual Instruction tuNing for Multimodel Large Language Model}, 
      author={Cheng Chen and Junchen Zhu and Xu Luo and Hengtao Shen and Lianli Gao and Jingkuan Song},
      year={2024},
      eprint={2403.08350},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{huang2024mitigating,
      title={Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal}, 
      author={Jianheng Huang and Leyang Cui and Ante Wang and Chengyi Yang and Xinting Liao and Linfeng Song and Junfeng Yao and Jinsong Su},
      year={2024},
      eprint={2403.01244},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{he2024dont,
      title={Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning}, 
      author={Yongquan He and Xuancheng Huang and Minghao Tang and Lingxun Meng and Xiang Li and Wei Lin and Wenyuan Zhang and Yifu Gao},
      year={2024},
      eprint={2403.10056},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{yin2022contintin,
    title = "{C}on{T}in{T}in: Continual Learning from Task Instructions",
    author = "Yin, Wenpeng  and
      Li, Jia  and
      Xiong, Caiming",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.218",
    doi = "10.18653/v1/2022.acl-long.218",
    pages = "3062--3072",
    abstract = "The mainstream machine learning paradigms for NLP often work with two underlying presumptions. First, the target task is predefined and static; a system merely needs to learn to solve it exclusively. Second, the supervision of a task mainly comes from a set of labeled examples. A question arises: how to build a system that can keep learning new tasks from their instructions?This work defines a new learning paradigm ConTinTin (Continual Learning from Task Instructions), in which a system should learn a sequence of new tasks one by one, each task is explained by a piece of textual instruction. The system is required to (i) generate the expected outputs of a new task by learning from its instruction, (ii) transfer the knowledge acquired from upstream tasks to help solve downstream tasks (i.e., forward-transfer), and (iii) retain or even improve the performance on earlier tasks after learning new tasks (i.e., backward-transfer). This new problem is studied on a stream of more than 60 tasks, each equipped with an instruction. Technically, our method InstructionSpeak contains two strategies that make full use of task instructions to improve forward-transfer and backward-transfer: one is to learn from negative outputs, the other is to re-visit instructions of previous tasks. To our knowledge, this is the first time to study ConTinTin in NLP. In addition to the problem formulation and our promising approach, this work also contributes to providing rich analyses for the community to better understand this novel learning problem.",
}
@inproceedings{wang2023orthogonal,
    title = "Orthogonal Subspace Learning for Language Model Continual Learning",
    author = "Wang, Xiao  and
      Chen, Tianze  and
      Ge, Qiming  and
      Xia, Han  and
      Bao, Rong  and
      Zheng, Rui  and
      Zhang, Qi  and
      Gui, Tao  and
      Huang, Xuanjing",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.715",
    doi = "10.18653/v1/2023.findings-emnlp.715",
    pages = "10658--10671",
    abstract = "Benefiting from massive corpora and advanced hardware, large language models (LLMs) exhibit remarkable capabilities in language understanding and generation. However, their performance degrades in scenarios where multiple tasks are encountered sequentially, also known as catastrophic forgetting. In this paper, we propose orthogonal low-rank adaptation (O-LoRA), a simple and efficient approach for continual learning in language models, effectively mitigating catastrophic forgetting while learning new tasks. Specifically, O-LoRA learns tasks in different (low-rank) vector subspaces that are kept orthogonal to each other in order to minimize interference. Our method induces only marginal additional parameter costs and requires no user data storage for replay. Experimental results on continual learning benchmarks show that our method outperforms state-of-the-art methods. Furthermore, compared to previous approaches, our method excels in preserving the generalization ability of LLMs on unseen tasks.",
}
@misc{zhao2024sapt,
      title={SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models}, 
      author={Weixiang Zhao and Shilong Wang and Yulin Hu and Yanyan Zhao and Bing Qin and Xuanyu Zhang and Qing Yang and Dongliang Xu and Wanxiang Che},
      year={2024},
      eprint={2401.08295},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{wei2022finetuned,
      title={Finetuned Language Models Are Zero-Shot Learners}, 
      author={Jason Wei and Maarten Bosma and Vincent Y. Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},
      year={2022},
      eprint={2109.01652},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{wang2024inscl,
      title={InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions}, 
      author={Yifan Wang and Yafei Liu and Chufan Shi and Haoling Li and Chen Chen and Haonan Lu and Yujiu Yang},
      year={2024},
      eprint={2403.11435},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{zhu2023minigpt4,
      title={MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models}, 
      author={Deyao Zhu and Jun Chen and Xiaoqian Shen and Xiang Li and Mohamed Elhoseiny},
      year={2023},
      eprint={2304.10592},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{liu2023visual,
      title={Visual Instruction Tuning}, 
      author={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
      year={2023},
      eprint={2304.08485},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{qi2024interactive,
      title={Interactive Continual Learning: Fast and Slow Thinking}, 
      author={Biqing Qi and Xingquan Chen and Junqi Gao and Dong Li and Jianxing Liu and Ligang Wu and Bowen Zhou},
      year={2024},
      eprint={2403.02628},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{he2023continual,
      title={Continual Instruction Tuning for Large Multimodal Models}, 
      author={Jinghan He and Haiyun Guo and Ming Tang and Jinqiao Wang},
      year={2023},
      eprint={2311.16206},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{zhu2024model,
      title={Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models}, 
      author={Didi Zhu and Zhongyi Sun and Zexi Li and Tao Shen and Ke Yan and Shouhong Ding and Kun Kuang and Chao Wu},
      year={2024},
      eprint={2402.12048},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{zhao2024reconstruct,
      title={Reconstruct before Query: Continual Missing Modality Learning with Decomposed Prompt Collaboration}, 
      author={Shu Zhao and Xiaohan Zou and Tan Yu and Huijuan Xu},
      year={2024},
      eprint={2403.11373},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{zhai2023investigating,
      title={Investigating the Catastrophic Forgetting in Multimodal Large Language Models}, 
      author={Yuexiang Zhai and Shengbang Tong and Xiao Li and Mu Cai and Qing Qu and Yong Jae Lee and Yi Ma},
      year={2023},
      eprint={2309.10313},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{peng2023kosmos2,
      title={Kosmos-2: Grounding Multimodal Large Language Models to the World}, 
      author={Zhiliang Peng and Wenhui Wang and Li Dong and Yaru Hao and Shaohan Huang and Shuming Ma and Furu Wei},
      year={2023},
      eprint={2306.14824},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{li2023otter,
      title={Otter: A Multi-Modal Model with In-Context Instruction Tuning}, 
      author={Bo Li and Yuanhan Zhang and Liangyu Chen and Jinghao Wang and Jingkang Yang and Ziwei Liu},
      year={2023},
      eprint={2305.03726},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{dai2023instructblip,
      title={InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning}, 
      author={Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi},
      year={2023},
      eprint={2305.06500},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{li2024videochat,
      title={VideoChat: Chat-Centric Video Understanding}, 
      author={KunChang Li and Yinan He and Yi Wang and Yizhuo Li and Wenhai Wang and Ping Luo and Yali Wang and Limin Wang and Yu Qiao},
      year={2024},
      eprint={2305.06355},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

% LLMs 
@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}
@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}
@article{anil2023palm,
  title={Palm 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403},
  year={2023}
}
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}
@article{achiam2022chatgpt,
  title={Introducing chatgpt. [Online]. Available: \url{https://openai.com/blog/chatgpt}},
  author={OpenAI},
  year={2022}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@misc{luo2023empirical,
      title={An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning}, 
      author={Yun Luo and Zhen Yang and Fandong Meng and Yafu Li and Jie Zhou and Yue Zhang},
      year={2023},
      eprint={2308.08747},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ghosh2024closer,
      title={A Closer Look at the Limitations of Instruction Tuning}, 
      author={Sreyan Ghosh and Chandra Kiran Reddy Evuru and Sonal Kumar and Ramaneswaran S and Deepali Aneja and Zeyu Jin and Ramani Duraiswami and Dinesh Manocha},
      year={2024},
      eprint={2402.05119},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zenke2017continual,
      title={Continual Learning Through Synaptic Intelligence}, 
      author={Friedemann Zenke and Ben Poole and Surya Ganguli},
      year={2017},
      eprint={1703.04200},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{madotto2020continual,
      title={Continual Learning in Task-Oriented Dialogue Systems}, 
      author={Andrea Madotto and Zhaojiang Lin and Zhenpeng Zhou and Seungwhan Moon and Paul Crook and Bing Liu and Zhou Yu and Eunjoon Cho and Zhiguang Wang},
      year={2020},
      eprint={2012.15504},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{shin2017continual,
      title={Continual Learning with Deep Generative Replay}, 
      author={Hanul Shin and Jung Kwon Lee and Jaehong Kim and Jiwon Kim},
      year={2017},
      eprint={1705.08690},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


% Interesting works discussed.
@inproceedings{zhang2022continual,
    title = "Continual Sequence Generation with Adaptive Compositional Modules",
    author = "Zhang, Yanzhe  and
      Wang, Xuezhi  and
      Yang, Diyi",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.255",
    doi = "10.18653/v1/2022.acl-long.255",
    pages = "3653--3667",
    abstract = "Continual learning is essential for real-world deployment when there is a need to quickly adapt the model to new tasks without forgetting knowledge of old tasks. Existing work on continual sequence generation either always reuses existing parameters to learn new tasks, which is vulnerable to catastrophic forgetting on dissimilar tasks, or blindly adds new parameters for every new task, which could prevent knowledge sharing between similar tasks. To get the best of both worlds, in this work, we propose continual sequence generation with adaptive compositional modules to adaptively add modules in transformer architectures and compose both old and new modules for new tasks. We also incorporate pseudo experience replay to facilitate knowledge transfer in those shared modules. Experiment results on various sequences of generation tasks show that our framework can adaptively add modules or reuse modules based on task similarity, outperforming state-of-the-art baselines in terms of both performance and parameter efficiency. We make our code public at \url{https://github.com/GT-SALT/Adaptive-Compositional-Modules}.",
}
@inproceedings{ying2021mitigating,
     author = {Yin, Haiyan and yang, peng and Li, Ping},
     booktitle = {Advances in Neural Information Processing Systems},
     editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
     pages = {10260--10272},
     publisher = {Curran Associates, Inc.},
     title = {Mitigating Forgetting in Online Continual Learning with  Neuron Calibration},
     url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/54ee290e80589a2a1225c338a71839f5-Paper.pdf},
     volume = {34},
     year = {2021}
}

@inproceedings{yang2022continual,
    title = "Continual Learning for Natural Language Generations with Transformer Calibration",
    author = "Yang, Peng  and
      Li, Dingcheng  and
      Li, Ping",
    editor = "Fokkens, Antske  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 26th Conference on Computational Natural Language Learning (CoNLL)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.conll-1.4",
    doi = "10.18653/v1/2022.conll-1.4",
    pages = "40--49",
    abstract = "Conventional natural language process (NLP) generation models are trained offline with a given dataset for a particular task, which is referred to as isolated learning. Research on sequence-to-sequence language generation aims to study continual learning model to constantly learning from sequentially encountered tasks. However, continual learning studies often suffer from catastrophic forgetting, a persistent challenge for lifelong learning. In this paper, we present a novel NLP transformer model that attempts to mitigate catastrophic forgetting in online continual learning from a new perspective, i.e., attention calibration. We model the attention in the transformer as a calibrated unit in a general formulation, where the attention calibration could give benefits to balance the stability and plasticity of continual learning algorithms through influencing both their forward inference path and backward optimization path. Our empirical experiments, paraphrase generation and dialog response generation, demonstrate that this work outperforms state-of-the-art models by a considerable margin and effectively mitigate the forgetting.",
}
@inproceedings{wang2022online,
  title={Online continual learning with contrastive vision transformer},
  author={Wang, Zhen and Liu, Liu and Kong, Yajing and Guo, Jiaxian and Tao, Dacheng},
  booktitle={European Conference on Computer Vision},
  pages={631--650},
  year={2022},
  organization={Springer}
}
@misc{bornschein2024transformers,
      title={Transformers for Supervised Online Continual Learning}, 
      author={Jorg Bornschein and Yazhe Li and Amal Rannen-Triki},
      year={2024},
      eprint={2403.01554},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{lu2023ibcl,
      title={IBCL: Zero-shot Model Generation for Task Trade-offs in Continual Learning}, 
      author={Pengyuan Lu and Michele Caprio and Eric Eaton and Insup Lee},
      year={2023},
      eprint={2310.02995},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{nguyen2022survey,
      title={A Survey of Machine Unlearning}, 
      author={Thanh Tam Nguyen and Thanh Trung Huynh and Phi Le Nguyen and Alan Wee-Chung Liew and Hongzhi Yin and Quoc Viet Hung Nguyen},
      year={2022},
      eprint={2209.02299},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{bourtoule2020machine,
      title={Machine Unlearning}, 
      author={Lucas Bourtoule and Varun Chandrasekaran and Christopher A. Choquette-Choo and Hengrui Jia and Adelin Travers and Baiwu Zhang and David Lie and Nicolas Papernot},
      year={2020},
      eprint={1912.03817},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}
@InProceedings{pourcel2022online,
    author="Pourcel, Julien
    and Vu, Ngoc-Son
    and French, Robert M.",
    editor="Avidan, Shai
    and Brostow, Gabriel
    and Ciss{\'e}, Moustapha
    and Farinella, Giovanni Maria
    and Hassner, Tal",
    title="Online Task-free Continual Learning with Dynamic Sparse Distributed Memory",
    booktitle="Computer Vision -- ECCV 2022",
    year="2022",
    publisher="Springer Nature Switzerland",
    address="Cham",
    pages="739--756",
    abstract="This paper addresses the very challenging problem of online task-free continual learning in which a sequence of new tasks is learned from non-stationary data using each sample only once for training and without knowledge of task boundaries. We propose in this paper an efficient semi-distributed associative memory algorithm called Dynamic Sparse Distributed Memory (DSDM) where learning and evaluating can be carried out at any point of time. DSDM evolves dynamically and continually modeling the distribution of any non-stationary data stream. DSDM relies on locally distributed, but only partially overlapping clusters of representations to effectively eliminate catastrophic forgetting, while at the same time, maintaining the generalization capacities of distributed networks. In addition, a local density-based pruning technique is used to control the network's memory footprint. DSDM significantly outperforms state-of-the-art continual learning methods on different image classification baselines, even in a low data regime. Code is publicly available: https://github.com/Julien-pour/Dynamic-Sparse-Distributed-Memory.",
    isbn="978-3-031-19806-9"
}
@misc{ramsauer2021hopfield,
      title={Hopfield Networks is All You Need}, 
      author={Hubert Ramsauer and Bernhard Schäfl and Johannes Lehner and Philipp Seidl and Michael Widrich and Thomas Adler and Lukas Gruber and Markus Holzleitner and Milena Pavlović and Geir Kjetil Sandve and Victor Greiff and David Kreil and Michael Kopp and Günter Klambauer and Johannes Brandstetter and Sepp Hochreiter},
      year={2021},
      eprint={2008.02217},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}
@article{wu2018kanerva,
  title={The kanerva machine: A generative distributed memory},
  author={Wu, Yan and Wayne, Greg and Graves, Alex and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:1804.01756},
  year={2018}
}
@article{wang2022sparcl,
  title={Sparcl: Sparse continual learning on the edge},
  author={Wang, Zifeng and Zhan, Zheng and Gong, Yifan and Yuan, Geng and Niu, Wei and Jian, Tong and Ren, Bin and Ioannidis, Stratis and Wang, Yanzhi and Dy, Jennifer},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={20366--20380},
  year={2022}
}
@inproceedings{prabhu2023computationally,
  title={Computationally budgeted continual learning: What does matter?},
  author={Prabhu, Ameya and Al Kader Hammoud, Hasan Abed and Dokania, Puneet K and Torr, Philip HS and Lim, Ser-Nam and Ghanem, Bernard and Bibi, Adel},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3698--3707},
  year={2023}
}
@misc{jin2024model,
      title={What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement}, 
      author={Xisen Jin and Xiang Ren},
      year={2024},
      eprint={2402.01865},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{wistuba2023continual,
      title={Continual Learning with Low Rank Adaptation}, 
      author={Martin Wistuba and Prabhu Teja Sivaprasad and Lukas Balles and Giovanni Zappella},
      year={2023},
      eprint={2311.17601},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{huang2023lorahub,
  title={Lorahub: Efficient cross-task generalization via dynamic lora composition},
  author={Huang, Chengsong and Liu, Qian and Lin, Bill Yuchen and Pang, Tianyu and Du, Chao and Lin, Min},
  journal={arXiv preprint arXiv:2307.13269},
  year={2023}
}
@inproceedings{shao2023class,
    title = "Class-Incremental Learning based on Label Generation",
    author = "Shao, Yijia  and
      Guo, Yiduo  and
      Zhao, Dongyan  and
      Liu, Bing",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-short.109",
    doi = "10.18653/v1/2023.acl-short.109",
    pages = "1263--1276",
    abstract = "Despite the great success of pre-trained language models, it is still a challenge to use these models for continual learning, especially for the class-incremental learning (CIL) setting due to catastrophic forgetting (CF). This paper reports our finding that if we formulate CIL as a continual label generation problem, CF is drastically reduced and the generalizable representations of pre-trained models can be better retained. We thus propose a new CIL method (VAG) that also leverages the sparsity of vocabulary to focus the generation and creates pseudo-replay samples by using label semantics. Experimental results show that VAG outperforms baselines by a large margin.",
}
@InProceedings{dalessandro2023multimodal,
    author    = {D'Alessandro, Marco and Alonso, Alberto and Calabr\'es, Enrique and Galar, Mikel},
    title     = {Multimodal Parameter-Efficient Few-Shot Class Incremental Learning},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops},
    month     = {October},
    year      = {2023},
    pages     = {3393-3403}
}
@article{cao2024generative,
  title={Generative Multi-modal Models are Good Class Incremental Learners},
  author={Cao, Xusheng and Lu, Haori and Huang, Linlan and Liu, Xialei and Cheng, Ming-Ming},
  journal={IEEE Computer Vision and Pattern Recognition (CVPR)},
  year={2024}
}
@misc{yang2024reawakening,
      title={Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training}, 
      author={Yanlai Yang and Matt Jones and Michael C. Mozer and Mengye Ren},
      year={2024},
      eprint={2403.09613},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wang2022supernaturalinstructions,
      title={Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks}, 
      author={Yizhong Wang and Swaroop Mishra and Pegah Alipoormolabashi and Yeganeh Kordi and Amirreza Mirzaei and Anjana Arunkumar and Arjun Ashok and Arut Selvan Dhanasekaran and Atharva Naik and David Stap and Eshaan Pathak and Giannis Karamanolakis and Haizhi Gary Lai and Ishan Purohit and Ishani Mondal and Jacob Anderson and Kirby Kuznia and Krima Doshi and Maitreya Patel and Kuntal Kumar Pal and Mehrad Moradshahi and Mihir Parmar and Mirali Purohit and Neeraj Varshney and Phani Rohitha Kaza and Pulkit Verma and Ravsehaj Singh Puri and Rushang Karia and Shailaja Keyur Sampat and Savan Doshi and Siddhartha Mishra and Sujan Reddy and Sumanta Patro and Tanay Dixit and Xudong Shen and Chitta Baral and Yejin Choi and Noah A. Smith and Hannaneh Hajishirzi and Daniel Khashabi},
      year={2022},
      eprint={2204.07705},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{mishra2021natural,
  title={Natural Instructions: Benchmarking Generalization to New Tasks from Natural Language Instructions},
  author={Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2104.08773},
  year={2021}
}

@misc{zhang2024instruction,
      title={Instruction Tuning for Large Language Models: A Survey}, 
      author={Shengyu Zhang and Linfeng Dong and Xiaoya Li and Sen Zhang and Xiaofei Sun and Shuhe Wang and Jiwei Li and Runyi Hu and Tianwei Zhang and Fei Wu and Guoyin Wang},
      year={2024},
      eprint={2308.10792},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{jiang2024instructiontuned,
      title={Instruction-tuned Language Models are Better Knowledge Learners}, 
      author={Zhengbao Jiang and Zhiqing Sun and Weijia Shi and Pedro Rodriguez and Chunting Zhou and Graham Neubig and Xi Victoria Lin and Wen-tau Yih and Srinivasan Iyer},
      year={2024},
      eprint={2402.12847},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{sanh2022multitask,
      title={Multitask Prompted Training Enables Zero-Shot Task Generalization}, 
      author={Victor Sanh and Albert Webson and Colin Raffel and Stephen H. Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Teven Le Scao and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Tali Bers and Stella Biderman and Leo Gao and Thomas Wolf and Alexander M. Rush},
      year={2022},
      eprint={2110.08207},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{goyal2017making,
      title={Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering}, 
      author={Yash Goyal and Tejas Khot and Douglas Summers-Stay and Dhruv Batra and Devi Parikh},
      year={2017},
      eprint={1612.00837},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{gurari2018vizwiz,
      title={VizWiz Grand Challenge: Answering Visual Questions from Blind People}, 
      author={Danna Gurari and Qing Li and Abigale J. Stangl and Anhong Guo and Chi Lin and Kristen Grauman and Jiebo Luo and Jeffrey P. Bigham},
      year={2018},
      eprint={1802.08218},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{lu2022learn,
      title={Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering}, 
      author={Pan Lu and Swaroop Mishra and Tony Xia and Liang Qiu and Kai-Wei Chang and Song-Chun Zhu and Oyvind Tafjord and Peter Clark and Ashwin Kalyan},
      year={2022},
      eprint={2209.09513},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{singh2019vqa,
      title={Towards VQA Models That Can Read}, 
      author={Amanpreet Singh and Vivek Natarajan and Meet Shah and Yu Jiang and Xinlei Chen and Dhruv Batra and Devi Parikh and Marcus Rohrbach},
      year={2019},
      eprint={1904.08920},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hudson2019gqa,
      title={GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering}, 
      author={Drew A. Hudson and Christopher D. Manning},
      year={2019},
      eprint={1902.09506},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@InProceedings{mishraICDAR19,
  author    = "Anand Mishra and Shashank Shekhar and Ajeet Kumar Singh and Anirban Chakraborty",
  title     = "OCR-VQA: Visual Question Answering by Reading Text in Images",
  booktitle = "ICDAR",
  year      = "2019",
}

@inproceedings{imagenet_cvpr09,
        AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
        TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},
        BOOKTITLE = {CVPR09},
        YEAR = {2009},
        BIBSOURCE = "http://www.image-net.org/papers/imagenet_cvpr09.bib"}

@inproceedings{kazemzadeh-etal-2014-referitgame,
    title = "{R}efer{I}t{G}ame: Referring to Objects in Photographs of Natural Scenes",
    author = "Kazemzadeh, Sahar  and
      Ordonez, Vicente  and
      Matten, Mark  and
      Berg, Tamara",
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1086",
    doi = "10.3115/v1/D14-1086",
    pages = "787--798",
}

@misc{mao2016generation,
      title={Generation and Comprehension of Unambiguous Object Descriptions}, 
      author={Junhua Mao and Jonathan Huang and Alexander Toshev and Oana Camburu and Alan Yuille and Kevin Murphy},
      year={2016},
      eprint={1511.02283},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{shah2023trillion,
      title={Trillion Dollar Words: A New Financial Dataset, Task \& Market Analysis}, 
      author={Agam Shah and Suvan Paturi and Sudheer Chava},
      year={2023},
      eprint={2305.07972},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hu2023meetingbank,
      title={MeetingBank: A Benchmark Dataset for Meeting Summarization}, 
      author={Yebowen Hu and Tim Ganter and Hanieh Deilamsalehy and Franck Dernoncourt and Hassan Foroosh and Fei Liu},
      year={2023},
      eprint={2305.17529},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{zhao-etal-2023-c,
    title = "{C}-{STANCE}: A Large Dataset for {C}hinese Zero-Shot Stance Detection",
    author = "Zhao, Chenye  and
      Li, Yingjie  and
      Caragea, Cornelia",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.747",
    doi = "10.18653/v1/2023.acl-long.747",
    pages = "13369--13385",
    abstract = "Zero-shot stance detection (ZSSD) aims to determine whether the author of a text is in favor of, against, or neutral toward a target that is unseen during training. Despite the growing attention on ZSSD, most recent advances in this task are limited to English and do not pay much attention to other languages such as Chinese. To support ZSSD research, in this paper, we present C-STANCE that, to our knowledge, is the first Chinese dataset for zero-shot stance detection. We introduce two challenging subtasks for ZSSD: target-based ZSSD and domain-based ZSSD. Our dataset includes both noun-phrase targets and claim targets, covering a wide range of domains. We provide a detailed description and analysis of our dataset. To establish results on C-STANCE, we report performance scores using state-of-the-art deep learning models. We publicly release our dataset and code to facilitate future research.",
}

@inproceedings{kew-etal-2023-20,
    title = "20 Minuten: A Multi-task News Summarisation Dataset for {G}erman",
    author = "Kew, Tannon  and
      Kostrzewa, Marek  and
      Ebling, Sarah",
    editor = {Ghorbel, Hatem  and
      Sokhn, Maria  and
      Cieliebak, Mark  and
      H{\"u}rlimann, Manuela  and
      de Salis, Emmanuel  and
      Guerne, Jonathan},
    booktitle = "Proceedings of the 8th edition of the Swiss Text Analytics Conference",
    month = jun,
    year = "2023",
    address = "Neuchatel, Switzerland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.swisstext-1.1",
    pages = "1--13",
}

@misc{lu2021codexglue,
      title={CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation}, 
      author={Shuai Lu and Daya Guo and Shuo Ren and Junjie Huang and Alexey Svyatkovskiy and Ambrosio Blanco and Colin Clement and Dawn Drain and Daxin Jiang and Duyu Tang and Ge Li and Lidong Zhou and Linjun Shou and Long Zhou and Michele Tufano and Ming Gong and Ming Zhou and Nan Duan and Neel Sundaresan and Shao Kun Deng and Shengyu Fu and Shujie Liu},
      year={2021},
      eprint={2102.04664},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{mishra2022numglue,
      title={NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks}, 
      author={Swaroop Mishra and Arindam Mitra and Neeraj Varshney and Bhavdeep Sachdeva and Peter Clark and Chitta Baral and Ashwin Kalyan},
      year={2022},
      eprint={2204.05660},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{huang2019cosmos,
      title={Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning}, 
      author={Lifu Huang and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},
      year={2019},
      eprint={1909.00277},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{khashabi-etal-2017-learning,
    title = "Learning What is Essential in Questions",
    author = "Khashabi, Daniel  and
      Khot, Tushar  and
      Sabharwal, Ashish  and
      Roth, Dan",
    editor = "Levy, Roger  and
      Specia, Lucia",
    booktitle = "Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K17-1010",
    doi = "10.18653/v1/K17-1010",
    pages = "80--89",
    abstract = "Question answering (QA) systems are easily distracted by irrelevant or redundant words in questions, especially when faced with long or multi-sentence questions in difficult domains. This paper introduces and studies the notion of essential question terms with the goal of improving such QA solvers. We illustrate the importance of essential question terms by showing that humans{'} ability to answer questions drops significantly when essential terms are eliminated from questions. We then develop a classifier that reliably (90{\%} mean average precision) identifies and ranks essential terms in questions. Finally, we use the classifier to demonstrate that the notion of question term essentiality allows state-of-the-art QA solver for elementary-level science questions to make better and more informed decisions,improving performance by up to 5{\%}.We also introduce a new dataset of over 2,200 crowd-sourced essential terms annotated science questions.",
}
@misc{zhou2019goingvacationtakeslonger,
      title={"Going on a vacation" takes longer than "Going for a walk": A Study of Temporal Commonsense Understanding}, 
      author={Ben Zhou and Daniel Khashabi and Qiang Ning and Dan Roth},
      year={2019},
      eprint={1909.03065},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.03065}, 
}

@inproceedings{khashabi-etal-2018-looking,
  title={Looking beyond the surface: A challenge set for reading comprehension over multiple sentences},
  author={Khashabi, Daniel and Chaturvedi, Snigdha and Roth, Michael and Upadhyay, Shyam and Roth, Dan},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  pages={252--262},
  year={2018}
}

@misc{khot2020qasc,
      title={QASC: A Dataset for Question Answering via Sentence Composition}, 
      author={Tushar Khot and Peter Clark and Michal Guerquin and Peter Jansen and Ashish Sabharwal},
      year={2020},
      eprint={1910.11473},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{dasigi-etal-2019-quoref,
    title = "{Q}uoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning",
    author = "Dasigi, Pradeep  and
      Liu, Nelson F.  and
      Marasovi{\'c}, Ana  and
      Smith, Noah A.  and
      Gardner, Matt",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1606",
    doi = "10.18653/v1/D19-1606",
    pages = "5925--5932",
    abstract = "Machine comprehension of texts longer than a single sentence often requires coreference resolution. However, most current reading comprehension benchmarks do not contain complex coreferential phenomena and hence fail to evaluate the ability of models to resolve coreference. We present a new crowdsourced dataset containing more than 24K span-selection questions that require resolving coreference among entities in over 4.7K English paragraphs from Wikipedia. Obtaining questions focused on such phenomena is challenging, because it is hard to avoid lexical cues that shortcut complex reasoning. We deal with this issue by using a strong baseline model as an adversary in the crowdsourcing loop, which helps crowdworkers avoid writing questions with exploitable surface cues. We show that state-of-the-art reading comprehension models perform significantly worse than humans on this benchmark{---}the best model performance is 70.5 F1, while the estimated human performance is 93.4 F1.",
}

@misc{lin2019reasoning,
      title={Reasoning Over Paragraph Effects in Situations}, 
      author={Kevin Lin and Oyvind Tafjord and Peter Clark and Matt Gardner},
      year={2019},
      eprint={1908.05852},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{sakaguchi2019winogrande,
      title={WinoGrande: An Adversarial Winograd Schema Challenge at Scale}, 
      author={Keisuke Sakaguchi and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},
      year={2019},
      eprint={1907.10641},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


%%%%%%%%%%%%%%%%%%%%% V2 %%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Added from 01/20/2024 - 05/20/2024
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CPT
@article{zhao2024large,
  title={Large Language Model Can Continue Evolving From Mistakes},
  author={Zhao, Haokun and Han, Haixia and Shi, Jie and Du, Chengyu and Liang, Jiaqing and Xiao, Yanghua},
  journal={arXiv preprint arXiv:2404.08707},
  year={2024}
}
@article{lin2024rho,
  title={Rho-1: Not All Tokens Are What You Need},
  author={Lin, Zhenghao and Gou, Zhibin and Gong, Yeyun and Liu, Xiao and Shen, Yelong and Xu, Ruochen and Lin, Chen and Yang, Yujiu and Jiao, Jian and Duan, Nan and others},
  journal={arXiv preprint arXiv:2404.07965},
  year={2024}
}
@article{ibrahim2024simple,
  title={Simple and scalable strategies to continually pre-train large language models},
  author={Ibrahim, Adam and Th{\'e}rien, Benjamin and Gupta, Kshitij and Richter, Mats L and Anthony, Quentin and Lesort, Timoth{\'e}e and Belilovsky, Eugene and Rish, Irina},
  journal={arXiv preprint arXiv:2403.08763},
  year={2024}
}
@article{yildiz2024investigating,
  title={Investigating Continual Pretraining in Large Language Models: Insights and Implications},
  author={Y{\i}ld{\i}z, {\c{C}}a{\u{g}}atay and Ravichandran, Nishaanth Kanna and Punia, Prishruit and Bethge, Matthias and Ermis, Beyza},
  journal={arXiv preprint arXiv:2402.17400},
  year={2024}
}
@article{chen2024take,
  title={Take the Bull by the Horns: Hard Sample-Reweighted Continual Training Improves LLM Generalization},
  author={Chen, Xuxi and Wang, Zhendong and Sow, Daouda and Yang, Junjie and Chen, Tianlong and Liang, Yingbin and Zhou, Mingyuan and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2402.14270},
  year={2024}
}

% DAP
@article{li2024blade,
  title={BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific Models},
  author={Li, Haitao and Ai, Qingyao and Chen, Jia and Dong, Qian and Wu, Zhijing and Liu, Yiqun and Chen, Chong and Tian, Qi},
  journal={arXiv preprint arXiv:2403.18365},
  year={2024}
}
@article{shen2024tag,
  title={Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains},
  author={Shen, Junhong and Tenenholtz, Neil and Hall, James Brian and Alvarez-Melis, David and Fusi, Nicolo},
  journal={arXiv preprint arXiv:2402.05140},
  year={2024}
}
@article{acikgoz2024hippocrates,
  title={Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare},
  author={Acikgoz, Emre Can and {\.I}nce, Osman Batur and Bench, Rayene and Boz, Arda An{\i}l and Kesen, {\.I}lker and Erdem, Aykut and Erdem, Erkut},
  journal={arXiv preprint arXiv:2404.16621},
  year={2024}
}
@article{he2024foundation,
  title={Foundation Model for Advancing Healthcare: Challenges, Opportunities, and Future Directions},
  author={He, Yuting and Huang, Fuxiang and Jiang, Xinrui and Nie, Yuxiang and Wang, Minghao and Wang, Jiguang and Chen, Hao},
  journal={arXiv preprint arXiv:2404.03264},
  year={2024}
}
@article{xie2024me,
  title={Me LLaMA: Foundation Large Language Models for Medical Applications},
  author={Xie, Qianqian and Chen, Qingyu and Chen, Aokun and Peng, Cheng and Hu, Yan and Lin, Fongci and Peng, Xueqing and Huang, Jimin and Zhang, Jeffrey and Keloth, Vipina and others},
  journal={arXiv preprint arXiv:2402.12749},
  year={2024}
}
@article{hirano2024construction,
  title={Construction of Domain-specified Japanese Large Language Model for Finance through Continual Pre-training},
  author={Hirano, Masanori and Imajo, Kentaro},
  journal={arXiv preprint arXiv:2404.10555},
  year={2024}
}
@article{takahashi2024pretraining,
  title={Pretraining and updating language-and domain-specific large language model: A case study in japanese business domain},
  author={Takahashi, Kosuke and Omi, Takahiro and Arima, Kosuke and Ishigaki, Tatsuya},
  journal={arXiv preprint arXiv:2404.08262},
  year={2024}
}
@article{thulke2024climategpt,
  title={ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on Climate Change},
  author={Thulke, David and Gao, Yingbo and Pelser, Petrus and Brune, Rein and Jalota, Rricha and Fok, Floris and Ramos, Michael and van Wyk, Ian and Nasir, Abdallah and Goldstein, Hayden and others},
  journal={arXiv preprint arXiv:2401.09646},
  year={2024}
}
@article{fujii2024continual,
  title={Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities},
  author={Fujii, Kazuki and Nakamura, Taishi and Loem, Mengsay and Iida, Hiroki and Ohi, Masanari and Hattori, Kakeru and Shota, Hirai and Mizuki, Sakae and Yokota, Rio and Okazaki, Naoaki},
  journal={arXiv preprint arXiv:2404.17790},
  year={2024}
}
@article{dou2024sailor,
  title={Sailor: Open Language Models for South-East Asia},
  author={Dou, Longxu and Liu, Qian and Zeng, Guangtao and Guo, Jia and Zhou, Jiahui and Lu, Wei and Lin, Min},
  journal={arXiv preprint arXiv:2404.03608},
  year={2024}
}
@article{nakamura2024aurora,
  title={Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the US Executive Order},
  author={Nakamura, Taishi and Mishra, Mayank and Tedeschi, Simone and Chai, Yekun and Stillerman, Jason T and Friedrich, Felix and Yadav, Prateek and Laud, Tanmay and Chien, Vu Minh and Zhuo, Terry Yue and others},
  journal={arXiv preprint arXiv:2404.00399},
  year={2024}
}
@article{vo2024vi,
  title={Vi-Mistral-X: Building a Vietnamese Language Model with Advanced Continual Pre-training},
  author={Vo, James},
  journal={arXiv preprint arXiv:2403.15470},
  year={2024}
}
@article{wang2024wise,
  title={WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models},
  author={Wang, Peng and Li, Zexi and Zhang, Ningyu and Xu, Ziwen and Yao, Yunzhi and Jiang, Yong and Xie, Pengjun and Huang, Fei and Chen, Huajun},
  journal={arXiv preprint arXiv:2405.14768},
  year={2024}
}
@article{yu2024boosting,
  title={Boosting continual learning of vision-language models via mixture-of-experts adapters},
  author={Yu, Jiazuo and Zhuge, Yunzhi and Zhang, Lu and Wang, Dong and Lu, Huchuan and He, You},
  journal={arXiv preprint arXiv:2403.11549},
  year={2024}
}
@article{yu2024select,
  title={Select and Distill: Selective Dual-Teacher Knowledge Transfer for Continual Learning on Vision-Language Models},
  author={Yu, Yu-Chu and Huang, Chi-Pin and Chen, Jr-Jen and Chang, Kai-Po and Lai, Yung-Hsuan and Yang, Fu-En and Wang, Yu-Chiang Frank},
  journal={arXiv preprint arXiv:2403.09296},
  year={2024}
}
@article{ye2024data,
  title={Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance},
  author={Ye, Jiasheng and Liu, Peiju and Sun, Tianxiang and Zhou, Yunhua and Zhan, Jun and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2403.16952},
  year={2024}
}
@article{fleshman2024adapterswap,
  title={AdapterSwap: Continuous Training of LLMs with Data Removal and Access-Control Guarantees},
  author={Fleshman, William and Khan, Aleem and Marone, Marc and Van Durme, Benjamin},
  journal={arXiv preprint arXiv:2404.08417},
  year={2024}
}
@article{malla2024copal,
  title={COPAL: Continual Pruning in Large Language Generative Models},
  author={Malla, Srikanth and Choi, Joon Hee and Choi, Chiho},
  journal={arXiv preprint arXiv:2405.02347},
  year={2024}
}
@article{gutierrez2024hipporag,
  title={HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models},
  author={Guti{\'e}rrez, Bernal Jim{\'e}nez and Shu, Yiheng and Gu, Yu and Yasunaga, Michihiro and Su, Yu},
  journal={arXiv preprint arXiv:2405.14831},
  year={2024}
}
@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{yang2024recent,
  title={Recent Advances of Foundation Language Models-based Continual Learning: A Survey},
  author={Yang, Yutao and Zhou, Jie and Ding, Xuanwen and Huai, Tianyu and Liu, Shunyu and Chen, Qin and He, Liang and Xie, Yuan},
  journal={arXiv preprint arXiv:2405.18653},
  year={2024}
}

@article{xu2023wizardlm,
  title={Wizardlm: Empowering large language models to follow complex instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  journal={arXiv preprint arXiv:2304.12244},
  year={2023}
}

@article{wang2024codeclm,
  title={CodecLM: Aligning Language Models with Tailored Synthetic Data},
  author={Wang, Zifeng and Li, Chun-Liang and Perot, Vincent and Le, Long T and Miao, Jin and Zhang, Zizhao and Lee, Chen-Yu and Pfister, Tomas},
  journal={arXiv preprint arXiv:2404.05875},
  year={2024}
}

@incollection{mccloskey1989catastrophic,
title = {Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem},
editor = {Gordon H. Bower},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {24},
pages = {109-165},
year = {1989},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(08)60536-8},
url = {https://www.sciencedirect.com/science/article/pii/S0079742108605368},
author = {Michael McCloskey and Neal J. Cohen},
abstract = {Publisher Summary
Connectionist networks in which information is stored in weights on connections among simple processing units have attracted considerable interest in cognitive science. Much of the interest centers around two characteristics of these networks. First, the weights on connections between units need not be prewired by the model builder but rather may be established through training in which items to be learned are presented repeatedly to the network and the connection weights are adjusted in small increments according to a learning algorithm. Second, the networks may represent information in a distributed fashion. This chapter discusses the catastrophic interference in connectionist networks. Distributed representations established through the application of learning algorithms have several properties that are claimed to be desirable from the standpoint of modeling human cognition. These properties include content-addressable memory and so-called automatic generalization in which a network trained on a set of items responds correctly to other untrained items within the same domain. New learning may interfere catastrophically with old learning when networks are trained sequentially. The analysis of the causes of interference implies that at least some interference will occur whenever new learning may alter weights involved in representing old learning, and the simulation results demonstrate only that interference is catastrophic in some specific networks.}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}
@article{reid2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}
@article{kim2023learnability,
  title={Learnability and Algorithm for Continual Learning},
  author={Kim, Gyuhak and Xiao, Changnan and Konishi, Tatsuya and Liu, Bing},
  journal={arXiv preprint arXiv:2306.12646},
  year={2023}
}