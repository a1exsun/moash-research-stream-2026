# 持续学习第一性原理讨论记录（2026-02-24）

## 文档目的

记录本轮从第一性原理出发的深度讨论，核心议题为：

1. 持续学习的三个本质问题（定位/容量/检索）
2. 压缩与可写性的根本张力
3. Complementary Learning Systems（CLS）的正确类比——海马体不是RAG
4. 海马体本身是稀疏的——activation sparsity vs weight sparsity的关键区分
5. 对研究方向的根本性修正：从"是否需要稀疏"到"什么样的表征分离"

本文档不讨论具体的实验提议（P01-P04/P03-2），而是记录更底层的理论思考演进。

---

## 背景与起因

前序讨论已确立以下共识（见 `20260224-0007.md`）：

1. "sparse topology是持续学习的必要条件"在固定架构下是trivially true（因为{dense, sparse}是穷举划分，且dense必然遗忘已被established）。
2. 因此"必要性"本身不是有价值的研究贡献。
3. 现有5个提议（P01-P04, P03-2）被认为不够好，需要从更基础的层面重新思考研究方向。

用户提出关键切入点：**以LLM自身的持续学习困境为例**——本次对话中讨论了有价值的内容，但重启上下文窗口后会全部丢失。如果有足够的研究和计算资源，如何将这次对话存储到模型权重中，成为长期记忆？

这个自指的例子驱动了后续所有的第一性原理分析。

---

## 1. 三个本质问题的提出

### 1.1 从具体例子出发

以LLM（Claude）为研究对象。设其权重为 $\theta \in \mathbb{R}^d$，编码了所有训练阶段获得的知识。本次对话包含新信息（如一个推理模式的修正）。持续学习 = 找到 $\theta'$，使得模型既保留旧知识又整合新知识。

对话中的新信息不是一个简单事实（如"法国首都是巴黎"），而是一个**meta-level的推理模式修正**：

> "当讨论X是必要条件时，先检查补集是否只有一个元素；如果是，且该元素已被否定，则必要性trivially成立。"

这种知识分布在attention circuit和MLP层中的某些pathway上，不是某个孤立参数可以承载的。

### 1.2 三个问题的定义

从"如何将新信息写入权重"这个具体问题出发，自然分解出三个本质子问题：

#### 问题一：定位（Where to write）

**哪些参数需要被修改？**

在dense网络中，功能不是模块化分配的。一个推理pattern散布在成千上万的参数上，与其他pattern高度overlap。无法做surgical update——修改任何一个参数，都会同时影响多个功能。

用loss landscape的语言：哪些参数的梯度方向与旧知识不冲突？
用memory系统的语言：内存分配（allocation）。

当前方法的局限：

- **Fisher Information方法**（EWC, Kirkpatrick et al., 2017）：估计哪些参数对旧任务"重要"，保护它们。但Fisher是对角近似，忽略了参数间的耦合。
- **Gradient Projection方法**（OGD, Farajtabar et al., 2020; GPM, Saha et al., 2021）：找到与旧任务梯度正交的更新方向。但随任务数增长，正交空间被填满。
- **Activation-based方法**：看哪些neuron被旧任务高度激活。但激活≠因果重要性。

这些方法的根本局限：它们都在**参数空间**做定位，但知识不以参数为单位存储——知识存储在**参数之间的关系**（circuit）中。改一个参数，影响的是包含它的所有circuit，而不是某个孤立的"知识单元"。

**Open question：** 能否找到一种表征方式，使得知识的存储单元与参数空间的某个可操作子结构对齐？即让"一个知识"对应"一组可独立修改的参数"？

#### 问题二：容量（How much room is left）

**参数空间中是否有足够的"空闲容量"来存储新知识？**

模型的权重已经被训练数据"填满"了。新信息需要空间。在dense网络中，所有容量都在使用，新旧信息争夺同一组参数。

用loss landscape的语言：$\theta$ 附近是否存在同时满足新旧任务的 $\theta'$？
用memory系统的语言：内存容量（capacity）。

Lottery Ticket Hypothesis（Frankle & Carlin, 2019）告诉我们训练好的dense网络中90%+的参数可以移除而不影响性能。但这个"冗余"可能并非真正空闲——这些参数可能在提供：

- 噪声鲁棒性
- 分布外泛化
- 编码了尚未被当前benchmark测到的知识

**Open question：** 训练后的网络中，有多少容量是**genuinely free**的（可以写入新知识而不影响任何现有功能）？这个free capacity如何随模型规模、训练数据量、压缩程度scaling？这个问题如果有清晰答案，就直接决定了"一个模型在不遗忘的前提下还能学多少新东西"——即持续学习的**容量理论**。

#### 问题三：检索（How to recall）

**新写入的知识能否在正确的上下文被激活？**

即使成功将新知识写入权重，下次遇到相关情境时，模型需要：

1. 识别出当前情境与新知识相关（**pattern matching**）
2. 激活新的推理pattern而非旧的错误pattern（**conflict resolution**）
3. 在没有显式task ID的情况下完成上述操作（**implicit routing**）

在transformer中，routing由attention完成——但attention pattern本身也是可被新训练覆写的。因此存在**meta-stability问题**：写入新知识的过程可能破坏检索旧知识的routing机制。

用loss landscape的语言：$f_{\theta'}$ 能否对正确的输入激活正确的知识？
用memory系统的语言：内存寻址（addressing）。

**Open question：** 能否设计一种routing机制，使得新知识的写入不破坏已有知识的检索路径？这个routing机制本身需要对持续学习robust。

### 1.3 三个问题之间的关系

三个问题**不独立**，存在根本性的张力（见下一节）。它们也与任何memory系统的基本问题直接对应：

| 持续学习问题 | Memory系统等价物 | 计算机内存 | 生物记忆 |
|-------------|----------------|-----------|---------|
| 定位 (Where) | Allocation | malloc/free | 突触可塑性目标选择 |
| 容量 (Room) | Capacity | RAM大小/虚拟内存 | 神经元数量/突触密度 |
| 检索 (Recall) | Addressing | 指针/哈希表 | 模式完成/上下文重激活 |

---

## 2. 核心张力：压缩与可写性的对抗

### 2.1 Superposition是问题的根源

LLM的参数空间之所以强大，根本原因是**superposition**——用 $d$ 个参数编码了远超 $d$ 个特征。多个知识片段共享同一组参数，通过不同的激活pattern区分。

这是一种高效的压缩策略，但它**摧毁了可写性**：

- **高压缩（重度superposition）**：每个参数同时参与大量功能 → 容量大，但动任何一个参数都影响大量功能 → 无法定位、无法安全写入
- **低压缩（轻度superposition）**：每个参数只参与少数功能 → 容量小，但可以精确定位和修改 → 安全写入，但装不下多少东西

### 2.2 信息论层面的根本矛盾

这不是工程tradeoff，而是信息论层面的根本矛盾：

> **表征的压缩效率与持续学习的可行性是对抗的。**

用rate-distortion theory的语言：压缩率越高，表征越分布式，单个参数修改的"blast radius"越大，surgical update越不可能。

这个矛盾意味着，在单一参数空间中，不可能同时最优化"存储效率"和"可更新性"。

### 2.3 与三个本质问题的连接

压缩-可写性矛盾直接影响三个问题：

- **定位**：高压缩 → 知识分布式存储 → 无法定位到可独立修改的参数子集
- **容量**：低压缩 → 容量浪费 → 可存储的总知识量受限
- **检索**：高压缩 → 检索路径与存储路径高度纠缠 → 写入新知识可能破坏检索旧知识的routing

---

## 3. CLS理论的正确类比——海马体不是RAG

### 3.1 初始（错误的）类比

讨论中最初提出：生物大脑用两个不同的系统解决压缩-可写性矛盾——海马体（低压缩、快写入）和新皮层（高压缩、慢整合）。这是Complementary Learning Systems (CLS)理论（McClelland, McNaughton & O'Reilly, 1995; 更新版本见 Kumaran, Hassabis & McClelland, 2016）。

并由此引出推论：由于排除了外部记忆系统（RAG、Agentic Search、File-based Memory），纯参数化方案是否无法模拟CLS？

### 3.2 用户的关键纠正

用户指出这个类比是**根本性错误**的。海马体不应该类比为RAG/外置记忆，理由如下：

**海马体 vs RAG的本质差异：**

| 属性 | 海马体 | RAG/外置记忆 |
|------|--------|-------------|
| 存储介质 | **突触权重（参数化）** | 文本/向量数据库（非参数化） |
| 写入方式 | **突触可塑性（学习过程）** | 复制粘贴（无学习） |
| 存储时效 | **临时**（数天到数周，之后巩固到皮层或被遗忘） | 永久（除非手动删除） |
| 与主系统关系 | 通过睡眠巩固**改写皮层权重** | **永远不改写**模型权重 |
| 编码方式 | 稀疏激活，压缩表征 | 原文存储，无压缩 |

**核心区别：** 海马体是一个**第二参数化系统**——它本身就是一个神经网络，有自己的突触权重，只是架构属性（学习率、编码方式、容量）与新皮层截然不同。RAG是非参数化的外挂存储，永远不会驱动模型权重更新。

### 3.3 睡眠巩固的正确对应

CLS理论中的睡眠巩固过程：

1. 睡眠期间，海马体的记忆（以突触权重编码）被重激活
2. 这种重激活产生replay信号
3. Replay信号驱动新皮层的突触可塑性（梯度更新）
4. 知识从海马体的"快速、低干扰、临时"存储逐步转移到新皮层的"慢速、高容量、持久"存储
5. 海马体的相应记忆逐步衰减，释放容量给新记忆

**整个过程是纯参数化的。** 没有数据库，没有文件系统。海马体的"记忆"就是它的权重状态，巩固就是用这个权重状态生成训练信号来更新另一组权重。

这等价于一种**跨参数区域的知识蒸馏**过程。

### 3.4 正确的LLM对应

CLS在LLM语境下的正确对应不是"模型 + RAG"，而是：

> **两组参数（或同一参数空间的两个区域），具有不同的学习动力学，加上一个巩固过程在两者之间转移知识。**

- **"海马体"参数区**：高学习率、稀疏激活导致pattern separation、低干扰、有限容量、临时存储
- **"新皮层"参数区**：低学习率、分布式编码、高容量、慢整合
- **"睡眠巩固"**：海马体参数区生成replay信号 → 驱动新皮层参数区的梯度更新 → 海马体逐步释放容量

**参考文献：**
- McClelland, J.L., McNaughton, B.L., & O'Reilly, R.C. (1995). Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory. *Psychological Review*, 102(3), 419-457.
- Kumaran, D., Hassabis, D., & McClelland, J.L. (2016). What learning systems do intelligent agents need? Complementary learning systems theory updated. *Trends in Cognitive Sciences*, 20(7), 512-534.
- 生成式重放的早期工作：Shin, H., Lee, J.K., Kim, J., & Kim, J. (2017). Continual learning with deep generative replay. *NeurIPS 2017*.
- 但注意：Shin et al.的generative replay虽然与CLS巩固过程有表面相似，但缺乏海马体的关键架构属性（见下节）。

---

## 4. 海马体本身是稀疏的——activation sparsity的关键性

### 4.1 用户的追问

在建立了"海马体 = 第二参数化系统"的正确类比后，用户提出关键追问：

> "难道海马体本身是稠密的？"

### 4.2 事实：海马体是大脑中最稀疏的区域之一

海马体不是dense的。它在多个层面上表现出极端的稀疏性：

**激活稀疏性（Activation Sparsity）：**
- **Dentate Gyrus (DG)**：在任何给定时刻，只有约**2-5%**的颗粒细胞（granule cells）处于激活状态。DG拥有约100万个颗粒细胞（大鼠数据），但每个记忆只激活其中极少数。
- 这种极端的激活稀疏性实现了**pattern separation**——输入空间中相似的模式被映射到激活空间中几乎不重叠的稀疏表征，从而使不同记忆之间的干扰最小化。

**连接稀疏性（Connectivity Sparsity）：**
- **CA3 recurrent connections**：每个CA3神经元只与约**2-4%**的其他CA3神经元形成突触连接。
- 整个大脑都是极度稀疏的——每个神经元连接约7000个其他神经元，占总神经元数（~860亿）的约**0.00001%**。

**参考文献/事实支撑：**
- Dentate Gyrus的稀疏编码：Leutgeb, J.K., Leutgeb, S., Moser, M.B., & Moser, E.I. (2007). Pattern separation in the dentate gyrus and CA3 of the hippocampus. *Science*, 315(5814), 961-966.
- DG的激活率估计：Chawla, M.K., et al. (2005). Sparse, environmentally selective expression of Arc RNA in the upper blade of the rodent fascia dentata by brief spatial experience. *Hippocampus*, 15(5), 579-586.
- CA3 recurrent connectivity: Amaral, D.G., Ishizuka, N., & Claiborne, B. (1990). Neurons, numbers and the hippocampal network. *Progress in Brain Research*, 83, 1-11.

### 4.3 新皮层也不是dense的

关键事实：新皮层（neocortex）同样不是fully connected的。

- 局部连接（同一column内）相对较密
- 长程连接（跨区域）极其稀疏
- 皮层以column/layer为单位组织，具有结构化稀疏性
- 典型皮层神经元的激活率约为**10-20%**，远高于DG的2-5%，但远低于"dense"

所以**大脑中不存在dense网络**。一切都是稀疏的。

### 4.4 对之前框架的根本性修正

之前的框架将海马体描述为"低压缩、快写入"，暗示它接近dense。这是**错误的**。

修正后的理解：

- 海马体之所以能快速写入且低干扰，**不是因为它"不压缩"或接近dense**
- 而是因为它用**极端的激活稀疏性**实现了**pattern separation**
- 两个不同的记忆激活几乎不重叠的神经元子集 → 写入一个不会覆写另一个
- 稀疏**不是**压缩的对立面，而是**一种特定的压缩策略**，牺牲单位神经元利用率，换取记忆之间的可分离性

### 4.5 海马体 vs 新皮层的真正区别

两个系统的真正区别不是sparse vs dense，而是**稀疏的程度和表征策略**：

| 属性 | 海马体 | 新皮层 |
|------|--------|--------|
| 连接拓扑 | 稀疏 | 也稀疏（但模式不同） |
| 激活稀疏度 | **极高**（~2-5%） | 较低（~10-20%） |
| 学习速率 | 快（接近one-shot，快速Hebbian可塑性） | 慢（需要反复暴露） |
| 表征策略 | **Pattern separation**（正交化：相似输入→不重叠表征） | **Distributed/overlapping**（相似输入→重叠表征） |
| 干扰特性 | 记忆间低干扰 | 新学习容易干扰旧记忆 |
| 容量 | 有限（受极端稀疏的制约） | 高（分布式编码允许更多信息共享同一参数） |
| 存储时效 | 临时（数天到数周） | 长期（数月到数年） |

**关键洞察：** 海马体和新皮层之间的区别对应的是**激活稀疏度的连续谱**和**pattern separation vs pattern overlap**的表征策略选择，而不是weight topology的sparse vs dense二元划分。

---

## 5. 研究方向的根本性修正

### 5.1 从weight sparsity到representation separation

基于以上讨论，对研究方向的核心修正：

**旧框架（已放弃）：**
> "结构性稀疏拓扑（weight sparsity）是持续学习的必要条件。"

**问题：**
1. 在固定架构下trivially true（因为non-sparse = dense，dense必然遗忘）
2. Weight sparsity可能只是实现activation sparsity的一种手段，不是本质

**新的核心问题（从本轮讨论中浮现）：**
> "对于持续学习，决定性的因素可能不是权重拓扑的稀疏性，而是**表征层面的pattern separation程度**——不同任务/记忆是否激活不重叠的神经元子集。"

即：

- **Weight sparsity**（物理移除连接）可能只是实现**activation sparsity**（不同记忆激活不重叠子集）的一种手段
- 真正的本质是**表征的正交性/可分离性**
- 如果能通过其他方式（训练目标、正则化、架构设计）实现高度的activation sparsity和pattern separation，可能不需要physical weight sparsity

### 5.2 三个值得深挖的研究方向

不预设解决方案（不预设sparse、不预设任何特定方法），从三个open question出发：

**方向1：压缩-可写性的Pareto frontier**
- 是否存在打破compression-writability tradeoff的架构或训练方法？
- 还是这是一个fundamental limit（类似信息论中的rate-distortion bound）？
- 如果是fundamental limit，其形式是什么？

**方向2：Free capacity的度量与管理**
- 能否建立一套"参数空间的内存管理理论"？
- 类似OS的malloc/free，但在连续参数空间中操作
- 能否度量一个训练后模型的"剩余可写容量"？

**方向3：Self-routing的稳定性**
- 在什么条件下，网络的attention/routing机制能在持续学习中保持stable？
- 能否设计routing机制使其对新知识写入robust？
- 这连接到CLS中海马体的pattern separation功能：routing本身需要保证不同记忆被路由到不同的表征子空间

### 5.3 CLS的参数化实现作为统一框架

从本轮讨论中浮现的一个可能的统一研究框架：

> **在单一模型内部实现功能分化——两个具有不同学习动力学的参数区域，通过内生的巩固机制实现知识转移。**

这不是"模型 + 外部记忆"，而是**模型架构内部的功能分化**：

1. 一个"快速学习区"（高activation sparsity → pattern separation → 低干扰 → 有限容量）
2. 一个"慢速整合区"（低activation sparsity → distributed coding → 高容量 → 需要巩固）
3. 一个"巩固机制"（快速区生成replay → 驱动慢速区更新 → 快速区释放容量）

这个框架的关键优势是它完全参数化，不依赖外部存储，且有明确的生物学对应。

---

## 6. 尚未解决的关键问题

本轮讨论到此暂停，以下问题标记为后续讨论的起点：

1. **Activation sparsity vs weight sparsity的因果关系**：weight sparsity是否是实现high activation sparsity的充分/必要/最优手段？还是存在不需要weight sparsity就能实现pattern separation的方法？

2. **CLS的参数化实现的具体架构**：如何在transformer架构中分化出"海马体区"和"新皮层区"？是通过不同的层、不同的attention head、还是同一层内的不同参数子集？

3. **巩固机制的具体实现**：海马体的replay在LLM中对应什么操作？是从"快速区"参数生成synthetic data来fine-tune"慢速区"？还是有更直接的权重转移方式？

4. **Pattern separation的度量**：如何量化一个网络的pattern separation程度？能否建立activation sparsity → forgetting bound的理论联系？

---

## 7. 讨论演进的逻辑链（时序记录）

为保持思维过程的完整性，记录讨论的演进路径：

### Step 1: 提出三个本质问题
- 以"如何将本次对话写入模型权重"为concrete example
- 自然分解出定位/容量/检索三个子问题
- 识别出这三个问题与任何memory系统（计算机内存、生物记忆）的基本问题同构

### Step 2: 发现压缩-可写性张力
- Superposition是LLM强大的根源，但也是持续学习的障碍
- 高压缩（重度superposition）→ 高容量但不可写
- 低压缩（轻度superposition）→ 可写但低容量
- 这是信息论层面的根本矛盾

### Step 3: 引入CLS理论（初始错误类比）
- 生物大脑用双系统解决这个矛盾
- 最初错误地将海马体类比为RAG/外置记忆
- 由此认为"排除外部记忆 = 排除CLS"

### Step 4: 用户纠正——海马体是参数化系统
- 海马体存储用突触权重（参数化），不是数据库
- 睡眠巩固是从海马体参数→新皮层参数的知识蒸馏
- RAG永远不更新模型权重，与CLS的巩固过程本质不同
- 正确类比：CLS = 两组不同属性的参数 + 跨区域巩固机制

### Step 5: 用户追问——海马体是dense的吗？
- 答案：不是。海马体是大脑中最稀疏的区域之一
- DG：~2-5%激活率（极端activation sparsity）
- CA3：~2-4%连接率（connectivity sparsity）
- 新皮层也不是dense的，只是activation sparsity程度较低（~10-20%）

### Step 6: 框架修正
- 海马体 vs 新皮层的区别不是sparse vs dense
- 而是activation sparsity的**程度**和表征策略（pattern separation vs overlap）
- Weight sparsity可能只是实现activation sparsity的手段，不是本质
- 研究问题从"是否需要weight sparsity"转向"什么样的表征分离策略最有效"

---

## 8. 相关参考文献汇总

### 持续学习基础
- Kirkpatrick, J., et al. (2017). Overcoming catastrophic forgetting in neural networks. *PNAS*, 114(13), 3521-3526. [EWC]
- Farajtabar, M., Azizan, N., Mott, A., & Li, A. (2020). Orthogonal gradient descent for continual learning. *AISTATS 2020*. [OGD]
- Saha, G., Garg, I., & Roy, K. (2021). Gradient projection memory for continual learning. *ICLR 2021*. [GPM]
- Knoblauch, J., Husain, H., & Diethe, T. (2020). Optimal continual learning has perfect memory and is NP-hard. *ICML 2020*.
- Shin, H., Lee, J.K., Kim, J., & Kim, J. (2017). Continual learning with deep generative replay. *NeurIPS 2017*.

### CLS理论
- McClelland, J.L., McNaughton, B.L., & O'Reilly, R.C. (1995). Why there are complementary learning systems in the hippocampus and neocortex. *Psychological Review*, 102(3), 419-457.
- Kumaran, D., Hassabis, D., & McClelland, J.L. (2016). What learning systems do intelligent agents need? Complementary learning systems theory updated. *Trends in Cognitive Sciences*, 20(7), 512-534.

### 海马体神经科学
- Leutgeb, J.K., Leutgeb, S., Moser, M.B., & Moser, E.I. (2007). Pattern separation in the dentate gyrus and CA3 of the hippocampus. *Science*, 315(5814), 961-966.
- Chawla, M.K., et al. (2005). Sparse, environmentally selective expression of Arc RNA in the upper blade of the rodent fascia dentata. *Hippocampus*, 15(5), 579-586.
- Amaral, D.G., Ishizuka, N., & Claiborne, B. (1990). Neurons, numbers and the hippocampal network. *Progress in Brain Research*, 83, 1-11.
- Rolls, E.T. (2013). The mechanisms for pattern completion and pattern separation in the hippocampus. *Frontiers in Systems Neuroscience*, 7, 74.

### 稀疏性与网络容量
- Frankle, J., & Carlin, M. (2019). The lottery ticket hypothesis: Finding sparse, trainable neural networks. *ICLR 2019*.
- Elhage, N., et al. (2022). Toy models of superposition. *Anthropic research*.

### 进化架构
- Rusu, A.A., et al. (2016). Progressive neural networks. *arXiv:1606.04671*.

---

## 9. 文档元信息

- **创建时间**：2026-02-24
- **讨论参与**：Alex (Xun Sun) + Claude
- **前序文档**：`docs/zh_CN/processing/20260224-0007.md`
- **状态**：讨论暂停于"activation sparsity vs weight sparsity"的因果关系问题，待后续继续
- **下一步**：沿第5节中标记的四个未解决问题继续讨论
