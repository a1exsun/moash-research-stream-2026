# 从NSA到持续学习：原生可训练稀疏注意力的研究脉络与未填补的空白

> **讨论日期**: 2025年2月25日  
> **背景**: 基于DeepSeek-AI论文 *"Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention"* (arXiv:2502.11089, ACL 2025 Best Paper) 的深度讨论，延伸至后续研究进展及与持续学习的交叉空白。

---

## 1. NSA论文核心内容

### 1.1 解决的问题

长上下文建模中，标准softmax attention的计算开销随序列长度二次增长。在64k长度时，attention计算占总推理延迟的70–80%。现有稀疏注意力方法存在两大缺陷：

- **推理加速的"幻觉"**：理论计算减少无法转化为实际延迟降低，原因包括阶段受限的稀疏性（如H2O仅加速解码、MInference仅加速prefill）以及与GQA/MQA等先进架构不兼容（如Quest的逐head独立选择导致KV-cache内存访问量仍然很高）。
- **可训练稀疏的"神话"**：大多数方法仅在推理阶段施加稀疏，而非训练阶段。后训练剪枝迫使模型偏离预训练优化轨迹——论文指出pretrained模型中top 20% attention仅覆盖70%的总attention分数。此外，ClusterKV的k-means聚类、MagicPIG的SimHash等操作不可微，阻断梯度流；HashAttention等token粒度选择方法导致非连续内存访问，无法利用FlashAttention的高效实现。

### 1.2 NSA的三分支架构

NSA将每个query的注意力计算分解为三条并行路径，通过可学习的门控机制（MLP + sigmoid）加权融合：

**（1）压缩注意力（Compression）**

- 将连续token块通过可学习的MLP（带块内位置编码）聚合为单个压缩key/value
- 参数：块长度 $l=32$，滑动步幅 $d=16$（$d < l$ 以减少信息碎片化）
- 捕获粗粒度全局语义信息，降低计算负担

**（2）选择注意力（Selection）**

- **块级选择**：将KV序列分为空间连续的块（块大小 $l'=64$），而非逐token选择——这既匹配GPU的连续内存访问模式以最大化Tensor Core利用率，也符合attention分数天然呈现空间连续性的经验观察
- **重要性评分零开销**：直接复用压缩注意力的中间softmax分数作为块重要性评分，无需额外计算
- **GQA兼容**：同一GQA组内的所有query head聚合重要性评分后统一选择block，确保共享KV-cache时内存访问一致
- **Top-n选择**：保留重要性排名前 $n=16$ 的块（含1个初始块和2个局部块），选中块中的原始fine-grained token参与attention计算
- 保留精确的细粒度信息

**（3）滑动窗口注意力（Sliding Window）**

- 维护最近 $w=512$ 个token的局部上下文
- 独立分支处理，防止局部模式"shortcut"压缩和选择分支的学习
- 三个分支使用独立的key/value以防止梯度干扰

### 1.3 硬件对齐的Kernel设计

核心优化在于query分组策略的改变：

- **不**按FlashAttention的方式加载时间连续的query块（因为同一块内的query可能需要不相交的KV块）
- **改为**对每个序列位置，加载同一GQA组内所有query head到SRAM，共享稀疏KV块索引
- 外层循环放在Triton的grid调度器上（因为不同query位置选择的block数量近似相同），内层循环顺序加载连续KV块
- 实现近最优算术强度：消除冗余KV传输 + 平衡GPU流多处理器的计算负载

### 1.4 实验结果（27B参数模型，270B tokens预训练）

| 维度 | 结果 |
|------|------|
| **通用benchmark** | 9项任务中7项超过Full Attention，平均分 0.456 vs 0.443；推理任务增益显著（DROP: +0.042, GSM8K: +0.034） |
| **长上下文** | 64k needle-in-a-haystack完美准确率；LongBench平均分 0.469（Full Attention 0.437, Exact-Top 0.423） |
| **CoT推理（AIME 24）** | 8k上下文：NSA-R 0.121 vs Full Attention-R 0.046；16k上下文：0.146 vs 0.092 |
| **训练加速** | 64k序列：前向 9.0×，反向 6.0×（Triton实现对比Triton FlashAttention-2） |
| **解码加速** | 64k序列：11.6×（内存访问量从65536 token降至5632 token） |

### 1.5 关键洞察

- **Attention分数天然呈块级聚类分布**：可视化Full Attention模型的attention map发现，相邻key倾向于具有相似的attention分数，为块级稀疏选择提供了经验依据。
- **预训练阶段引入稀疏强制模型学习更优的attention分配**：稀疏约束迫使模型将重要信息集中到少数关键位置，类似信息瓶颈效应，可能通过过滤无关attention通道的噪声来增强性能。
- **替代方案的失败**：Key-Clustering方法（动态聚类开销大、MoE系统中负载不均）、辅助loss驱动的块选择（额外开销且性能下降）、启发式无参数选择（召回率低）均表现不如NSA。

---

## 2. NSA稀疏性的本质辨析

### 2.1 不是权重稀疏

NSA的稀疏**不发生在权重层面**——Q/K/V投影矩阵、FFN层、MoE专家的参数矩阵仍然是完全稠密的。

### 2.2 是注意力计算的动态结构化稀疏

稀疏发生在attention score的计算过程中：每个query不再与所有preceding key计算attention，而是只与一个精心选择的子集交互：

- 压缩后的粗粒度token（约 $\lfloor(t-l)/d\rfloor$ 个）
- 选出的top-n个细粒度token块（$n \times l'$ 个token）
- 滑动窗口内的局部token（$w$ 个）

以论文配置为例，对于64k长度的序列，每个query实际参与计算的token数约为几千个（远小于65536），实现了 $N_t \ll t$ 的高稀疏比。

关键特征：**动态的、query-dependent的、块级的结构化稀疏**——每个query根据自身内容（通过压缩attention的中间分数）决定关注哪些KV块，模式不固定、不预定义。

---

## 3. 人脑类比

NSA的三条路径可以类比人脑处理信息的不同层次：

| NSA路径 | 人脑类比 | 说明 |
|---------|---------|------|
| **压缩注意力** | 要点记忆/语义记忆 | 读完一本书后形成章节级别的"大意"，不记住每个字但保留概念性知识；需要回忆时先通过粗粒度摘要定位相关段落 |
| **选择注意力** | 选择性注意力/情景检索 | 根据当前思考的问题（query），从记忆中精确调取最相关的细节片段；有人问"去年旅行中最好吃的一顿饭"时直接跳到那个关键场景，而非逐天回忆 |
| **滑动窗口** | 工作记忆/短期记忆 | 对话中总是清晰记得最近几句话的内容，不需要检索，直接可用 |
| **门控机制** | 元认知调控 | 根据任务性质动态调整依赖哪种信息源——做数学推导时更依赖工作记忆，写综述时更依赖全局概括，回答具体问题时更依赖精确检索 |

最有趣的共性是**块级聚类**：论文观察到attention score天然呈现空间连续的块状分布——相邻token倾向于被一起关注。这类似人类的**情景记忆**——我们不会孤立地记住单个词或瞬间，而是以连贯的"场景片段"为单位存储和检索记忆。

---

## 4. NSA与持续学习的关系初探

### 4.1 NSA不直接解决持续学习

灾难性遗忘和可塑性丧失的根源在**参数更新层面**：新任务的梯度覆盖旧任务学到的权重表征。NSA改变的是"计算哪些token的attention"，而不是"如何更新参数"。即使attention是稀疏的，FFN/MoE的权重在微调时仍然被全局更新，遗忘照样发生。

### 4.2 间接有利条件

- **功能分化潜力**：三条路径天然形成功能分工（全局模式 / 精确检索 / 局部依赖），不同类型的知识可能被编码到不同路径中，新任务若主要影响某一路径，对其他路径干扰更小——类似持续学习中**模块化网络**的思路。
- **稀疏激活减少干扰**：每个query只激活少量KV块，不同输入pattern激活的计算路径重叠度更低——概念上类似MoE架构在持续学习中的优势（不同任务路由到不同专家，减少参数竞争）。
- **更长的有效上下文**：NSA让模型能高效处理超长序列，使得"用context window代替参数记忆"的工程方案更可行——虽然不是真正的持续学习，但在实用中缓解同样的问题。

### 4.3 真正的挑战

持续学习的核心矛盾在权重层面。即使attention是稀疏的，FFN和MoE的参数仍然在新数据训练时被全局更新。**将稀疏性引入参数本身的激活和更新**——而非仅引入注意力计算——可能是更直接的路径。

---

## 5. NSA是用于训练而非仅用于推理

这是NSA论文标题中"Natively Trainable"的核心含义，也是区别于几乎所有先前稀疏注意力方法的关键创新。

### 5.1 现有方法的问题

大多数稀疏注意力方法的工作流程是：先用Full Attention正常预训练 → 推理时才施加稀疏剪枝。这导致：

- 模型的attention模式是为稠密计算优化的，强行剪枝必然丢失信息
- pretrained模型中top 20% attention仅覆盖70%总分数
- 推理时的稀疏模式与训练时的优化目标存在架构偏差（architectural bias）

### 5.2 NSA的做法

从预训练第一步就使用稀疏注意力，让模型从一开始就学会在稀疏约束下分配attention。这样：

- 模型主动把重要信息集中到少数关键位置
- attention pattern与整个模型的其他组件（FFN、MoE等）协同适应
- 覆盖完整的模型生命周期：预训练 → 微调 → prefill → 解码，全部稀疏

论文专门为反向传播实现了高效kernel（64k序列6x反向加速），使端到端训练成为可能。

这也解释了为什么NSA在AIME数学推理任务上大幅超过Full Attention：不是因为推理时更快，而是因为模型在训练时就学会了更高效的attention分配模式，反而比稠密注意力学得更好。

---

## 6. NSA之后的研究进展（2025年2月至今）

### 6.1 DeepSeek自身演进：DSA（DeepSeek Sparse Attention）

- **论文**: DeepSeek-V3.2 (arXiv:2512.02556, 2025年12月)
- **核心**: DSA由lightning indexer + 细粒度token选择机制组成，实现了token级别（而非NSA的block级别）的细粒度稀疏选择
- **与MLA结合**: 在MLA（Multi-head Latent Attention）的MQA模式下实例化DSA，每个latent vector（MLA的key-value entry）被所有query head共享
- **训练流程**: 先用短warm-up阶段初始化lightning indexer（保持dense attention，冻结主模型参数），再用sparse训练阶段同时优化主模型和indexer
- **部署**: 已用于DeepSeek-V3.2（685B参数），性能与GPT-5相当，在IMO 2025和IOI 2025获金牌
- **意义**: NSA → DSA代表了从研究原型到生产级落地的演进

### 6.2 MoBA（Mixture of Block Attention）— Moonshot AI/Kimi

- **论文**: arXiv:2502.13189, NeurIPS 2025 Spotlight
- **核心思想**: 将MoE原理应用到attention机制——KV序列分块后，学习一个路由器让每个query token动态选择top-k个最相关的KV块
- **设计哲学**: "less structure"原则，不引入预定义偏置（如sink、固定窗口），让模型自主决定关注哪里
- **与NSA的区别**: MoBA更简洁（单一路由机制 vs NSA的三分支），能在full attention和sparse attention之间无缝切换
- **部署**: 已用于Kimi的长上下文服务

### 6.3 FlashMoBA / Optimizing MoBA

- **论文**: arXiv:2511.11571, 2025年11月
- **理论贡献**: 建立MoBA的统计模型，推导出信噪比（SNR）公式，揭示性能关键取决于路由器区分相关/不相关block的能力
- **改进路径**: 更小的block size + 对key施加短卷积以聚类相关信号 → 提升路由精度
- **工程贡献**: FlashMoBA CUDA kernel，使理论最优的小block配置变得实用，比FlashAttention-2最高14.7×加速

### 6.4 ASA（Alternating Sparse Attention）

- **论文**: arXiv:2511.00819, 2025年11月
- **对NSA的系统分析**: 发现滑动窗口分支在常识推理中占主导，压缩/选择分支主要提供全局上下文
- **关键改进**: 在层间**交替**使用局部（滑动窗口）和全局（压缩+选择）注意力，而非NSA每层都用固定三分支 → 更有效的长程依赖传播
- **架构增强**: 用MLA替换GQA（滑动窗口分支用MLA，压缩/选择分支用GLA——Group-head Latent Attention）
- **结果**: KV-cache内存降低50%，性能持平或超过Full Attention和NSA

### 6.5 SSA（Sparse Sparse Attention）

- **论文**: arXiv:2511.20102, 2025年11月
- **发现关键悖论**: NSA和MoBA等原生稀疏注意力模型，训练后的attention分布反而比Full Attention模型**更不稀疏**——这与直觉相反
- **原因分析**: "梯度更新缺陷"——稀疏训练中被排除的KV对既无前向贡献也无反向梯度，永远学不到适当的抑制（suppression），导致模型不知道该忽略什么
- **解决方案**: 训练中以50%概率交替使用full attention和sparse attention流，加上双向对齐（counterpart attention alignment）——确保所有token都有梯度流，同时鼓励sparse输出对齐full attention输出
- **结果**: 在两种推理模式下都达到SOTA，并支持灵活的稀疏度预算调整；意外发现原生稀疏训练还能改善长上下文外推能力

### 6.6 FSA（Flash Sparse Attention）

- **论文**: arXiv:2508.18224, 2025年8月
- **解决的问题**: NSA的kernel循环顺序（外层遍历query token，内层遍历KV块）仅在GQA组内query head数量较多时高效，但主流LLM（如Llama系列）通常每组只有少量head
- **解决方案**: 反转循环顺序——外层遍历KV块、内层遍历query token，避免padding浪费
- **结果**: kernel级延迟降低最高3.5×（平均1.6×），端到端训练加速最高1.25×，prefill加速最高1.36×

### 6.7 VMoBA（Video MoBA）

- **论文**: arXiv:2506.23858, 2025年6月
- **动机**: 直接将MoBA应用到视频扩散模型（VDM）会导致严重质量下降（VBench 68.25 → 56.88），因为MoBA为1D文本序列设计，而视频attention具有1D/2D/3D的时空局部性
- **创新**: 层间循环使用1D-2D-3D block partition、query-global block selection、基于阈值（而非固定top-k）的block selection
- **意义**: 将原生稀疏注意力从语言模型扩展到视觉生成领域

### 6.8 其他相关工作

- **TabNSA**: 将NSA应用于表格数据学习（将feature视为token），实现feature-incremental learning
- **SeerAttention-R**: 通过蒸馏目标在微调阶段引入稀疏性
- **fla-org/native-sparse-attention**: 社区实现的高效Triton NSA kernel，支持完整训练流程
- **lucidrains/native-sparse-attention-pytorch**: 社区PyTorch参考实现

### 6.9 研究趋势总结

| 方向 | 代表工作 | 核心贡献 |
|------|---------|---------|
| 生产落地 | DSA (DeepSeek-V3.2) | token级细粒度稀疏 + MLA集成 |
| 架构简化 | MoBA (Kimi) | MoE原理的统一路由 |
| kernel优化 | FSA, FlashMoBA | 适配更多架构的高效实现 |
| 训练方法论 | SSA | 解决梯度缺陷悖论 |
| 架构改进 | ASA | 层间交替 + MLA集成 |
| 跨模态扩展 | VMoBA | 视频扩散模型 |

---

## 7. 确认的研究空白：原生稀疏注意力 × 持续学习

### 7.1 空白的确认

经过系统检索（截至2025年2月25日），**没有任何已发表或预印本工作**将NSA/DSA/SSA/ASA/MoBA这一系列原生可训练稀疏注意力与持续学习（continual learning）/持续微调（continual fine-tuning）结合起来。

两个研究社区完全割裂：

- **稀疏注意力社区**：所有工作聚焦"单次训练的效率和性能"——更快的kernel、更好的block选择、更高的稀疏度。没有一篇讨论sequential task adaptation、forgetting或plasticity。
- **持续学习社区**：方法仍在传统路径上——EWC/正则化、replay buffer、task vector pruning、LoRA变体、梯度投影、FIP（Functionally Invariant Paths）等。虽然有工作提到"sparse transformer可能有不同的遗忘机制"，但仅作为future work一笔带过。

Sebastian Raschka在2025年度总结中明确指出："持续学习目前还没有实质性突破。"

### 7.2 最接近的边缘工作

**（1）Google Research: Nested Learning（2026年2月）**

- 将transformer中的不同组件视为不同频率更新的嵌套优化问题
- Attention = 短期记忆，FFN = 长期记忆，不同组件以不同速率更新来缓解遗忘
- 理念上最接近"利用attention的结构特性辅助持续学习"
- **但**没有使用NSA/MoBA类原生稀疏注意力

**（2）Meta: Sparse Memory Finetuning（2025年10月）**

- 提出只更新极少量参数（0.01%级别）来实现持续学习
- 利用memory layer实现稀疏参数更新
- **但**稀疏性在参数级别，而非注意力计算级别

**（3）EMNLP 2025: Task Vector Pruning for Catastrophic Forgetting**

- 对微调产生的task vector进行稀疏剪枝来缓解遗忘
- **但**这是后训练的参数级操作，与训练时的注意力稀疏无关

**（4）Mechanistic Analysis of CF in LLMs（2026年1月）**

- 最新的机制分析工作，系统研究了attention权重中的梯度干扰、中间层的表征漂移、loss landscape的扁平化
- 提到"sparse transformer可能表现出不同的遗忘机制"
- **但**仅作为future direction提及，未做实验

### 7.3 为何这是有价值的研究空白

**（1）SSA的"梯度缺陷"悖论可以被重新解读为持续学习的机制**

SSA发现稀疏训练中被排除的KV对收不到梯度，这对单任务学习是缺陷。但对持续学习来说，"部分参数/路径不被更新"恰恰是防止遗忘的核心机制——EWC、PackNet等经典方法本质上都在做这件事。如果能让这种"不更新"变成task-aware的（旧任务的关键attention pattern被保护，新任务的梯度被引导到未使用的路径），就能桥接两个领域。

**（2）ASA的层间交替策略暗示功能隔离**

不同层负责不同粒度的信息处理（局部 vs 全局）。持续微调时可以选择性地只更新某些层/分支，冻结其他层/分支，同时利用稀疏选择机制让新旧知识的attention pattern自然分离。

**（3）DSA的lightning indexer本质上是"记忆检索"模块**

它决定每个query应该检索哪些历史token。如果将indexer与task-specific routing结合，让不同任务的知识被索引到不同的KV子空间，就可能在不修改主模型参数的情况下实现任务切换——类似于持续学习中的context-dependent processing。

**（4）MoBA的MoE-style路由天然支持任务分流**

MoE架构已被认为对持续学习更友好（不同任务路由到不同专家）。MoBA将同样的原理应用到attention level——如果不同任务自然形成不同的block routing pattern，就可能实现attention层面的任务隔离。

**（5）完整的结合方案可能是：**

- **注意力稀疏**（NSA/DSA级别）：减少前向计算中的路径重叠
- **参数稀疏**（MoE + selective update）：限制哪些权重被新任务更新
- **梯度保护**（EWC/Fisher信息矩阵 + SSA的对齐机制）：保护旧任务的关键参数
- **记忆机制**（DSA indexer + episodic memory）：task-aware的KV检索

---

## 8. 总结

本次讨论从NSA论文出发，完成了以下认知链条：

1. **理解NSA本身**：三分支层级稀疏注意力（压缩 + 选择 + 滑动窗口），硬件对齐的kernel设计，端到端可训练
2. **辨析稀疏的本质**：不是权重稀疏，而是注意力计算的动态结构化稀疏
3. **人脑类比**：压缩≈语义记忆，选择≈选择性注意力，滑动窗口≈工作记忆，门控≈元认知调控
4. **与持续学习的间接关系**：功能分化、稀疏激活减少干扰、超长上下文替代参数记忆
5. **澄清NSA是训练时方法**：从预训练第一步就使用稀疏注意力，覆盖完整模型生命周期
6. **梳理后续研究（6项主要工作）**：DSA（生产落地）、MoBA（架构简化）、FlashMoBA（理论分析+高效kernel）、ASA（层间交替+MLA集成）、SSA（梯度缺陷悖论）、FSA（适配更多架构）
7. **确认研究空白**：截至目前，没有任何工作将原生可训练稀疏注意力与持续学习/持续微调结合——这是一个明确的、有价值的、两个成熟领域之间的未填补空白

---

## 参考文献

### 原生可训练稀疏注意力

- Yuan et al. (2025). *Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention.* ACL 2025 Best Paper. arXiv:2502.11089
- DeepSeek-AI (2025). *DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models.* arXiv:2512.02556
- Lu et al. (2025). *MoBA: Mixture of Block Attention for Long-Context LLMs.* NeurIPS 2025 Spotlight. arXiv:2502.13189
- Xiao et al. (2025). *Optimizing Mixture of Block Attention.* arXiv:2511.11571
- Hu et al. (2025). *Optimizing Native Sparse Attention with Latent Attention and Local Global Alternating Strategies (ASA).* arXiv:2511.00819
- Shen et al. (2025). *SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space.* arXiv:2511.20102
- Yan et al. (2025). *FSA: An Alternative Efficient Implementation of Native Sparse Attention Kernel.* arXiv:2508.18224
- VMoBA (2025). *VMoBA: Mixture-of-Block Attention for Video Diffusion Transformers.* arXiv:2506.23858

### 持续学习

- Van de Ven et al. (2024). *Continual Learning and Catastrophic Forgetting.* arXiv:2403.05175
- Shi et al. (2024). *Continual Learning of Large Language Models: A Comprehensive Survey.* ACM Computing Surveys. arXiv:2404.16789
- Behrouz et al. (2026). *Nested Learning: A New ML Paradigm for Continual Learning.* Google Research Blog.
- Berges et al. (2025). *Continual Learning via Sparse Memory Finetuning.* arXiv:2510.15103
- Imanov (2026). *Mechanistic Analysis of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning.* arXiv:2601.18699
- Raschka (2025). *The State of LLMs 2025.* Sebastian Raschka's Newsletter.
