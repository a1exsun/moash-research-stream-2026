# 开源稀疏架构模型全景（截至 2026 年 2 月）

## 概述

"稀疏"在大语言模型中有两个层次：

1. **MoE 稀疏**（FFN 层）：Mixture-of-Experts，每次推理只激活部分专家网络
2. **注意力稀疏**（Attention 层）：原生稀疏注意力机制，注意力计算本身从 O(L²) 降低到亚二次复杂度

本文聚焦于**架构层面的全流程原生稀疏**——不仅仅是 MoE，而是像 NSA、DSA 这样在注意力计算中引入原生稀疏的模型和技术。

---

## DeepSeek V3.2

### 基本信息

| 属性 | 规格 |
|------|------|
| 总参数 | 685B |
| 激活参数 | ~37B（MoE） |
| 上下文长度 | 128K |
| 许可证 | MIT |
| 发布时间 | 2025 年 12 月 |

### 变体

- **DeepSeek-V3.2**：旗舰版，支持推理 + 工具调用 + 对话
- **DeepSeek-V3.2-Speciale**：专注深度推理的实验版本，不支持工具调用，在 2025 IMO 和 IOI 中获得金牌成绩

DeepSeek V3.2 官方只有 685B 这一个参数规模。社区中提到的 7B/14B/32B 版本实际上是 DeepSeek R1 的蒸馏模型（基于 Qwen2.5 和 Llama3 蒸馏），并非 V3.2 的官方版本。

### 硬件需求

| 精度 | 显存需求 | 硬件方案 |
|------|---------|---------|
| FP16/BF16 | ~1.3TB+ | 8+ 张 H200 (141GB) + NVLink |
| FP8 | ~690GB | 5-8 张 H100/H200 |
| INT4 | ~350-400GB | 4-5 张 H100 (80GB) |

完整部署约 8 张 H200 GPU，硬件成本约 $200K。对于月处理量低于 5000 万 token 的场景，API 比自建硬件更划算。由于 MoE 架构每次只激活 37B 参数，社区存在利用 CPU offloading 在消费级硬件上运行的方案，但推理速度不具备实用性。

### 核心稀疏技术：DSA（DeepSeek Sparse Attention）

DSA 是 DeepSeek V3.2 相对 V3.1 的唯一架构修改。其核心机制：

- **Lightning Indexer**：超轻量 FP8 评分器，为每个 query token 识别最相关的 KV 条目
- **Top-k Token Selection**：只对评分最高的 KV 条目做注意力计算
- 在 MLA（Multi-head Latent Attention）上实例化，复杂度从 O(L²) 降至 O(Lk)

DSA 直接基于 NSA（Native Sparse Attention）演化，NSA 获得了 ACL 2025 Best Paper。

---

## 原生稀疏注意力的开源模型

### DeepSeek V3.2 — DSA

如上所述。DSA 将注意力复杂度从 O(L²) 降到 O(Lk)，在 128K 长上下文推理中实现约 50% 计算量缩减。

### Qwen3-Next / Qwen3.5 — Gated DeltaNet 混合注意力

| 属性 | Qwen3-Next | Qwen3.5 |
|------|-----------|---------|
| 总参数 | 80B-A3B | 397B-A17B |
| 注意力架构 | Gated DeltaNet + Gated Attention（3:1） | 同上 |
| 上下文 | 262K（原生） | 1M（API） |
| 许可证 | Apache 2.0 | Apache 2.0 |

3:1 的混合布局意味着大部分层使用 Gated DeltaNet（线性注意力，O(L) 复杂度），剩余层使用 Gated Attention（全注意力）。这不是推理时的近似优化，而是**训练时就原生使用**的亚二次注意力。DeltaNet 层不增长 KV cache，保持固定大小的循环状态，内存随上下文长度保持恒定。

### Kimi K2.5 / Kimi Linear — KDA + MLA

| 属性 | 规格 |
|------|------|
| 总参数 | 1T+ |
| 激活参数 | ~32B |
| 注意力架构 | KDA（channel-wise gating）+ MLA（3:1） |
| 许可证 | MIT |

与 Qwen3.5 同样采用 3:1 混合比例，但实现有差异：Kimi 使用 channel-wise gating（KDA）替代 Qwen 的 scalar gating，并将 Full Attention 层换成 DeepSeek 的 Multi-Head Latent Attention（MLA）。

### MiniMax-M1 / M2.5 — Lightning Attention

| 属性 | M1 | M2.5 |
|------|-----|------|
| 总参数 | 456B | ~230B |
| 激活参数 | 45.9B | ~10B |
| 注意力架构 | Lightning Attention（7:1 混合） | Lightning Attention |
| 上下文 | 1M | 205K |

MiniMax-M1 使用混合堆叠：每 7 个 Lightning Attention 块穿插 1 个标准 Softmax Attention 块。Lightning Attention 以线性复杂度运行，生成 100K token 时仅需 DeepSeek-R1 约 25-30% 的计算量。

值得注意的发展：MiniMax 在 M2 版本中**放弃了线性注意力**，回归全注意力，团队表示线性注意力在推理和多轮任务中准确度有问题。但 M2.5 又重新引入了 Lightning Attention。

### GLM-5 — DSA + MLA

| 属性 | 规格 |
|------|------|
| 注意力架构 | 传统 Transformer + DeepSeek Sparse Attention (DSA) + MLA |
| 发布 | 2026 年 2 月 |

GLM-5 保留传统 Transformer 结构，但加入了 DeepSeek Sparse Attention（DSA）实现 token 级稀疏，同时也使用了 MLA。这是 DeepSeek 技术影响力扩散的典型案例。

---

## 研究层面的稀疏注意力方法

这些是开源的学术方法/代码，但未绑定到具体的大规模生产模型。

### NSA（Native Sparse Attention）

- **来源**：DeepSeek + 北京大学，ACL 2025 Best Paper
- **机制**：动态分层稀疏策略，三条并行注意力路径——粗粒度 token 压缩、细粒度 token 选择、滑动窗口本地上下文
- **特点**：硬件对齐优化，支持端到端训练（不是推理时的近似）
- **意义**：DSA 的学术基础，在 64K 长序列上实现全阶段（解码、前向、反向）的显著加速

### MoBA（Mixture of Block Attention）

- 块级稀疏注意力，训练时原生使用
- 与 NSA 同属 Sparse-Sparse 范式

### SSA（Sparse Sparse Attention）

- 通过对齐全注意力和稀疏注意力的输出来训练原生稀疏模型
- 发现一个关键悖论：专门为稀疏设计的 Sparse-Sparse 模型反而比全注意力模型的注意力稀疏度更低
- 提出双流训练框架，交替执行稀疏注意力流和全注意力流

### SpargeAttn / SageAttention（推理优化，非原生训练）

- 清华大学，ICLR 2025 / ICML 2025
- 推理时免训练的稀疏 + 量化注意力加速
- 可即插即用到任何模型，2.5x-5x 加速
- 注意：这属于**推理时优化**，而非训练时的原生稀疏架构

---

## MoE 稀疏的开源模型（FFN 层稀疏）

虽然本文聚焦注意力稀疏，MoE 作为另一维度的稀疏也值得列出作参考。

### 大规模旗舰级

| 模型 | 总参数 | 激活参数 | 许可证 |
|------|--------|---------|--------|
| DeepSeek V3/V3.1/V3.2 | 685B | 37B | MIT |
| Mistral Large 3 | 675B | 41B | Apache 2.0 |
| Qwen3-235B-A22B | 235B | 22B | Apache 2.0 |
| Kimi K2 | 1T+ | 32B | MIT |
| DBRX | 132B | 36B | Databricks Open |
| Llama 4 Maverick | 400B+ | MoE | Llama 许可证 |

### 中小规模（消费级硬件可运行）

| 模型 | 总参数 | 激活参数 | 说明 |
|------|--------|---------|------|
| Qwen3-30B-A3B | 30B | 3B | 表现超越 QwQ-32B |
| Qwen3-Coder-Next | 80B | 3B | 编程专用 |
| Mixtral 8x7B / 8x22B | 47B/141B | 13B/39B | Mistral 经典 MoE |
| GLM-4.7-Flash | 30B | 3B | 轻量部署 |
| NVIDIA Nemotron | 30B | 3.5B | 通用 |
| OLMoE | - | - | AI2 研究型 |

---

## 当前格局与趋势

### 注意力机制：新的战场

一年前的核心问题是「MoE 还是 Dense？」——现在这个问题已经解决（MoE 胜出）。当前的分化集中在注意力机制的选择上：

| 路线 | 代表模型 | 方式 |
|------|---------|------|
| 混合线性-全注意力 | Qwen3.5, Kimi K2.5 | Gated DeltaNet/KDA + 全注意力（3:1） |
| 全线性注意力 | MiniMax M1/M2.5 | Lightning Attention |
| 稀疏选择（亚二次） | DeepSeek V3.2, GLM-5 | DSA/NSA |
| 传统全注意力 | MiniMax M2 | 放弃线性注意力回归全注意力 |

### DeepSeek 的技术影响力

DeepSeek 的技术组件正在被广泛采用：

- **MLA**（Multi-head Latent Attention）→ 被 Kimi K2.5 和 GLM-5 采用
- **DSA**（DeepSeek Sparse Attention）→ 被 GLM-5 采用
- **NSA**（学术基础）→ ACL 2025 Best Paper，影响了整个稀疏注意力研究方向

### 线性注意力的争议

MiniMax 的经历揭示了线性注意力在生产环境中的挑战：M1 使用 Lightning Attention，M2 放弃并回归全注意力（理由是推理和多轮任务准确度下降），M2.5 又重新引入。这说明线性/稀疏注意力在理论效率和实际效果之间仍存在张力。

### 激活比例的竞赛

模型正在变得越来越"稀疏"：

- Qwen3: 9.36% 激活（22B/235B）
- Qwen3.5: ~4.3% 激活（17B/397B）
- MiniMax M2.5: ~4.3% 激活（10B/230B）
- Kimi K2.5: 更低激活比例（32B/1T+）

更低的激活比例意味着更好的推理效率，但也对路由策略和训练稳定性提出更高要求。

---

*整理日期：2026 年 2 月 25 日*
