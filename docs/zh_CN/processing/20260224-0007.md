# 持续学习路线推进记录（2026-02-24）

## 文档目的

记录本轮新增进展，重点覆盖：

1. 对 `proposal/research_proposal_draft.md` 的价值评估与结构修正
2. “dense上持续复杂化是否dead end”的路线判断
3. 人类级长期持续学习目标下的充分条件讨论
4. 最新开源稀疏模型（含Kimi/MiniMax）澄清
5. 固定拓扑物理稀疏模型与dense->sparse算法盘点

本记录用于支撑下一步实验决策与proposal迭代。

---

## 1. Proposal 价值判断（阶段性）

目标文件：
- `/Users/alex/Alex/monash/continual-learning/proposal/research_proposal_draft.md`

阶段性结论：

1. 提案有明确价值，研究问题有发表潜力。
2. 原版本存在“主张强于证据”的问题，主要体现在：
- 绝对化断言（必要条件、从未有人做过、首次）
- 理想化公式外推到真实Transformer
- 实验对照中潜在容量混淆
- 与真实工程持续学习场景的外推性不足
3. 如果收敛叙事强度、增强控制变量设计，提案说服力会显著提高。

---

## 2. 已执行的proposal修订（本轮已落地）

对应提交：
- commit: `a90e814`
- message: `refine proposal claims and strengthen experimental controls`

### 2.1 主张强度修正

1. 将“必要条件假说”调整为“强约束/归纳偏置假说”。
2. 将“根本原因唯一归因”改为“重要来源之一”。
3. 将“首次/从未做过”改为“据当前检索范围/工作性判断”。

### 2.2 理论表述修正

1. 保留 `rho^(L-1)` 作为理想化近似，只在限定前提下成立。
2. 明确指出真实Transformer中残差与注意力全局混合会偏离该近似。
3. 引入“需实证估计有效干扰半径”的表述，避免理论过拟合。

### 2.3 实验设计修正

在核心实验中新增公平性与统计协议：

1. 等形状对照（same-shape）
2. 等非零参数对照（matched-nonzero）
3. 等FLOPs对照（matched-FLOPs）
4. 多随机种子 + CI 统计报告
5. 预注册主指标（遗忘率、前向迁移）

### 2.4 外推性增强

新增“实验1B（最小机制基线）”：

1. Dense + LoRA
2. Dense + Replay（固定小缓存）
3. Sparse-topology + LoRA

目的：检查拓扑效应是否能在常见工程机制中持续存在并可叠加。

---

## 3. 路线判断：dense持续复杂化是否dead end

讨论问题：
- 在dense模型上不断加机制，是否终局无解？

阶段性判断：

1. 若复杂化不改变“全局共享参数写入物理”，长期大概率收益递减并走向工程死胡同。
2. 若复杂化直接改变写入机制（局部写入、模块隔离、动态扩容、离线巩固），则属于有效过渡路径。
3. 对“人类级长期持续学习”目标，纯dense+补丁体系大概率不足。

关键区分：

- 死路复杂化：正则/回放/调度叠加但仍全局耦合写入
- 非死路复杂化：写入局部化与结构分离，逐步逼近新范式

---

## 4. 人类级目标下的充分性讨论

用户目标明确：
- 人类级、长期、持续学习

本轮结论：

1. `稀疏/模块化拓扑 + 选择性写入 + 离线巩固` 是必要条件集合之一，但大概率仍不充分。
2. 仍可能需要补充：
- 在线局部学习规则（降低对频繁全局反传依赖）
- 长期容量管理（增长-整合-压缩循环）
3. 不一定必须“推翻Transformer算子”，但大概率需要推翻“纯Transformer+全局SGD写入范式”。

表达收敛为：
- 不预设必须抛弃Transformer
- 预设必须抛弃纯全局密集写入范式

---

## 5. 开源模型稀疏性澄清（新增）

### 5.1 关于“新模型是否稀疏”

问题聚焦：
- Kimi 2.5 / MiniMax 新模型是否具备稀疏网络特性

阶段性回答：

1. Kimi K2.5：具备MoE稀疏激活特性。
2. MiniMax M2.5：配置体现MoE专家路由（本地专家数与每token激活专家数）。

关键澄清：
- 这类属于“计算稀疏（activation sparse）”，不等于“固定拓扑物理稀疏（weight physically absent）”。

### 5.2 固定拓扑物理稀疏模型（代表性）

本轮盘点到的公开代表包括（以RedHatAI/NeuralMagic系为主）：

1. Sparse-Llama-3.1-8B-2of4
2. SparseLlama-3-8B-pruned_50.2of4
3. Llama-2-7b-pruned50-retrained / pruned70-retrained
4. OpenHermes-2.5-Mistral-7B-pruned2.4
5. Nous-Hermes-2-Yi-34B-pruned2.4
6. phi-2-pruned50
7. mpt-7b-gsm8k-pruned60-pt

方法生态补充：
- MaskLLM 提供2:4稀疏mask/导出流程与训练支持

---

## 6. dense -> sparse 算法盘点（新增）

本轮给出的算法框架：

### 6.1 一次性/后训练剪枝

1. Magnitude pruning / GMP
2. OBC（OBS家族二阶近似）
3. SparseGPT
4. Wanda / Wanda++

### 6.2 剪枝后再恢复

1. Movement Pruning
2. L0 Regularization
3. LLM-Pruner
4. LoRAPrune

### 6.3 硬件友好结构稀疏

1. N:M / 2:4 稀疏（如MaskLLM相关路线）
2. 实践上更容易得到真实吞吐收益

### 6.4 训练前/中子网发现

1. SNIP
2. Lottery Ticket / IMP

工程结论：

1. 非结构化稀疏不一定带来推理加速（依赖kernel与runtime）。
2. 2:4 / N:M 更适合工程落地与部署验证。
3. MoE激活稀疏与固定权重拓扑稀疏需严格区分。

---

## 7. 当前共识（可直接复用）

1. 近期研究主线不应是“继续堆dense补丁”，而是“改变写入物理与调度机制”。
2. 若目标是人类级长期持续学习，建议将“固定拓扑稀疏”纳入核心实证对象，而非仅作为工程加速技巧。
3. 提案现在已从“强叙事”向“强实验设计”收敛，具备执行基础。

---

## 8. 下一步落地建议

1. 用已修订proposal启动首轮最小实验：dense vs sparse 的等预算对照。
2. 并行跑实验1B最小机制基线，检验外推性。
3. 优先选择1-2个可复现的固定拓扑稀疏开源权重作为启动点（2:4优先）。

---

## 9. 相关文件与提交

1. 前序记录：
- `/Users/alex/Alex/monash/continual-learning/processing/20260222-0353.md`
- `/Users/alex/Alex/monash/continual-learning/processing/20260223-2152.md`
- `/Users/alex/Alex/monash/continual-learning/processing/20260223-2331.md`

2. 本轮修订提案：
- `/Users/alex/Alex/monash/continual-learning/proposal/research_proposal_draft.md`
- commit: `a90e814`

3. 本文档：
- `/Users/alex/Alex/monash/continual-learning/processing/20260224-0007.md`

