# Proposal 03: 探索 SOTA 模型隐式稀疏性与面向持续学习的微调架构改造

## 研究动机

一个我觉得值得探讨的问题：**模型规模（Scale）本身是否已经在一定程度上缓解了持续学习中的干扰？**

随着模型参数量和特征维度的增大，SOTA 级大语言模型内部可能会自然涌现出"隐式的稀疏性（Implicit Sparsity）"——在高维语义空间中，处理不同领域知识时的激活通路可能天然具备较低的重叠率，从而降低相互覆写的风险。

如果这一猜想成立，那么面对已有的大模型，一个可尝试的研究路径是：**在微调阶段引入稀疏感知的架构改造，既探索隐式稀疏性的实际程度，又尝试利用这一特性来缓解持续微调中的遗忘。**

## 假设

### H1 (规模涌现的隐式稀疏性)

随着模型特征维度的增大，Dense 模型内部处理不同任务（如编程域与医疗域）时的神经元激活群以及梯度分布可能呈现出较低的重叠率与近正交趋向。即使底座在物理架构上是 Dense 的，在特征路线上也可能具备一定程度的逻辑稀疏性，但这仍需要通过 probing 验证。

### H2 (顺应稀疏性的微调架构改造)

在保留 Dense 架构的前提下，通过在微调阶段引入**"稀疏感知/正交保护"的改造层（如基于激活稀疏路由的 PEFT 或正交子空间参数低秩更新）**，可能有助于隔离微调时的梯度干扰。如果大模型确实具备天然的局部稀疏特征，这类方法可能利用该特性来降低连续域微调中的知识覆写风险。

## 方案

### 基础配置

- **模型基座选择**：选取具备"涌现能力级"体量且架构前沿的开源 SOTA 模型，具体考虑以下两个候选：
  - **Qwen3.5-27B（Dense）**：27B 全参数密集架构，所有参数在每次前向传播中均被激活，推理密度最高。适合作为 dense baseline 探测隐式稀疏性。
  - **Qwen3.5-35B-A3B（MoE）**：35B 总参数 / 3B 激活参数，采用 Gated Delta Networks + MoE 混合架构。自带条件性稀疏路由，可直接观测 expert routing 与任务之间的对应关系，为稀疏感知微调提供更自然的切入点。
- **任务序列流**：多领域序列设定（代码 -> 数学 -> 医疗 -> 日常对话逻辑）。

### 微调架构改造方案设计

在全参微调（Full Fine-Tuning）和常规 LoRA 之外，探索一种利用"隐式任务边界"的微调策略：

- **方案 A - 激活稀疏感知更新 (Activation-mask Tuning)**：冻结模型主体。通过对新任务样本前向传播时的神经元激活模式进行探测，仅解冻或建立针对当前任务高激活通路的旁支更新（通过稀疏掩码限制无关神经元被微调梯度影响）。
- **方案 B - 隐式正交空间映射**：在引入 Adapter/LoRA 时，计算新领域参数增量相对于旧领域参数的历史梯度的正交方向，尝试在该任务的空闲子空间内生长新的表示。

### 可视化与探测验证 (Probing)

除了常规评估各种领域的任务得分外，建议设计专门的探针来验证"隐式稀疏性"是否真实存在：

- **高能激活交集计算 (Activation Overlap)**：统计两项完全不同的任务经过不同层时，Top-K 激活神经元群落的交并比（IoU）。
- **参数扰动追踪**：对比该专项微调架构开启与不开启时，模型在吸收目标新知识后，其底层核心通识隐向量在关键子空间的漂移距离（Subspace Representation Drift）。

## 算力成本评估

微调阶段使用 PEFT（LoRA/Adapter），推理时需前向传播进行激活探测。以下分别评估两个候选基座的资源需求：

### 方案 A：Qwen3.5-27B（Dense）

- **显存占用**：BF16 权重约 54 GB，LoRA 微调 + 激活缓存约 70–90 GB，可在 1×A100-80GB 上完成（开启 gradient checkpointing）
- 激活探测阶段（per domain）：单次前向 pass ~4–8 A100 GPU 小时
- 微调阶段（per domain）：LoRA 微调 ~16–32 A100 GPU 小时
- 4 个域完整实验 + 探针分析：~160–320 A100 GPU 小时
- 云端成本：~$400–800

### 方案 B：Qwen3.5-35B-A3B（MoE）

- **显存占用**：虽仅激活 3B 参数，但全部 35B 参数需加载至显存（BF16 约 70 GB）；LoRA 微调约 80–100 GB，需 1×A100-80GB（紧凑）或 2×A100-40GB
- 激活探测阶段（per domain）：由于每 token 仅 3B 激活参数，前向推理速度显著快于 27B Dense，~3–6 A100 GPU 小时
- 微调阶段（per domain）：梯度仅流经激活 expert，LoRA 微调 ~12–24 A100 GPU 小时
- 4 个域完整实验 + 探针分析：~120–240 A100 GPU 小时
- 云端成本：~$300–600
- **额外优势**：MoE 天然具备 expert routing 记录，可直接分析不同域数据的 expert 选择模式，无需额外设计激活探针

## 预期产出

1. **隐式稀疏性的实证观察**：通过神经元和子空间的探针分析，希望提供关于 SOTA 级大模型内部不同领域激活通路重叠程度的初步定量观察，为理解大模型中"通用能力"与"领域技能"的编码关系积累证据。
2. **面向持续学习的微调策略探索**：在观察到隐式稀疏特征（如果存在）的前提下，初步评估稀疏感知微调方案在缓解持续微调遗忘方面的可行性，为后续更系统的方法设计提供方向参考。
