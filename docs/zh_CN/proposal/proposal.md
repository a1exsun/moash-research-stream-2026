## 人脑的持续学习架构

我的思考从对比人脑和目前的LLM架构出发的。

## 消融实验

我的推测是，transformer架构的dense特性可能从物理层面限制了LLM的持续学习能力。一个权重如果交叉继承了很多的知识（superposition），那么任何一次微调改动都不可避免地会破坏掉一部分旧知识。

此外，人脑最重要的持续学习机制来源于新皮层和海马体的分离。人脑会在睡眠时将海马体中的短期记忆重放给新皮层，从而巩固记忆。尽管我已经看到了一些论文尝试模拟这个过程（比如用新旧数据混合微调来模拟经验重放），但是我难以查询到一个定量的结论来了解这些过程对模型持续学习能力的提升。

所以我想，也许可以设计一个消融实验，基于nanochat这样的小规模模型，来系统地研究这些问题，给出定量结论。我想这具备一定地价值，因为nanochat和SOTA模型具备相似的架构特性，可以通过scaling来推测SOTA模型的表现。

基于这个假设，我整理了提议01。

## cross-over假设

在思考消融实验的实验设计时，我产生了一个想法：很多研究者执着于SOTA模型相关的探索，试图通过设计复杂的机制延缓灾难性遗忘。但是有没有可能，人脑的持续学习也不是全能的，人类实际上每天也会遗忘大量旧的知识，并且随着年龄和知识的增长，我们也更倾向于成为一个领域专家（或者是一个广泛涉猎各种行业但不精通的人）。

所以应当把追求的目标改为：在一种理想地持续学习架构下，模型仍然会从一个通识模型，逐渐演化成一个领域专家。它仍然需要保留通用智能（比如逻辑能力、语言能力等），但是我们允许它遗忘非目标领域的知识，只考察其在目标领域的能力。比如我们只考察它是否完全具备了和用户对话的所有记忆、掌握了某个企业的领域知识等等，不再考察它的编码能力、文学能力等等。

进一步演绎，我认为存在这样一个核心的 cross-over 假设：持续学习模型最初的能力往往不如基线模型（因为为了搭建持续学习架构，可能需要牺牲一部分transformer天然的信息压缩与泛化能力，其全局注意力与密集更新在赋予强大初始能力的同时也限制了持续学习表现）。但是，只要这个新的持续学习模型在多轮目标领域的持续微调之后，其能力能够不仅超越同等规模的transformer基线模型，还能突破基线模型本身的微调极限，实现明确的 cross-over 交叉上穿，那么这种架构上针对“初始能力”的牺牲就是极具价值的。

这不是领域微调，也不是领域模型，而是一个具备持续学习能力的模型。相比传统领域微调/领域模型，它能够持续吸收当前领域的新知识，只不过我们不要求它每天学习完全不同领域的知识。我认为这就和人类一样，比如我记得我的第一份工作是Java开发，然而由于多年不写Java代码，我对它的语法、框架的知识已经遗忘了一大半。我想假设未来我多年不从事开发/研究工作了，那么我也会遗忘这个领域的大部分知识。

基于这个假设，我整理了提议02。

## Compression-Writability Bound

在探索 cross-over 假设的过程中，我意识到一个更深层的问题：stability-plasticity dilemma 在持续学习领域被反复提及，但它始终只是一个模糊的定性说法，从未被形式化为一个可以证明或证伪的命题。

我尝试从 activation sparsity 的角度切入这个问题。核心逻辑链是：模型把多个概念压缩编码到同一组参数里（superposition），激活稀疏度决定了写入新知识时对旧知识的干扰程度。那么对于任何只使用一种固定激活模式的系统（single-regime），它能学习的"任务数量 × 每个任务的学习深度"应当存在一个数学上的硬性上界（capacity ceiling）。想要突破这个上界，至少需要两套不同稀疏度的子系统配合工作——这恰好对应了大脑的海马体（极端稀疏，快速记录）和新皮层（适度稠密，深度压缩）的分工。

这个理论如果能成立，就能解释为什么 EWC、GPM 等现有方法在长任务序列上必然退化——它们始终在单系统的天花板之下操作。当然如果理论不成立，也是一个有价值的中间成果。

基于这个假设，我整理了提议03。

## scale dense model的潜在sparsity

在深入了解目前的LLM架构过程中，我意识到：也许规模本身就是核心问题？

在scaling law不断扩展的前提下，也许LLM模型内部就会涌现出隐式的稀疏性？也许在完全不同的技能、不同的知识之间，天然具备正交性，从而在微调时不会相互干扰。

我想这也值得深入研究一下。不过相比起单纯地构造实验，我认为面对SOTA模型最好的策略是直接通过设计一种面向持续学习问题的微调架构改造，在过程中验证其稀疏性是否真实存在。

基于这个假设，我整理了提议04。

## benchmark

在广泛了解相关论文的过程中，我发现持续学习方向的评估体系非常单一，无论是面向持续预训练、持续微调还是工程化记忆，基本都是通过构造数据集的方式来跑分评估（比如TiC-LM、EvoWiki等）。截止目前最前沿的成果MemoryBench，实际上也是将11组公开数据集与LLM-as-User-Simulator结合，来模拟评估。

对此我的想法是：在作为软件开发者时，我从来不在乎静态CodeBench的跑分结果（我想很多人也是如此）。我们只在乎LLM Arena或Reddit/X讨论这样的真实用户反馈。目前持续学习领域没有这样的评估系统，我想也许可以尝试构建一个。正好我搭建了一个结构化叙事的AI陪伴应用，也许可以在此基础上构建一套依赖Human Feedback的评估系统（用户天然在乎哪个LLM在长程叙事时最符合剧情框架和历史记忆，感知也最明确）。

基于这个假设，我整理了提议05。

## NSA (Native Sparse Attention)

在了解最新的LLM演进方向的过程中，我注意到NSA已然成为整个2025-2026年的核心趋势。几乎所有的SOTA 开源LLM都采用了NSA/DSA/MLA机制：

- DeepSeek v3.2: 采用 DSA (DeepSeek Sparse Attention) 实现原生亚二次稀疏注意力。
- Qwen v3.5: 采用 Gated DeltaNet 与全注意力 3:1 混合架构。
- Kimi K2.5: 采用 KDA 与 MLA 3:1 混合架构。
- GLM-5: 保留传统 Transformer 结构，引入了 DSA 与 MLA。
- MiniMax M2.5: 采用 Lightning Attention 全线性框架。

我注意到 DSA 这种将注意力计算拓扑解耦为“压缩常识流”和“优先选择聚焦细粒度流”的原生设计，可能提供了解决灾难性遗忘的突破口。如果在持续微调中加以非对称利用（仅更新聚焦流而冻结或减缓压缩流），大模型可能将具备极强的抗遗忘免疫。

然而我发现这个方向的算力预算要求太高，所有的稀疏注意力开源模型都太大了，故暂时没想到合适的推进方式。

## 小规模验证

原生 DSA 架构的开源模型规模均达数百亿级别，导致进行探索验证所需的实验算力完全超出了预算；另一方面，当前百兆级的“小稀疏模型”往往仅是 FFN 层的 MoE，无法论证 Attention 层解耦的有效性。

故我认为也许可以借用开源的 DSA/Indexer 算子，在小规模数据集上从零预训练一个小尺寸（例如~0.1B 级）的 DSA 架构，进而与传统 Transformer 进行洗礼对比实验。

基于这个假设，我整理了提议06。
