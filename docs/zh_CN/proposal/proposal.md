## 几个research proposals

这篇笔记简单记录了我的研究过程，并且关联了几个我在研究过程中产生的research proposal。我希望可以得到您的反馈，选择一个最合适的proposal作为我minor thesis的研究方向，或者基于我目前的进展进一步衍生其他有价值的研究方向。

## 消融实验

我的思考从人脑的持续学习机制与LLM模型的对比开始，其中我认为人脑的结构性稀疏（而不仅仅是MoE稀疏或激活稀疏）与新皮层-海马体分离机制尤其重要。

我的推测是，transformer架构的dense特性可能从物理层面限制了LLM的持续学习能力。一个权重如果交叉继承了很多的知识（superposition），那么任何一次微调改动都可能会破坏掉一部分旧知识。

此外，人脑最重要的持续学习机制来源于新皮层和海马体的分离。人脑会在睡眠时将海马体中的短期记忆重放给新皮层，从而巩固记忆。尽管我已经看到了一些论文尝试模拟这个过程（比如用新旧数据混合微调来模拟经验重放），但是我难以查询到一个定量的结论来了解这些过程对模型持续学习能力的提升。

所以我想，也许可以设计一个消融实验，基于nanochat这样的小规模模型，来系统地研究这些问题，获得初步定量证据。我想这具备一定的价值，因为nanochat和SOTA模型具备相似的架构特性，或许能为后续scaling研究提供线索。

基于这个假设，我整理了[提议01](proposal_01_ablation_experiments.md)。

## cross-over假设

在思考消融实验的实验设计时，我产生了一个想法：很多研究者执着于SOTA模型相关的探索，试图通过设计复杂的机制延缓灾难性遗忘。但是有没有可能，人脑的持续学习也不是全能的，人类实际上每天也会遗忘大量旧的知识，并且随着年龄和知识的增长，我们也更倾向于成为一个领域专家（或者是一个广泛涉猎各种行业但不精通的人）。

所以也许可以把追求的目标调整为：在一种理想的持续学习架构下，模型可能会从一个通识模型，逐渐演化成一个领域专家。它仍然需要保留通用智能（比如逻辑能力、语言能力等），但是我们允许它遗忘非目标领域的知识，重点考察其在目标领域的能力。比如我们更关注它是否尽可能保留和用户对话相关的历史记忆、掌握某个企业的领域知识等，而不再强调其编码能力、文学能力等无关维度。

进一步演绎，我认为存在这样一个核心的 cross-over 假设：持续学习模型最初的能力可能不如基线模型（因为为了搭建持续学习架构，可能需要牺牲一部分transformer天然的信息压缩与泛化能力，其全局注意力与密集更新在赋予强大初始能力的同时也可能限制持续学习表现）。但如果这个新的持续学习模型在多轮目标领域的持续微调之后，在部分设定下能够超越同等规模的transformer基线模型，甚至接近或达到基线模型的既有微调上限，那么这种架构上针对“初始能力”的取舍就值得进一步研究。

这不是领域微调，也不是领域模型，而是一个具备持续学习能力的模型。相比传统领域微调/领域模型，它能够持续吸收当前领域的新知识，只不过我们不要求它每天学习完全不同领域的知识。我觉得这和人类学习过程有一定相似性，比如我记得我的第一份工作是Java开发，然而由于多年不写Java代码，我对它的语法、框架的知识已经遗忘了一大半。我也会假设，未来如果多年不从事开发/研究工作，自己可能会继续遗忘这个领域的大部分知识。

基于这个假设，我整理了[提议02](proposal_02_nanogpt_pretrain_dense_vs_sparse.md)。

## scale dense model的潜在sparsity

在深入了解目前的LLM架构过程中，我意识到：也许规模本身就是一个关键因素？

在scaling law不断扩展的前提下，也许LLM模型内部会涌现出隐式的稀疏性？也许在完全不同的技能、不同的知识之间，激活通路重叠会更低，从而在微调时干扰减弱。

我想这也值得深入研究一下。不过相比起单纯地构造实验，我认为面对SOTA模型，一个可行策略是直接设计一种面向持续学习问题的微调架构改造，并在过程中验证其稀疏性是否真实存在。

基于这个假设，我整理了[提议03](proposal_03_sota_sparse_conversion.md)。

## benchmark

在广泛了解相关论文的过程中，我发现持续学习方向的评估体系相对集中，无论是面向持续预训练、持续微调还是工程化记忆，基本都是通过构造数据集的方式来跑分评估（比如TiC-LM、EvoWiki等）。截止目前，MemoryBench 也是将公开数据集与LLM-as-User-Simulator结合，来进行模拟评估。

对此我的想法是：在应用场景里，静态 benchmark 跑分往往不够，我们通常也会参考LLM Arena或Reddit/X讨论这类真实用户反馈。目前持续学习领域这类评估系统还比较少，我想也许可以尝试构建一个。正好我搭建了一个结构化叙事的AI陪伴应用，也许可以在此基础上构建一套依赖Human Feedback的评估系统（用户天然在乎哪个LLM在长程叙事时最符合剧情框架和历史记忆，感知也最明确）。

基于这个假设，我整理了[提议04](proposal_04_benchmark.md)。

## NSA (Native Sparse Attention)

在了解最新的LLM演进方向的过程中，我注意到NSA/DSA/MLA在2025-2026年的部分开源模型中出现频率较高。下面列出我目前关注到的一些例子：

- DeepSeek v3.2: 采用 DSA (DeepSeek Sparse Attention) 实现原生亚二次稀疏注意力。
- Qwen v3.5: 采用 Gated DeltaNet 与全注意力 3:1 混合架构。
- Kimi K2.5: 采用 KDA 与 MLA 3:1 混合架构。
- GLM-5: 保留传统 Transformer 结构，引入了 DSA 与 MLA。
- MiniMax M2.5: 采用 Lightning Attention 全线性框架。

我注意到 DSA 这种将注意力计算拓扑解耦为“压缩常识流”和“优先选择聚焦细粒度流”的原生设计，可能提供一个值得检验的方向。如果在持续微调中加以非对称利用（仅更新聚焦流而冻结或减缓压缩流），大模型也许能够表现出更好的抗遗忘能力。

然而我发现这个方向的算力预算要求较高，现有稀疏注意力开源模型体量普遍较大，故暂时没想到更稳妥的推进方式。

## 小规模验证

原生 DSA 架构的开源模型规模通常较大，导致进行探索验证所需的实验算力显著超出当前预算；另一方面，当前百兆级的“小稀疏模型”往往仅是 FFN 层的 MoE，难以直接论证 Attention 层解耦的有效性。

故我认为也许可以借用开源的 DSA/Indexer 算子，在小规模数据集上从零预训练一个小尺寸（例如~0.1B 级）的 DSA 架构，进而与传统 Transformer 做对照实验。

基于这个假设，我整理了[提议05](proposal_05_dsa_tiny_sandbox.md)。
