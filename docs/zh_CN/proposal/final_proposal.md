## 人脑的持续学习架构

我的思考从对比人脑和目前的LLM架构出发的。

## 消融实验

我的推测是，transformer架构的dense特性可能从物理层面限制了LLM的持续学习能力。一个权重如果交叉继承了很多的知识，那么任何一次微调改动都不可避免地会破坏掉一部分旧知识。

此外，人脑最重要的持续学习机制来源于新皮层和海马体的分离。人脑会在睡眠时将海马体中的短期记忆重放给新皮层，从而巩固记忆。尽管我已经看到了一些论文尝试模拟这个过程（比如用新旧数据混合微调来模拟经验重放），但是我难以查询到一个定量的结论来了解这些过程对模型持续学习能力的提升。

所以我想，也许可以设计一个消融实验，基于nanoGPT这样的小规模模型，来系统地研究这些问题，给出定量结论。我想这具备一定地价值，因为nanoGPT和SOTA模型具备相似的架构特性，可以通过scaling来推测SOTA模型的表现。

基于这个假设，我整理了提议01。

## cross-over假设

在思考消融实验的实验设计时，我产生了一个想法：很多研究者执着于SOTA模型相关的探索，试图通过设计复杂的机制延缓灾难性遗忘。但是有没有可能，人脑的持续学习也不是全能的，人类实际上每天也会遗忘大量旧的知识，并且随着年龄和知识的增长，我们也更倾向于成为一个领域专家（或者是一个广泛涉猎各种行业但不精通的人）。

所以应当把追求的目标改为：在一种理想地持续学习架构下，模型仍然会从一个通识模型，逐渐演化成一个领域专家。它仍然需要保留通用智能（比如逻辑能力、语言能力等），但是我们允许它遗忘非目标领域的知识，只考察其在目标领域的能力。比如我们只考察它是否完全具备了和用户对话的所有记忆、掌握了某个企业的领域知识等等，不再考察它的编码能力、文学能力等等。

进一步演绎，我认为也许存在一个这样的cross-over效应：为了搭建持续学习架构，我们需要牺牲一部分transformer架构天然的信息压缩与泛化能力。因为全局注意力机制与反向传播在给予模型强大初始能力的同时也天然限制了持续学习表现。那么只要这个新的持续学习架构模型能在一定周期的学习后，在目标领域的能力上超越同等规模的transformer模型，实现cross-over，那么它就是有价值的。

基于这个假设，我整理了提议02。

## Compression-Writability Bound

在探索 cross-over 假设的过程中，我意识到一个更深层的问题：stability-plasticity dilemma 在持续学习领域被反复提及，但它始终只是一个模糊的定性说法，从未被形式化为一个可以证明或证伪的命题。

我尝试从 activation sparsity 的角度切入这个问题。核心逻辑链是：模型把多个概念压缩编码到同一组参数里（superposition），激活稀疏度决定了写入新知识时对旧知识的干扰程度。那么对于任何只使用一种固定激活模式的系统（single-regime），它能学习的"任务数量 × 每个任务的学习深度"应当存在一个数学上的硬性上界（capacity ceiling）。想要突破这个上界，至少需要两套不同稀疏度的子系统配合工作——这恰好对应了大脑的海马体（极端稀疏，快速记录）和新皮层（适度稠密，深度压缩）的分工。

这个理论如果能成立，就能解释为什么 EWC、GPM 等现有方法在长任务序列上必然退化——它们始终在单系统的天花板之下操作。

基于这个假设，我整理了提议03。

## scale dense model的潜在sparsity

在深入了解目前的LLM架构过程中，我意识到：也许规模本身就是核心问题？

在scaling law不断扩展的前提下，也许LLM模型内部就会涌现出隐式的稀疏性？也许在完全不同的技能、不同的知识之间，天然具备正交性，从而在微调时不会相互干扰。

我想这也值得深入研究一下。不过相比起单纯地构造实验，我认为面对SOTA模型最好的策略是直接通过设计一种面向持续学习问题的微调架构改造，在过程中验证其稀疏性是否真实存在。

基于这个假设，我整理了提议04。

## benchmark

在广泛了解相关论文的过程中，我发现持续学习方向的评估体系非常单一，无论是面向持续预训练、持续微调还是工程化记忆，基本都是通过构造数据集的方式来跑分评估（比如TiC-LM、EvoWiki等）。截止目前最前沿的成果MemoryBench，实际上也是将11组公开数据集与LLM-as-User-Simulator结合，来模拟评估。

对此我的想法是：在作为软件开发者时，我从来不在乎静态CodeBench的跑分结果（我想很多人也是如此）。我们只在乎LLM Arena或Reddit/X讨论这样的真实用户反馈。目前持续学习领域没有这样的评估系统，我想也许可以尝试构建一个。正好我搭建了一个结构化叙事的AI陪伴应用，也许可以在此基础上构建一套依赖Human Feedback的评估系统（用户天然在乎哪个LLM在长程叙事时最符合剧情框架和历史记忆，感知也最明确）。

基于这个假设，我整理了提议05。

## NSA (Native Sparse Attention)

在了解最新的LLM演进方向的过程中，我注意到NSA已然成为整个2025-2026年的核心趋势。几乎所有的SOTA 开源LLM都采用了NSA/DSA/MLA机制：

- DeepSeek v3.2: 采用 DSA (DeepSeek Sparse Attention) 实现原生亚二次稀疏注意力。
- Qwen v3.5: 采用 Gated DeltaNet 与全注意力 3:1 混合架构。
- Kimi K2.5: 采用 KDA 与 MLA 3:1 混合架构。
- GLM-5: 保留传统 Transformer 结构，引入了 DSA 与 MLA。
- MiniMax M2.5: 采用 Lightning Attention 全线性框架。

我注意到 DSA 这种将注意力计算拓扑解耦为“压缩常识流”和“优先选择聚焦细粒度流”的原生设计，可能提供了解决灾难性遗忘的第一性原理突破口。如果在持续微调中加以非对称利用（仅更新聚焦流而冻结或减缓压缩流），大模型将具备极强的抗遗忘免疫。

基于这个假设，我整理了提议06。

## 小规模验证

然而由于原生 DSA 架构的开源模型规模均达数百亿级别，导致进行探索验证所需的实验算力完全超出了常规实验室预算；另一方面，当前百兆级的“小稀疏模型”往往仅是 FFN 层的 MoE，无法论证 Attention 层解耦的有效性。

为解决“原生稀疏架构验证”与“极低显存花销”的核心矛盾，基于不同深度的降维假说，我构思整理了两个补充提议：

- **提议07（后天手术魔改）：** 针对现有的 1-3B 级稠密底座，设计强行割裂其注意力权重矩阵的代码级“外科手术”（一半作为静态常识，一半套用 Top-K 形成后天硬稀疏）。以极高的算力性价比验证非对称稀疏微调能否有效抵抗遗忘。
- **提议08（微型原生沙盒）：** 如果魔改稠密底座不够纯净且可能破坏原有机制，那么就借用纯粹的 DSA/Indexer 算子，在微型合成数据集（如 TinyStories）上从零预训练一个极微小尺寸（例如~0.1B 级）的 DSA 架构，进而与传统 Transformer 进行洗礼对比实验。
