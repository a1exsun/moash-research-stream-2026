## 人脑的持续学习架构

我的思考从对比人脑和目前的LLM架构出发的。

## 消融实验

我的推测是，transformer架构的dense特性可能从物理层面限制了LLM的持续学习能力。一个权重如果交叉继承了很多的知识，那么任何一次微调改动都不可避免地会破坏掉一部分旧知识。

此外，人脑最重要的持续学习机制来源于新皮层和海马体的分离。人脑会在睡眠时将海马体中的短期记忆重放给新皮层，从而巩固记忆。尽管我已经看到了一些论文尝试模拟这个过程（比如用新旧数据混合微调来模拟经验重放），但是我难以查询到一个定量的结论来了解这些过程对模型持续学习能力的提升。

所以我想，也许可以设计一个消融实验，基于nanoGPT这样的小规模模型，来系统地研究这些问题，给出定量结论。我想这具备一定地价值，因为nanoGPT和SOTA模型具备相似的架构特性，可以通过scaling来推测SOTA模型的表现。

基于这个假设，我整理了提议01。

## cross-over假设

在思考消融实验的实验设计时，我产生了一个想法：很多研究者执着于SOTA模型相关的探索，试图通过设计复杂的机制延缓灾难性遗忘。但是有没有可能，人脑的持续学习也不是全能的，人类实际上每天也会遗忘大量旧的知识，并且随着年龄和知识的增长，我们也更倾向于成为一个领域专家（或者是一个广泛涉猎各种行业但不精通的人）。

所以应当把追求的目标改为：在一种理想地持续学习架构下，模型仍然会从一个通识模型，逐渐演化成一个领域专家。它仍然需要保留通用智能（比如逻辑能力、语言能力等），但是我们允许它遗忘非目标领域的知识，只考察其在目标领域的能力。比如我们只考察它是否完全具备了和用户对话的所有记忆、掌握了某个企业的领域知识等等，不再考察它的编码能力、文学能力等等。

进一步演绎，我认为也许存在一个这样的cross-over效应：为了搭建持续学习架构，我们需要牺牲一部分transformer架构天然的信息压缩与泛化能力。因为全局注意力机制与反向传播在给予模型强大初始能力的同时也天然限制了持续学习表现。那么只要这个新的持续学习架构模型能在一定周期的学习后，在目标领域的能力上超越同等规模的transformer模型，实现cross-over，那么它就是有价值的。

基于这个假设，我整理了提议02。

## scale dense model的潜在sparsity

在深入了解目前的LLM架构过程中，我意识到：也许规模本身就是核心问题？

在scaling law不断扩展的前提下，也许LLM模型内部就会涌现出隐式的稀疏性？也许在完全不同的技能、不同的知识之间，天然具备正交性，从而在微调时不会相互干扰。

我想这也值得深入研究一下。不过相比起单纯地构造实验，我认为面对SOTA模型最好的策略是直接通过设计一种面向持续学习问题的微调架构改造，在过程中验证其稀疏性是否真实存在。

基于这个假设，我整理了提议03。

##
