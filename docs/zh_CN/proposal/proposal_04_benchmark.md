# Proposal 04: 探索基于人类长程互动的持续学习动态评测方案

## 研究背景与核心动机

当前持续学习（Continual Learning）领域的评测大多依赖于静态的 Benchmark 数据集。然而，随着大型语言模型（LLM）能力的提升，静态数据集在反映模型真实的长期记忆维持和长程推理能力时，可能存在一定的局限性（例如由于数据污染导致的过拟合风险）。

为了更真实地评估模型在复杂、长周期情境下的持续学习表现，**本提案计划探索一种基于人类真实反馈（Human Feedback）的动态评测方案。** 想法是引入人类用户对“模型是否维持了历史设定的一致性”的敏感感知，来作为传统自动化评估指标的有益补充。

### 实验沙盒：基于结构化状态机的文本互动环境

为了开展这项研究，一个可能的切入点是复用我个人 side project Anify 引擎中的结构化交互沙盒环境，也可以考虑其他开源可用的方案（如文字冒险/TRPG 类引擎）进行初步试点。

以 Anify 为例，这类环境不仅包含自由的自然语言交互，还在底层维护了一个结构化的状态机（State Machine），包括但不限于：

- **角色关系图谱**：如好感度数值与关键互动历史摘要。
- **环境与任务状态**：如已触发的事件标志（Flags）和任务完成进度。

**引入该环境的初步设想：**
这种“自然语言 + 结构化状态”的交互模式，可能为持续学习提供了一个较好的观察切入点。在较长的时间跨度内，模型生成的文本需要与底层的结构化状态保持逻辑上的一致。例如，模型不应遗忘之前的关键剧情或改变已固化的角色关系。我的直觉是，用户在这种场景下对“记忆矛盾”的主观纠错反馈，可能是一种高质量的数据信号，但这还需要验证。

## 探索性假设

### H1（人工反馈的补充价值）

在长周期的叙事或交互场景中，基于真实互动的用户反馈可能比当前的静态自动化指标更有效地暴露出模型在知识保持上的一致性断裂。

### H2（状态机作为锚点）

底层的结构化状态集（如属性值、事件记录）可以作为客观的探针（Probing），用于自动化检测模型生成文本是否发生了隐含的知识漂移或事实冲突。

## 初步实验方案

初步想法是在受控的小规模环境下，尝试以下几种评估维度的收集：

### 1. 初步的 A/B 侧边栏对比 (Side-by-side Evaluation)

在涉及长程记忆调用（如触发历史重大事件）的关键对话轮次中，后台并行输出两种基座模型（或记忆维持策略）的生成结果。可邀请测试用户基于“剧情连贯性”和“无遗忘程度”给出偏好选择，以此观察在长文本跨度下的方案差异。

### 2. 交互式的一致性纠错反馈

在用户界面引入轻量级的纠错标记（例如“设定偏离”或“记忆错误”标签）。尝试统计和分析在连续多轮对话中，不同架构导致的用户主观纠错率（Error Correction Rate），探索其作为一种遗忘惩罚指标的可行性。

### 3. 基于状态探针的客观一致性检测

除主观反馈外，还可以在后台加入定期检测脚本，比对模型近期对话的隐式表征与物理状态集的锚点。观察在未进行针对性训练的情况下，模型经过长序列输入后，其输出倾向偏离设定基准的客观速率。

## 资源需求与初步评估

作为一项偏重应用端的评测探索，算力需求相对较低，主要的挑战在于工程环境的搭建与早期测试数据的获取：

- **工程侧**：初期需要投入一定的工程精力完成沙盒平台的通讯链路和数据采集打点。
- **算力侧**：基于初步试点（如数十位受控测试参与者），推理成本可暂通过实验室已有的 API 额度或中小规模开源模型本地部署来覆盖。
- **实施计划**：建议先在一个极其简化的微缩场景（如单一NPC互动）中跑通数据闭环，验证方法的可行性后，再考虑丰富场景的维度。

## 预期讨论价值

1. **评测视角的拓展**：尝试为持续学习领域引入一种基于动态用例的评估思路，探讨真实 human-in-the-loop 数据在缓解过度依赖静态跑分方面的作用。
2. **经验数据的积累**：通过收集包含真实遗忘痛点和人类纠偏的交互数据，可能为后续的大模型长程对齐研究（如 RLHF / DPO）提供早期的数据样本。
3. **暴露真实痛点**：直观地为后续深入研究长效记忆机制确立一个现实的 Baseline 和优化靶标。
