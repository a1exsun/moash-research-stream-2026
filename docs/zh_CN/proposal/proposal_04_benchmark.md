我假期搭建了一个AI陪伴类应用，目前准备发布。我希望通过真实对话数据构建一个基于Human Feedback的评估系统，用来替代目前低效的持续学习benchmark，评估不同持续学习策略在真实的长程用例下的表现。

核心洞察是：类似LLM Benchmark的情况，大量的固定套路Benchmark非常容易被作弊、过拟合。而类似LLM arena这样的评估系统成为了事实上最有影响力的评估方式。

我构建的AI陪伴类应用，与市面上的竞品相比最大的不同是，我采用了结构化剧情大纲+长程冒险机制，用户会和AI伙伴一同进行trpg冒险，在无数次冒险中提升等级、装备、好感度，获得新的道具、展开新的剧情。我认为这一类长程用例非常适合作为持续学习的评估基准。
