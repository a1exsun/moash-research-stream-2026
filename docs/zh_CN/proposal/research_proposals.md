# 持续学习研究提案

## 提案 1：机制消融研究

**目标：** 在统一的训练预算下，系统地分解三种核心持续学习机制的独立贡献及其相互作用。

**研究机制：**

- 结构化稀疏拓扑（Structural sparse topology）
- 选择性写入门控（Selective write gating）
- 离线整合 / 重放（Offline consolidation / replay）

**方法：** 采用 2×2×2 析因设计（factorial design），隔离每种机制的边际贡献以及它们之间的两两/三方交互作用。

---

## 提案 2：面向持续学习的后验稀疏转换

**目标：** 测试将预训练的稠密大语言模型（LLM）转换为结构化稀疏模型（例如 2:4 稀疏）是否能直接减少序列微调过程中的灾难性遗忘。

---

## 提案 3：原生稀疏预训练与干扰相位边界

**目标：** 探索原生稀疏预训练模型是否会改变“高遗忘”和“低遗忘”阶段之间的关键边界。

**核心主张：** 持续学习中的遗忘表现出可复现的**相变**（phase transitions），而原生稀疏性会系统地移动这些边界。这被定义为一个机制性发现，而不仅仅是“方法 X 分数更高”。

**框架：** 风险比率 $R_t = I_t / C_t$（干扰与塑性容量之比），用于描述遗忘加速的临界点。

---

## 提案 4：曲率分解持续学习 (CDCL)

**目标：** 一种纯粹的训练规则修改（无需架构更改，无需外部记忆）来实现持续学习。

**方法：** 将梯度分解为：

- **知识敏感子空间**（knowledge-sensitive subspace）分量 → 施加强约束（保留旧知识）
- **塑性子空间**（plasticity subspace）分量 → 保持高学习率（赋能新学习）

这本质上是 EWC/GPM 的低秩子空间泛化版本。

---

## 提案 5：压缩-可写性容量边界

**目标：** 从第一性原理出发，将稳定性-塑性困境形式化为可证明的容量边界。

**核心思想：** 在任何具有固定激活稀疏度的单一机制（single-regime）系统中，都存在一个根本性的天花板：

$$C(s, \epsilon) \cdot B(s) \leq \Phi(d)$$

其中 $C$ = 在不发生遗忘的情况下可学习的任务数量，$B$ = 每个任务的学习质量，$\Phi(d)$ 是模型维度的函数。

**方法：**

1. 在 Hopfield 网络上精确证明该边界（基于 Tsodyks & Feigelman, 1988 的研究）
2. 在 Transformer 上作为实证猜想进行验证
3. 证明**双机制架构**（类似于海马体-新皮层互补学习系统，但完全参数化）是突破单一机制天花板的充分条件
