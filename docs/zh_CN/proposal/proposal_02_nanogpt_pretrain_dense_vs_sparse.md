# Proposal 02: 基于 nanochat 的持续学习 Cross-over 效应

## 研究动机

从我目前的阅读来看，持续学习研究大多聚焦于延缓灾难性遗忘，希望模型尽可能保留既有知识。但人脑的学习似乎并非如此——人类在成长过程中也会遗忘部分早期或低价值事实，最终更趋向于形成**领域专家**（同时保留基础逻辑与通用语言能力）。

由此我想到一个值得检验的猜想（暂称 **Cross-over 假设**）：**持续学习模型最初的能力可能不如基线模型，但在多轮微调后，可能在某个阶段超过基线模型，并在部分设定下接近或超过其既有微调上限。**

如果这个猜想成立，意味着为了探索更可持续的持续学习架构，可能需要接受一部分 Dense Transformer 在初始压缩与泛化上的优势。如果新型架构（例如原生稀疏架构）能在多轮目标领域微调后展现出更好的长期演化潜力，那么这种对"初始容量"的取舍就值得进一步研究。

## 假设

### H1 (初始能力差异)

在相同的预训练算力与数据下，传统的 Dense Transformer 由于其全局注意力和密集的反向传播，可能会展现出比"具备独立隔离机制的新型架构（如原生稀疏）"更强的初始通识能力和更低的 Perplexity。

### H2 (Cross-over 交叉效应)

在进入特定目标领域的多轮持续微调后：

- **基线模型 (Dense)**：受限于权重交叉继承，可能较早遭遇目标领域吸收瓶颈。在持续微调过程中，目标性能增益可能放缓，并伴随一定通用能力退化风险。
- **持续学习架构模型 (Sparse 等)**：按 Cross-over 假设，该模型**最初能力可能低于基线模型**（起点较低）。但如果其相互覆写更弱，则有机会在更长的微调序列中保持持续改进，并在某个节点与基线形成可观测的 Cross-over 交叉曲线。

## 方案

### 基础配置

- **基座规模**：nanochat (约 561M 参数，depth=20 官方推荐配置)，以保证验证此类底层预训练假设的高效可行性。
- **评测分离体系**：
  - **通用智能底线集 (General Intelligence Baseline)**：仅测试最基础的语言流利度、基础逻辑推断（允许遗忘具体百科事实知识，但不允许丧失底层的"思考流利度"）。
  - **目标深水区领域集 (Target Domain)**：例如深度的医疗长文本、复杂的代码库或特定企业的私有知识域。

### Phase A: 预训练阶段

分别使用相同的通用语料（如 FineWeb-Edu 的一个子集）从头预训练两个架构：

1. `Model-Dense`：标准结构 nanochat。
2. `Model-CL`：带有持续学习特殊机制的 nanochat（如引入 Proposal 01 中的结构化稀疏设计来限制交叉继承）。
   _设定检查点：检验在通识测试集上，`Model-Dense` 是否存在初始性能优势。_

### Phase B: 持续域演化阶段

将两个预训练好的模型同时投入目标领域（Domain X）进行持续的流式长序列微调。

- 每注入一定数据量（例如每 10M Tokens）作为一个 Epoch 节点，同时抓取两个模型进行 Domain X 专业测试与通用底线集测试。
- 绘制两条随训练时间/数据量推移的能力演进对比曲线。

## 算力成本评估

利用 nanochat 的中小规模（561M），跑通这套包含"预训练 + 持续微调"的端到端实验，整体算力在可控范围内（单卡 A100 约一周、8×H100 集群数天内即可跑完 Phase A+B）。这使得在不依赖超大规模模型的前提下，初步探索"底层架构调整是否值得"成为可能。

## 预期产出

### 核心观察点 - 领域的 Cross-over 曲线

构建 X 轴为"多轮持续微调的 Token 注入量"，Y 轴为"Domain X 表现"的折线图。如果 Cross-over 假设成立，预期会观察到：在图表左侧（起始阶段），`Model-CL` 表现低于 `Model-Dense`；在后续阶段，`Model-CL` 可能与 `Model-Dense` 出现交叉，并在部分设定下达到更高峰值。当然，也完全有可能观察不到这个交叉，这本身也是有价值的结论。

### 辅助观察点 - 通用底线维持

在通用智能底线测试集上，观察两者遗忘速率的差异，评估 `Model-CL` 是否更接近"有选择地遗忘非必要事实"而非底层语言能力显著退化。
