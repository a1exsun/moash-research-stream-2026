# Proposal 02: 基于 nanoGPT 的持续学习 Cross-over 效应检验

## 核心定位与动机

传统的持续学习研究执着于试图延缓灾难性遗忘，让模型成为不遗忘任何旧知识的“全知全能”者。但根据人脑学习的启发，人类在成长过程中也会遗忘大量早期或无用的事实知识，最终收敛为一个**领域专家**（同时保留基础的逻辑与通用语言能力）。

因此，本实验的核心主张是：**为了实现真正的持续学习架构，我们可能需要牺牲一部分 Transformer 架构（Dense）天然的强大信息压缩与初始泛化能力。**

只要这种新型的持续学习架构（例如原生稀疏架构）能够在长周期的目标领域持续学习后，能够在目标领域的纯粹能力上限超越同等规模的传统 Dense 模型，实现 **Cross-over（能力交叉）**，那么这种架构上针对“初始容量”的牺牲就是具备巨大价值的。

---

## 实验假设

### H1 (初始能力差异)

在相同的预训练算力与数据下，传统的 Dense Transformer 由于其全局注意力和密集的反向传播，会展现出比“具备独立隔离机制的新型持续学习架构（如原生稀疏）”更强的初始通识能力和更低的 Perplexity。

### H2 (Cross-over 交叉效应)

在进入特定目标领域的长序列流式持续微调后：

- **Dense 模型**：受限于物理层面的权重交叉继承，很快会遭遇领域知识吸收的瓶颈。由于在强行洗掉旧有特征，表现为目标领域的性能提升停滞，甚至因为过度微调导致基础通用能力崩溃。
- **持续学习架构模型 (Sparse 等)**：虽然起点较低，但得益于其架构对灾难性遗忘的相互覆写防御，能够以更高的效率持续且稳定地吸收极大量的目标领域新知识。最终在微调序列的某个节点，其在目标领域的表现将稳步超越 Dense 模型，形成明确的 Cross-over 性能交叉曲线。

---

## 实验设计

### 1. 基础配置

- **基座规模**：nanoGPT (约 124M 参数)，以保证验证此类底层预训练假设的高效可行性。
- **评测分离体系**：
  - **通用智能底线集 (General Intelligence Baseline)**：仅测试最基础的语言流利度、基础逻辑推断（允许遗忘具体百科事实知识，但不允许丧失底层的“思考流利度”）。
  - **目标深水区领域集 (Target Domain)**：例如深度的医疗长文本、复杂的代码库或特定企业的私有知识域。

### 2. 实验阶段划分

**Phase A: 预训练阶段 (Pre-training)**
分别使用相同的通用语料（如 FineWeb-Edu 的一个子集）从头预训练两个架构：

1. `Model-Dense`：标准结构 nanoGPT。
2. `Model-CL`：带有持续学习特殊机制的 nanoGPT（如引入 Proposal 01 中的结构化稀疏设计来限制交叉继承）。
   _设定检查点：验证在通识测试集上，`Model-Dense` 的表现确有初始压制优势。_

**Phase B: 持续域演化阶段 (Continual Domain Specialization)**
将两个预训练好的模型同时投入目标领域（Domain X）进行无尽的流式长序列微调。

- 每注入一定数据量（例如每 10M Tokens）作为一个 Epoch 节点，同时抓取两个模型进行 Domain X 专业测试与通用底线集测试。
- 绘制两条随训练时间/数据量推移的能力演进对比曲线。

---

## 预期观察与验收指标

1. **核心观察点 - 领域的 Cross-over 曲线**：构建 X 轴为“持续微调 Token 注入量”，Y 轴为“Domain X 表现”的折线图。我们期待证实在 X 轴的右侧深水层（长序列末端），`Model-CL` 在领域专精能力上成功上穿跨越 `Model-Dense`。
2. **辅助观察点 - 通用底线维持 (Baseline Defense)**：在通用智能底线测试集上，观察两者遗忘速率的差异，证明 `Model-CL` 是“健康地遗忘非必要事实”，而不是像 Dense 那样最终陷入“灾难性的底层语言架构崩塌”。

---

## 算力成本与可行性论证

利用 nanoGPT 的极小规模（124M），跑通这套包含“预训练 + 无尽微调”的全链路端到端实验，整体算力被限制在极小范围内（单卡 RTX 4090 或 A100 几天内即可跑完 Phase A+B）。这使得我们可以跳出在庞大 SOTA 模型上只能做“表面修补”的困境，直接且彻底地回答“底层架构重置是否有意义”这一根本性哲学问题。
