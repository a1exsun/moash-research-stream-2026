# Proposal 04: 探索 SOTA 模型隐式稀疏性与面向持续学习的微调架构改造

## 核心洞察

根据我们在主提案文档中提出的一项核心洞察：**也许模型规模（Scale）本身就是解决持续学习干扰的核心要素？**

在 Scaling Law 不断扩展的前提下，极其庞大的 SOTA 级大语言模型内部可能会自然涌现出"隐式的稀疏性（Implicit Sparsity）"。这意味着在极大参数量所构成的高维语义空间中，处理完全不同技能和不同领域的知识时，模型内部激活的通路也许天然具备强烈的正交特征，使得它们原本就不太容易发生相互干扰覆写。

如果这一猜想成立，面对 SOTA 级别模型时，最好的策略就不是从头验证微小机制或者将其极其昂贵地转化为物理结构上的稀疏（如重构为 MoE）；而是采取更加工程化的破局方式：**直接设计一种专门面向持续学习的微调架构改造，在注入和更新新知识的过程中，既解决实际遗忘问题，又反向验证并利用其真实的隐式稀疏性。**

## 假设

### H1 (规模涌现的隐式稀疏性)

随着模型特征维度的急剧增大，Dense 模型内部处理不同任务（如编程域与医疗域）的神经元激活群以及梯度分布呈现出天然的低重叠率与近正交趋向。即使底座在物理架构上是 Dense 的，在特征路线上却是逻辑 Sparse 的。

### H2 (顺应稀疏性的微调架构改造)

在无需破坏其 Dense 架构根基的前提下，仅仅通过在微调阶段引入一种**"稀疏感知/正交保护"的架构改造层（如基于激活稀疏路由的 PEFT 或正交子空间参数低秩更新）**，即可高效隔离微调时梯度的干扰。依靠放大大模型的天然局部稀疏特征，我们可以让原本会引发灾难性遗忘的连续域微调，平滑转化成为知识互不干扰的模块化吸收。

## 方案

### 基础配置

- **模型基座选择**：选取具备"涌现能力级"体量且架构稳定前沿的开源 SOTA 模型（例如 Llama-3-8B 及以上，或开源的 Qwen/Mistral 规模）。
- **任务序列流**：多领域序列设定（代码 -> 数学 -> 医疗 -> 日常对话逻辑）。

### 微调架构改造方案设计

放弃传统简单粗暴的全参微调（Full Fine-Tuning）与不设防的常规 LoRA。设计一种探索"隐式任务边界"的微调模块：

- **方案 A - 激活稀疏感知更新 (Activation-mask Tuning)**：冻结模型主体。通过对新任务样本前向传播时的神经元进行探测，仅解冻或仅建立针对当前任务"高频高光激活"通路的旁支更新（通过稀疏掩码阻断低频无关神经元被微调梯度"污染"）。
- **方案 B - 隐式正交空间映射**：在引入 Adapter/LoRA 时，计算新领域参数增量相对于旧领域参数的历史梯度的正交方向，强制在该任务"专属空闲的稀疏维度"内生长新的表示。

### 可视化与探测验证 (Probing)

除了常规评估各种领域的任务得分外，必须设计专门的探针来验证"隐式稀疏性"是否真实存在：

- **高能激活交集计算 (Activation Overlap)**：统计两项完全不同的任务经过不同层时，Top-K 激活神经元群落的交并比（IoU）。
- **参数扰动追踪**：对比该专项微调架构开启与不开启时，模型在吸收目标新知识后，其底层核心通识隐向量在关键子空间的漂移距离（Subspace Representation Drift）。

## 算力成本评估

基座为 8B+ 参数的 SOTA 开源模型，微调阶段使用 PEFT（LoRA/Adapter），推理时需前向传播进行激活探测。

- 激活探测阶段（per domain）：单次前向 pass ~2-4 A100 GPU 小时
- 微调阶段（per domain）：LoRA 微调 ~8-16 A100 GPU 小时
- 4 个域完整实验 + 探针分析：~80-120 A100 GPU 小时
- 云端成本：~$200-300

## 预期产出

1. **"隐密分布"实体证据**：通过神经元和子空间的探针分析报告，首次清楚解答在 SOTA 级别大模型内部，"通用底线智能"和"特定领域专家技能"究竟是如何交织（或者是如何被分隔开）的。
2. **新型 SOTA 持续学习解决方案**：提出一套可以直接挂载运行在新一代开源大语言模型上的"抗遗忘微调架构"（或者某种插件方案的算法理论依据），真正意义上让工业界以低成本、极小的微调侵入性来实现领域专家的持续进化。
