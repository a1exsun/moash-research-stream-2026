# From NSA to Continual Learning: The Research Trajectory of Natively Trainable Sparse Attention and Unfilled Gaps

> **Discussion Date**: February 25, 2025  
> **Background**: An in-depth discussion based on the DeepSeek-AI paper _"Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention"_ (arXiv:2502.11089, ACL 2025 Best Paper), extending to subsequent research progress and cross-disciplinary gaps with continual learning.

---

## 1. Core Content of the NSA Paper

### 1.1 The Problem Addressed

In long-context modeling, the computational overhead of standard softmax attention grows quadratically with sequence length. At a 64k length, attention computation accounts for 70–80% of the total inference latency. Existing sparse attention methods suffer from two major flaws:

- **The "Illusion" of Inference Acceleration**: Theoretical reductions in computation fail to translate into actual latency decreases. Reasons include stage-restricted sparsity (e.g., H2O only accelerates decoding, MInference only accelerates prefill) and incompatibility with advanced architectures like GQA/MQA (e.g., Quest's independent head-wise selection still results in high KV-cache memory access).
- **The "Myth" of Trainable Sparsity**: Most methods solely impose sparsity during inference, not during training. Post-training pruning forces the model to deviate from its pre-training optimization trajectory—the paper points out that the top 20% of attention in pretrained models covers only 70% of the total attention score. Furthermore, operations like ClusterKV's k-means clustering or MagicPIG's SimHash are non-differentiable, blocking gradient flow; token-level selection methods like HashAttention lead to non-contiguous memory access, preventing the utilization of highly efficient implementations like FlashAttention.

### 1.2 NSA's Three-Branch Architecture

NSA decomposes each query's attention computation into three parallel pathways, which are then weighted and fused via a learnable gating mechanism (MLP + sigmoid):

** (1) Compression Attention**

- Aggregates contiguous token chunks into a single compressed key/value format through a learnable MLP (with intra-chunk positional encoding).
- Parameters: chunk length $l=32$, sliding stride $d=16$ ($d < l$ to reduce information fragmentation).
- Captures coarse-grained global semantic information, lowering the computational burden.

** (2) Selection Attention**

- **Block-level Selection**: Divides the KV sequence into spatially contiguous chunks (block size $l'=64$) rather than performing token-by-token selection—this both aligns with the contiguous memory access patterns of GPUs to maximize Tensor Core utilization, and matches the empirical observation that attention scores naturally exhibit spatial block-level clustering.
- **Zero-overhead Importance Scoring**: Directly reuses the intermediate softmax scores from the compression attention as block importance scores, requiring no additional computation.
- **GQA Compatibility**: All query heads within the same GQA group aggregate importance scores and then uniformly select blocks, ensuring consistent memory access when sharing the KV-cache.
- **Top-n Selection**: Retains the top $n=16$ blocks with the highest importance scores (including 1 initial block and 2 local blocks), engaging the original fine-grained tokens within the selected blocks in the attention computation.
- Preserves precise fine-grained information.

** (3) Sliding Window Attention**

- Maintains the local context of the most recent $w=512$ tokens.
- Processed as an independent branch to prevent local pattern "shortcuts" from impacting the learning of the compression and selection branches.
- The three branches use independent keys/values to prevent gradient interference.

### 1.3 Hardware-Aligned Kernel Design

The core optimization lies in altering the query grouping strategy:

- **Does NOT** sequentially load contiguous query blocks like FlashAttention does (because queries within the same block might require disjoint KV blocks).
- **Instead**, for each sequence location, loads all query heads within the same GQA group into SRAM, sharing the sparse KV block indices.
- The outer loop is placed on Triton's grid scheduler (since different query positions select roughly the same number of blocks), while the inner loop sequentially loads contiguous KV blocks.
- Achieves near-optimal arithmetic intensity: eliminates redundant KV transfers + balances the computational load across GPU stream multiprocessors.

### 1.4 Experimental Results (27B Parameter Model, 270B Tokens Pre-trained)

| Dimension                   | Result                                                                                                                                          |
| --------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- |
| **General Benchmarks**      | Surpassed Full Attention on 7 out of 9 tasks; average score 0.456 vs 0.443. Significant gains in reasoning tasks (DROP: +0.042, GSM8K: +0.034). |
| **Long Context**            | 64k needle-in-a-haystack perfect accuracy; LongBench average score 0.469 (Full Attention 0.437, Exact-Top 0.423).                               |
| **CoT Reasoning (AIME 24)** | 8k context: NSA-R 0.121 vs Full Attention-R 0.046; 16k context: 0.146 vs 0.092.                                                                 |
| **Training Acceleration**   | 64k sequence: Forward pass 9.0×, Backward pass 6.0× (Triton implementation vs Triton FlashAttention-2).                                         |
| **Decoding Acceleration**   | 64k sequence: 11.6× (memory access reduced from 65536 tokens to 5632 tokens).                                                                   |

### 1.5 Key Insights

- **Attention scores naturally cluster in blocks**: Visualizing the attention maps of Full Attention models reveals that adjacent keys tend to have similar attention scores, providing empirical grounding for block-level sparse selection.
- **Introducing sparsity during pre-training forces models to learn superior attention allocation**: Sparse constraints compel models to concentrate critical information in a few key locations, akin to an information bottleneck effect, potentially enhancing performance by filtering out noise from irrelevant attention channels.
- **Failures of alternative solutions**: Key-Clustering methods (expensive dynamic clustering, load imbalances in MoE systems), auxiliary loss-driven block selection (extra overhead and performance drops), and heuristic parameter-free selection (low recall) all underperformed NSA.

---

## 2. Discerning the Essence of NSA's Sparsity

### 2.1 Not Weight Sparsity

NSA's sparsity **does not manifest at the weight level**—the Q/K/V projection matrices, FFN layers, and MoE expert parameter matrices remain completely dense.

### 2.2 Dynamic Structured Sparsity in Attention Computation

The sparsity occurs during the computation of the attention score: each query no longer interacts with all preceding keys, but rather interacts exclusively with a meticulously curated subset:

- Compressed coarse-grained tokens (approximately $\lfloor(t-l)/d\rfloor$)
- Selected top-n fine-grained token blocks ($n \times l'$ tokens)
- Local tokens within the sliding window ($w$)

For a 64k sequence under the paper's configuration, the tokens actually computing attention with each query number only in the low thousands (vastly less than 65536), achieving a high sparsity ratio of $N_t \ll t$.
Key characteristics: **Dynamic, query-dependent, block-level structured sparsity**—each query dynamically decides which KV blocks to focus on based on its own content (via the intermediate compression attention scores), with no fixed or predefined patterns.

---

## 3. Human Brain Analogy

NSA's three pathways map intuitively to human cognitive levels:

| NSA Pathway               | Brain Analogy                          | Explanation                                                                                                                           |
| ------------------------- | -------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |
| **Compression Attention** | Gist/Semantic Memory                   | Forming a chapter-level "summary" after reading a book without remembering every word; recalling requires checking the summary first. |
| **Selection Attention**   | Selective Attention/Episodic Retrieval | Retrieving structurally relevant, precise memory snippets based on the query; skipping directly to a specific scene.                  |
| **Sliding Window**        | Working Memory/Short-Term Memory       | Knowing exactly what was said in the last few sentences of a dialogue; directly usable without deep retrieval.                        |
| **Gating Mechanism**      | Metacognitive Regulation               | Dynamically shifting reliance based on the task (e.g., relying on working memory for math, semantic memory for summaries).            |

The most interesting commonality is **block-level clustering**: The paper observes that adjacent tokens are attended to together. This mirrors human **episodic memory**—we retrieve coherent "scene blocks" rather than isolated words.

---

## 4. Preliminary Exploration of the Nexus Between NSA and Continual Learning

### 4.1 NSA Does Not Directly Solve Continual Learning

The root causes of catastrophic forgetting and loss of plasticity lie at the **parameter update level**: gradients from new tasks overwrite the weights learned by old tasks. NSA changes "which tokens compute attention," not "how parameters update." Even if attention is sparse, FFN/MoE parameters are still globally updated during fine-tuning, so forgetting still occurs.

### 4.2 Indirect Favorable Conditions

- **Functional Differentiation Potential**: The three paths naturally divide labor. Different knowledge might encode into different paths. New tasks might only impact one path, causing less interference—similar to the **modular network** ideology.
- **Sparse Activation Reduces Interference**: Each query activates fewer KV blocks, reducing the overlap in active computational paths—similar to why MoE architecture excels in continual learning (reducing parameter competition).
- **Extended Effective Context**: Large context windows serve as a proxy for parameter-level continual learning by accommodating more explicit memory.

### 4.3 The True Challenge

Continual learning's friction remains in the weights. **Injecting sparsity into the parameter activations and updates directly**—rather than just the attention computation—is the required step forward.

---

## 5. NSA Built for Training, Not Just Inference

This is the essence of "Natively Trainable."

### 5.1 The Problem with Existing Methods

Pruning Full Attention models only during inference forces the model to lose information. Pretrained models contain broad attention scores that disappear via hard pruning limits, inducing architectural bias.

### 5.2 NSA's Approach

Using sparse constraints from the very first step of pre-training forces the model to inherently route critical information into tight structural matrices. The attention patterns natively synergize with the FFN/MoEs, carrying sparsity completely spanning Pre-training → Fine-tuning → Prefill → Decoding.

---

## 6. Post-NSA Research Trajectories (February 2025 to Present)

### 6.1 DeepSeek's Own Evolution: DSA (DeepSeek Sparse Attention)

- **Paper**: DeepSeek-V3.2 (arXiv:2512.02556, December 2025)
- **Core**: DSA utilizes a lightning indexer + fine-grained token selection, achieving point token-level sparsity (rather than NSA's block-level).
- **MLA Integration**: Instantiated atop MLA (Multi-head Latent Attention).
- **Deployment**: Integrated completely in DeepSeek-V3.2 (685B parameters). NSA → DSA marks the transition from academic prototype to production architecture.

### 6.2 MoBA (Mixture of Block Attention) — Moonshot AI/Kimi

- **Paper**: arXiv:2502.13189, NeurIPS 2025 Spotlight
- **Core Idea**: Applies MoE principles directly to attention—routing tokens to the top-k most relevant KV blocks via a learned router matrix.
- **Significance**: Simpler than NSA; actively deployed inside Kimi's long-context frameworks.

### 6.3 FlashMoBA / Optimizing MoBA

- **Paper**: arXiv:2511.11571, November 2025
- **Focus**: Improving MoBA's block tracking with short-convolution key clustering. Provided the FlashMoBA CUDA kernel, significantly accelerating beyond FlashAttention-2 metrics natively.

### 6.4 ASA (Alternating Sparse Attention)

- **Paper**: arXiv:2511.00819, November 2025
- **Core Update**: Alternating completely local (sliding window) layers with global (compression+selection) layers strictly inter-layer natively, rather than universally applying three static branches per layer. Provides vastly superior long-range tracking.

### 6.5 SSA (Sparse Sparse Attention)

- **Paper**: arXiv:2511.20102, November 2025
- **Gradient Deficit identified**: Sparse-Sparse models demonstrate less sparsity post-training due to gradient tracking bypassing excluded KV pairs natively. SSA resolves this explicitly by alternating full attention streams against sparse variants organically.

### 6.6 FSA (Flash Sparse Attention)

- **Paper**: arXiv:2508.18224, August 2025
- **Focus**: Inverting the query vs KV scanning loop architecture natively inside the kernel to drastically reduce latency in architectures with narrow head clusters.

### 6.7 VMoBA (Video MoBA)

- **Paper**: arXiv:2506.23858, June 2025
- **Focus**: Extending natively sparse spatial tracking frameworks into multidimensional Video Diffusion Models seamlessly.

### 6.8 Additional Peripheral Prototypes

- **TabNSA, SeerAttention-R, fla-org/native-sparse-attention** respectively mapped explicit functionality domains explicitly defining novel operational bounds.

### 6.9 Research Trend Summary

| Direction                 | Representative Work | Core Contribution                                                   |
| ------------------------- | ------------------- | ------------------------------------------------------------------- |
| Production Integration    | DSA (DeepSeek-V3.2) | Natively distinct token hierarchical extraction                     |
| Architectural Refinement  | MoBA (Kimi)         | Unified MoE principles mapped to KV blocks                          |
| Kernel Dynamics           | FSA, FlashMoBA      | Efficient CUDA implementation across distinct hardware abstractions |
| Training Paradigms        | SSA                 | Solving gradient deficit paradox inherently                         |
| Architectural Alterations | ASA                 | Inter-layer alternating paths                                       |
| Cross-modality            | VMoBA               | Video diffusion expansion cleanly                                   |

---

## 7. Confirmed Research Gap: Native Sparse Attention × Continual Learning

### 7.1 Confirming the Gap

Systematic retrieval (as of February 25, 2025) confirms **no published or pre-print work** combines the NSA/DSA/SSA/ASA/MoBA generation of natively trainable sparse attention with continual learning or continual fine-tuning.

The two research communities are completely isolated:

- **Sparse Attention Community**: Focuses entirely on single-training efficiency and performance. Zero papers discuss sequential task adaptation or forgetting.
- **Continual Learning Community**: Remains focused on traditional regularizations, replay buffers, and parameter-level projections. Sebastian Raschka explicitly noted in his 2025 end-of-year review that "Continual Learning has yet to see significant breakthrough."

### 7.2 The Closest Works

**(1) Google Research: Nested Learning (February 2026)**
Treats varying transformer components as nested optimization problems with varying update rates. No NSA incorporation.

**(2) Meta: Sparse Memory Finetuning (October 2025)**
Proposes updating only an infinitesimal fraction (0.01%) of parameters. Sparsity is parameter-level, not attention computation-level.

**(3) EMNLP 2025: Task Vector Pruning for Catastrophic Forgetting**
Applies post-training pruning. Distinct from training-time attention sparsity.

**(4) Mechanistic Analysis of CF in LLMs (January 2026)**
Theoretically mentions that "sparse transformers may demonstrate variable forgetting mechanics" purely as future work.

### 7.3 Why This is a Valuable Research Gap

**(1) SSA's "Gradient Deficit" Paradox is the Continual Learning Mechanic**
What SSA considered a deficit (missing gradients for excluded KV pairs) is exactly what prevents forgetting: excluding specific paths from updating explicitly protects knowledge. EWC and PackNet try to do this manually.

**(2) ASA's Alternating Layers Hint at Functional Isolation**
Different layers process varying granularities cleanly. Focusing fine-tuning only on specific functional layers allows task separation linearly.

**(3) DSA's Lightning Indexer is a Memory Retrieval Module**
Linking the indexer to task-specific routing arrays naturally enables task switching without drastically updating main weights natively.

**(4) MoBA's MoE-style Routing naturally supports task shunting**
MoE structures inherently segregate sequential learning gracefully.

**(5) Complete Proposed Combination Matrix:**

- **Attention Sparsity** (NSA/DSA): Reductions in sequence forward pass overlapping paths.
- **Parameter Sparsity** (MoE): Limiting parameter matrix impact.
- **Gradient Stasis** (EWC/Fisher metrics): Freezing old task variables linearly.
- **Contextual Matrices Retrieval** (DSA indexing): Task-aware exact KV memory bounds uniquely.

---

## 8. Summary

This trajectory confirms that natively trainable sparse attention is fully mature at a production level (DSA, MoBA) but remains completely unexplored within the Continual Learning sector. This definitively illuminates an untouched, explicitly critical nexus between two highly advanced research vectors uniquely awaiting cross-pollination.

---

## References

- Yuan et al. (2025). Native Sparse Attention... ACL 2025. arXiv:2502.11089
- DeepSeek-AI (2025). DeepSeek-V3.2... arXiv:2512.02556
- Lu et al. (2025). MoBA... NeurIPS 2025. arXiv:2502.13189
- Xiao et al. (2025). Optimizing MoBA. arXiv:2511.11571
- Hu et al. (2025). ASA. arXiv:2511.00819
- Shen et al. (2025). SSA. arXiv:2511.20102
- Yan et al. (2025). FSA. arXiv:2508.18224
- VMoBA (2025). arXiv:2506.23858
- Van de Ven et al. (2024). Continual Learning... arXiv:2403.05175
- Shi et al. (2024). Continual Learning of LLMs... arXiv:2404.16789
- Behrouz et al. (2026). Nested Learning... Google Research Blog.
- Berges et al. (2025). Continual Learning via Sparse Memory Finetuning. arXiv:2510.15103
- Imanov (2026). Mechanistic Analysis of CF in LLMs... arXiv:2601.18699
- Raschka (2025). The State of LLMs 2025.
