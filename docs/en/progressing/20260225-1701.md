# Landscape of Open-Source Sparse Architecture Models (As of February 2026)

## Overview

"Sparsity" in Large Language Models has two levels:

1. **MoE Sparsity** (FFN Layer): Mixture-of-Experts, where only a portion of the expert networks are activated during each inference.
2. **Attention Sparsity** (Attention Layer): Native sparse attention mechanisms, where the attention computation itself is reduced from O(L²) to sub-quadratic complexity.

This article focuses on **architectural-level full-pipeline native sparsity**—not just MoE, but models and technologies that introduce native sparsity precisely in their attention computations, such as NSA and DSA.

---

## DeepSeek V3.2

### Basic Information

| Attribute         | Specification |
| ----------------- | ------------- |
| Total Parameters  | 685B          |
| Active Parameters | ~37B (MoE)    |
| Context Length    | 128K          |
| License           | MIT           |
| Release Date      | December 2025 |

### Variants

- **DeepSeek-V3.2**: Flagship version, supports inference + tool-use + conversational chat.
- **DeepSeek-V3.2-Speciale**: An experimental version focused strictly on deep reasoning; does not support tool-use, achieved gold medal performances in IMO and IOI 2025.

Officially, DeepSeek V3.2 only comes in the 685B parameter scale. The 7B/14B/32B versions mentioned in the community are actually distilled models inherited from DeepSeek R1 (based on Qwen2.5 and Llama3 distillation) and are not official variants of V3.2.

### Hardware Requirements

| Precision | VRAM Requirement | Hardware Solution        |
| --------- | ---------------- | ------------------------ |
| FP16/BF16 | ~1.3TB+          | 8+ H200 (141GB) + NVLink |
| FP8       | ~690GB           | 5-8 H100/H200            |
| INT4      | ~350-400GB       | 4-5 H100 (80GB)          |

Full deployment requires roughly 8 H200 GPUs, with hardware costs around $200K. For scenarios processing under 50 million tokens per month, API access is vastly more cost-effective than self-hosted hardware. Because the MoE architecture only activates 37B parameters at a time, there are community solutions utilizing CPU offloading to run it on consumer-grade hardware, though the inference speed lacks practical utility.

### Core Sparsity Technology: DSA (DeepSeek Sparse Attention)

DSA is the sole architectural modification implemented in DeepSeek V3.2 compared to V3.1. Its core mechanisms include:

- **Lightning Indexer**: An ultra-lightweight FP8 scorer that identifies the most relevant KV entries for each query token.
- **Top-k Token Selection**: Calculates attention exclusively for the highest-scoring KV entries.
- Instantiated atop MLA (Multi-head Latent Attention), reducing complexity from O(L²) down to O(Lk).

DSA is a direct evolution of NSA (Native Sparse Attention), which won the ACL 2025 Best Paper award.

---

## Open-Source Models with Native Sparse Attention

### DeepSeek V3.2 — DSA

As detailed above. DSA drops the attention complexity from O(L²) to O(Lk), achieving an approximately 50% reduction in computational load during 128K long-context inference.

### Qwen3-Next / Qwen3.5 — Gated DeltaNet Hybrid Attention

| Attribute              | Qwen3-Next                             | Qwen3.5      |
| ---------------------- | -------------------------------------- | ------------ |
| Total Parameters       | 80B-A3B                                | 397B-A17B    |
| Attention Architecture | Gated DeltaNet + Gated Attention (3:1) | Same as left |
| Context                | 262K (Native)                          | 1M (API)     |
| License                | Apache 2.0                             | Apache 2.0   |

The 3:1 hybrid layout means that the majority of layers utilize Gated DeltaNet (linear attention, O(L) complexity), while the remaining layers use Gated Attention (full attention). This is not an approximate optimization applied only during inference; it is a sub-quadratic attention **natively utilized during training**. The DeltaNet layers do not grow the KV cache; instead, they maintain a fixed-size recurrent state, keeping memory consumption constant regardless of context length.

### Kimi K2.5 / Kimi Linear — KDA + MLA

| Attribute              | Specification                         |
| ---------------------- | ------------------------------------- |
| Total Parameters       | 1T+                                   |
| Active Parameters      | ~32B                                  |
| Attention Architecture | KDA (channel-wise gating) + MLA (3:1) |
| License                | MIT                                   |

It adopts a similar 3:1 hybrid ratio as Qwen3.5, but differing in implementation: Kimi employs channel-wise gating (KDA) as a replacement for Qwen's scalar gating, and substitutes the Full Attention layers with DeepSeek's Multi-Head Latent Attention (MLA).

### MiniMax-M1 / M2.5 — Lightning Attention

| Attribute              | M1                               | M2.5                |
| ---------------------- | -------------------------------- | ------------------- |
| Total Parameters       | 456B                             | ~230B               |
| Active Parameters      | 45.9B                            | ~10B                |
| Attention Architecture | Lightning Attention (7:1 Hybrid) | Lightning Attention |
| Context                | 1M                               | 205K                |

MiniMax-M1 uses a hybrid block stack: for every 7 Lightning Attention blocks, 1 standard Softmax Attention block is interspersed. Lightning Attention operates at linear complexity, requiring only about 25-30% of the computational load of DeepSeek-R1 when generating 100K tokens.

A notable development: MiniMax **abandoned linear attention** in its M2 version, reverting completely to full attention, with the team explaining that linear attention struggled with accuracy during inference and multi-turn tasks. However, Lightning Attention was reintroduced in M2.5.

### GLM-5 — DSA + MLA

| Attribute              | Specification                                                   |
| ---------------------- | --------------------------------------------------------------- |
| Attention Architecture | Traditional Transformer + DeepSeek Sparse Attention (DSA) + MLA |
| Release                | February 2026                                                   |

GLM-5 retains the traditional Transformer structure but incorporates DeepSeek Sparse Attention (DSA) to achieve token-level sparsity, while also actively utilizing MLA. This serves as a quintessential case study of the outward diffusion of DeepSeek's technological influence.

---

## Research-Level Sparse Attention Methods

These represent open-source academic methodologies and codes, not yet tied to specific massive open-weights production models.

### NSA (Native Sparse Attention)

- **Source**: DeepSeek + Peking University, ACL 2025 Best Paper
- **Mechanism**: Dynamic hierarchical sparse strategy featuring three parallel attention pathways—coarse-grained token compression, fine-grained token selection, and sliding-window local context.
- **Features**: Hardware-aligned optimization; supports end-to-end training (not an inference-time approximation).
- **Significance**: The academic foundation of DSA. Achieved massive acceleration across all stages (decoding, forward, backward) on 64K long sequences.

### MoBA (Mixture of Block Attention)

- Block-level sparse attention, natively employed during training.
- Falls under the Sparse-Sparse paradigm alongside NSA.

### SSA (Sparse Sparse Attention)

- Trains native sparse models by aligning the outputs of full attention and sparse attention.
- Uncovered a crucial paradox: Sparse-Sparse models designed explicitly for sparsity exhibit lower attention sparsity than their Full Attention counterparts.
- Proposes a duel-stream training framework sequentially alternating between a sparse attention stream and a full attention stream.

### SpargeAttn / SageAttention (Inference Optimization, Non-native Training)

- Tsinghua University, ICLR 2025 / ICML 2025
- Training-free sparsity + quantized attention acceleration acting exclusively during inference.
- Plug-and-play adaptability for any given model, yielding 2.5x-5x acceleration.
- Note: This strictly falls under **inference-time optimization**, not a native sparse architecture incorporated during training.

---

## Open-Source Models with MoE Sparsity (FFN Layer Sparsity)

Although this article is heavily spotlighting attention sparsity, MoE serves as another dimension of sparsity and merits inclusion for comprehensive reference.

### Large-Scale Flagships

| Model                 | Total Parameters | Active Parameters | License         |
| --------------------- | ---------------- | ----------------- | --------------- |
| DeepSeek V3/V3.1/V3.2 | 685B             | 37B               | MIT             |
| Mistral Large 3       | 675B             | 41B               | Apache 2.0      |
| Qwen3-235B-A22B       | 235B             | 22B               | Apache 2.0      |
| Kimi K2               | 1T+              | 32B               | MIT             |
| DBRX                  | 132B             | 36B               | Databricks Open |
| Llama 4 Maverick      | 400B+            | MoE               | Llama License   |

### Mid-to-Small Scale (Consumer Hardware Runnable)

| Model                | Total Parameters | Active Parameters | Notes                         |
| -------------------- | ---------------- | ----------------- | ----------------------------- |
| Qwen3-30B-A3B        | 30B              | 3B                | Performance surpasses QwQ-32B |
| Qwen3-Coder-Next     | 80B              | 3B                | Specialized for programming   |
| Mixtral 8x7B / 8x22B | 47B/141B         | 13B/39B           | Mistral's classic MoE         |
| GLM-4.7-Flash        | 30B              | 3B                | Lightweight deployment        |
| NVIDIA Nemotron      | 30B              | 3.5B              | General purpose               |
| OLMoE                | -                | -                 | AI2 Research                  |

---

## Current Landscape and Trends

### Attention Mechanisms: The New Battleground

A year ago, the central debate was "MoE vs. Dense?"—that question is now decisively settled (MoE won). The overarching divergence currently centers entirely on the selection of attention mechanisms:

| Pathway                          | Representative Models | Method                                                 |
| -------------------------------- | --------------------- | ------------------------------------------------------ |
| Hybrid Linear/Full Attention     | Qwen3.5, Kimi K2.5    | Gated DeltaNet/KDA + Full Attention (3:1)              |
| Pure Linear Attention            | MiniMax M1/M2.5       | Lightning Attention                                    |
| Sparse Selection (Sub-Quadratic) | DeepSeek V3.2, GLM-5  | DSA/NSA                                                |
| Traditional Full Attention       | MiniMax M2            | Abandoned linear attention; reverted to full attention |

### DeepSeek's Technological Sway

DeepSeek's technological components are undergoing widespread adoption:

- **MLA** (Multi-head Latent Attention) → Adopted by Kimi K2.5 and GLM-5
- **DSA** (DeepSeek Sparse Attention) → Adopted by GLM-5
- **NSA** (Academic foundation) → ACL 2025 Best Paper, profoundly influencing the entire trajectory of sparse attention research.

### The Controversy of Linear Attention

MiniMax's trajectory illuminates the extreme challenges linear attention faces in production environments: M1 utilized Lightning Attention, M2 abandoned it to forcefully revert to full attention (citing severe accuracy degradation during inference and multi-turn tasks), yet M2.5 reintroduced it. This definitively demonstrates that a pronounced tension persists between the theoretical efficiency and the practical efficacy of linear/sparse attention.

### The Race for Lower Activation Ratios

Models are demonstrably becoming increasingly "sparse":

- Qwen3: 9.36% Activity (22B/235B)
- Qwen3.5: ~4.3% Activity (17B/397B)
- MiniMax M2.5: ~4.3% Activity (10B/230B)
- Kimi K2.5: Even lower activation ratios (32B/1T+)

Lower activation ratios translate directly to superior inference efficiency but concurrently impose significantly higher requirements on routing strategies and overall training stability.

---

_Date compiled: February 25, 2026_
