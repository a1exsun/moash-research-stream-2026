# 2026-02-25: Resolving Compute Bottlenecks in Validating DSA's Anti-Forgetting Mechanisms and Evolutionary Thoughts

## 1. Background and Problem Statement

Based on the core insights from [Proposal 05](../proposal/proposal_05_dsa_tiny_sandbox.md), we realized that the **Dynamic Sparse Attention (DSA)** mechanism introduced in DeepSeek V3 inherently maps to "compressed commonsense streams" and "focused fine-grained streams" at the base architectural level. This asymmetric decoupled structure—much like the separation between the neocortex and hippocampus in the human brain—if leveraged during Continual Fine-Tuning (by exclusively updating the focused stream while freezing the compressed stream), could potentially eradicate the catastrophic forgetting encountered by large models when acquiring new knowledge.

**The Challenge**: Open-source models equipped with native DSA architectures (e.g., DeepSeek V3/R1 and naturally V3.2) have parameter counts ranging from tens to hundreds of billions. Even if we strictly apply asymmetric fine-tuning to a fraction of the weights, the VRAM and computational costs would far exceed the daily experimental budgets of most academic labs.

## 2. First Exploration: Missteps with Miniature Open-Source MoE Models

To circumvent the compute budget constraints, the initial thought was to find highly compact open-source sparse alternatives on HuggingFace as surrogate foundation models. Due to the generalization of the "sparse computing" concept, initial searches yielded scaled-down Mixture of Experts (MoE) models such as `OLMoE-1B-7B` or `Qwen1.5-MoE-A2.7B`.

**Blind Spot and Course Correction**:
We quickly realized that the low-parameter "sparse models" promoted by the open-source community **predominantly implement sparse routing in the Feed-Forward Network (FFN / MLP) layers (Sparse FFN)**. Conversely, the breakthrough for addressing catastrophic forgetting outlined in Proposal 05 hinges on decoupling the Query-Key representation matrices within extended contexts, which translates to **sparsely partitioning the Attention layer inherently**. Therefore, a standard miniature MoE model cannot validate the benefits brought about by attention topological isolation.

To date, the open-source community has not released any extreme miniature language models (in the 1B–3B parameter range) natively equipped with DeepSeek's DSA weights (Indexer + Top-K gated attention algorithms). This forced us to find an alternative approach for mechanistic validation.

## 3. Second Deduction: Dimensionality Reduction and First-Principles Mechanism Validation

In standard top-tier conference experimental settings, verifying the hypothesis that "a physically isolated attention structure prevents forgetting" does not necessarily require harnessing an immaculate, production-grade large model with omniscient world knowledge. We pivoted to designing two validation branches with exceptionally low computational costs:

### Route A: Post-Hoc Surgery on Dense Sub-1.5B Models

Since native miniature DSAs are inaccessible, an alternative is to hijack a mature open-source small dense model (such as `Qwen2.5-1.5B` or `Llama-3.2-1B`) and forcibly perform a 1:1 split rewrite on its Attention Heads during the PyTorch forward pass.

- **Simulating the Compressed Stream**: Completely suspend the first half of the Attention heads and underlying MLPs by setting `requires_grad=False` in the backward pass graph, effectively turning them into a static dictionary of commonsense.
- **Simulating the Sparse Focused Stream**: Forcibly impose Top-K Masking on the second half of the Attention heads (converting their attention into hard sparse pathways), and allow or even amplify the update rates of these specific parameters.
- **Expectation**: Via this highly cost-effective code-level surgery, we preserve the robust natural language commonsense of the underlying dense architecture while artificially erecting a DSA-like immune isolation barrier. This tests whether this "acquired asymmetric attention fine-tuning" can resist catastrophic forgetting in TRPG long-context roleplay setups.

_(This route has been formalized as: [Proposal 06](../proposal/proposal_06_dsa_attention_surgery.md))_

### Route B: Tiny-DSA Sandbox Pre-training

Considering that conference reviewers might argue that "forcibly surgically slicing the attention network of a dense large model will severely distort its pre-trained micro-structural knowledge," the most "unadulterated" and persuasive approach is to revert to an extremely rudimentary parameter scale (~100M, or roughly 0.1B).

- **Structure**: Directly extract the pure DSA Indexer and Top-K operator compute units from the DeepSeek open-source repositories and construct a miniature backbone consisting of merely 4 to 8 layers.
- **Testbed**: Pre-train a tiny foundational model mimicking infant-level storytelling comprehension from scratch on an ultra-minimal synthetic language dataset (such as `TinyStories`, where convergence can be achieved in a day or two on a single GPU). Utilizing this as a baseline, pit it against a conventional Dense Transformer of identical size on continual fine-tuning sets to witness which architecture maintains narrative coherence after enduring "a hundred script reversals."

_(This route has been formalized as: [Proposal 07](../proposal/proposal_07_dsa_tiny_sandbox.md))_

## 4. Conclusion and Next Steps

This line of reasoning successfully resolves the core dilemma of **"how to validate an architectural innovation that strictly requires a colossal, native math-heavy foundation, while incurring minimal hardware expenditure."** It downgrades the grand constraint of continual learning from a "compute supremacy monopoly" back to an "architectural math-isolation" experiment accessible to a solo researcher armed with an RTX 4090. This secures a concrete and highly actionable path for our subsequent academic validations.
