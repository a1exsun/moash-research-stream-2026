# Discussion Record on First Principles of Continual Learning (2026-02-24)

## Document Purpose

To record the in-depth discussion from first principles, focusing on the following core issues:

1. The three fundamental problems of continual learning (Allocation / Capacity / Recall)
2. The fundamental tension between compression and writability
3. Correct analogy of Complementary Learning Systems (CLS) — the hippocampus is not RAG
4. The inherent sparsity of the hippocampus — the critical distinction between activation sparsity and weight sparsity
5. Fundamental correction of research direction: from "whether sparsity is needed" to "what kind of representation separation"

This document does not discuss specific experimental proposals (P01-P04/P03-2) but records the evolution of underlying theoretical thinking.

---

## Background and Origin

Prior discussions established the following consensus (see `20260224-0007.md`):

1. The statement "sparse topology is a necessary condition for continual learning" is trivially true under a fixed architecture (since {dense, sparse} is an exhaustive partition, and the forgetting of dense models has been established).
2. Therefore, "necessity" itself is not a valuable research contribution.
3. The existing five proposals (P01-P04, P03-2) are considered insufficient, necessitating a rethink of the research direction from a more fundamental level.

The user proposed a key entry point: **Taking the continual learning dilemma of LLMs themselves as an example** — valuable content discussed in this conversation session will all be lost once the context window is reset. If sufficient research and computational resources were available, how could this conversation be stored into the model weights as a long-term memory?

This self-referential example drove all subsequent first-principles analysis.

---

## 1. Formulation of the Three Essential Problems

### 1.1 Starting from a Concrete Example

Taking an LLM (Claude) as the object of study. Let its weights be $\theta \in \mathbb{R}^d$, encoding all knowledge acquired during training. This conversation contains new information (e.g., a correction to a reasoning pattern). Continual learning = finding $\theta'$ such that the model retains old knowledge while integrating the new information.

The new information in the conversation is not a simple fact (like "the capital of France is Paris"), but a **meta-level correction of a reasoning pattern**:

> "When discussing whether X is a necessary condition, first check if the complement set has only one element; if so, and if that element has been negated, then necessity holds trivially."

This kind of knowledge is distributed across certain pathways in the attention circuits and MLP layers, not something a single isolated parameter can carry.

### 1.2 Definition of the Three Problems

Starting from the specific question of "how to write new information into weights," three essential sub-problems naturally emerge:

#### Problem 1: Allocation (Where to write)

**Which parameters need to be modified?**

In dense networks, functions are not modularly allocated. A reasoning pattern is scattered across thousands of parameters, highly overlapping with other patterns. Surgical updates are impossible — modifying any single parameter simultaneously affects multiple functions.

In the language of loss landscapes: Which parameter gradient directions do not conflict with old knowledge?
In the language of memory systems: Memory allocation.

Limitations of current methods:

- **Fisher Information methods** (EWC, Kirkpatrick et al., 2017): Estimating which parameters are "important" for old tasks and protecting them. However, Fisher is a diagonal approximation, ignoring parameter coupling.
- **Gradient Projection methods** (OGD, Farajtabar et al., 2020; GPM, Saha et al., 2021): Finding update directions orthogonal to old task gradients. However, as the number of tasks grows, the orthogonal space becomes saturated.
- **Activation-based methods**: Looking at which neurons are highly activated by old tasks. However, activation $\neq$ causal importance.

The fundamental limitation of these methods: They all perform localization in **parameter space**, but knowledge is not stored in units of parameters — it is stored in the **relationships between parameters** (circuits). Changing a parameter affects all circuits that include it, rather than an isolated "knowledge unit."

**Open question:** Can a representation be found such that the storage units of knowledge align with an operable substructure in parameter space? i.e., making "one piece of knowledge" correspond to "a set of independently modifiable parameters"?

#### Problem 2: Capacity (How much room is left)

**Is there enough "free capacity" in the parameter space to store new knowledge?**

The model's weights are already "filled" with training data. New information requires space. In dense networks, all capacity is in use, and new and old information compete for the same set of parameters.

In the language of loss landscapes: Does a $\theta'$ exist near $\theta$ that satisfies both new and old tasks?
In the language of memory systems: Memory capacity.

The Lottery Ticket Hypothesis (Frankle & Carlin, 2019) tells us that 90%+ of parameters in a trained dense network can be removed without affecting performance. However, this "redundancy" may not be genuinely idle — these parameters might be providing:

- Noise robustness
- Out-of-distribution generalization
- Encoding knowledge not yet measured by current benchmarks

**Open question:** In a trained network, how much capacity is **genuinely free** (can be written with new knowledge without affecting any existing functions)? How does this free capacity scale with model size, training data volume, and compression levels? A clear answer to this would directly determine "how much more a model can learn without forgetting" — the **capacity theory** of continual learning.

#### Problem 3: Recall (How to recall)

**Can the newly written knowledge be activated in the correct context?**

Even if new knowledge is successfully written into the weights, the next time a relevant situation is encountered, the model needs to:

1. Identify that the current context is relevant to the new knowledge (**pattern matching**)
2. Activate the new reasoning pattern instead of the old incorrect one (**conflict resolution**)
3. Perform the above operations without an explicit task ID (**implicit routing**)

In Transformers, routing is performed by attention — but the attention pattern itself can also be overwritten by new training. Thus, a **meta-stability problem** exists: the process of writing new knowledge might destroy the routing mechanism for retrieving old knowledge.

In the language of loss landscapes: Can $f_{\theta'}$ activate the correct knowledge for the correct input?
In the language of memory systems: Memory addressing.

**Open question:** Can a routing mechanism be designed such that writing new knowledge does not destroy existing retrieval paths? This routing mechanism itself needs to be robust to continual learning.

### 1.3 Relationship Between the Three Problems

The three problems are **not independent**; there is a fundamental tension between them (see next section). They also correspond directly to the basic problems of any memory system:

| Continual Learning Problem | Memory System Equivalent | Computer Memory         | Biological Memory                       |
| -------------------------- | ------------------------ | ----------------------- | --------------------------------------- |
| Allocation (Where)         | Allocation               | malloc/free             | Synaptic plasticity target selection    |
| Capacity (Room)            | Capacity                 | RAM size/Virtual memory | Neuron count/Synaptic density           |
| Recall (Recall)            | Addressing               | Pointers/Hash tables    | Pattern completion/Context reactivation |

---

## 2. Core Tension: The Conflict Between Compression and Writability

### 2.1 Superposition as the Root of the Problem

The power of LLM parameter space fundamentally stems from **superposition** — encoding far more than $d$ features with $d$ parameters. Multiple knowledge snippets share the same set of parameters, distinguished by different activation patterns.

While this is an efficient compression strategy, it **destroys writability**:

- **High Compression (Heavy Superposition)**: Each parameter participates in many functions → High capacity, but modifying any parameter affects many functions → Impossible to locate or safely write.
- **Low Compression (Light Superposition)**: Each parameter participates in few functions → Low capacity, but precise localization and modification possible → Safe writing, but limited storage.

### 2.2 Fundamental Contradiction at the Information-Theoretic Level

This is not an engineering tradeoff, but a fundamental contradiction at the information-theoretic level:

> **The compression efficiency of representation is adversarial to the feasibility of continual learning.**

In the language of rate-distortion theory: The higher the compression rate, the more distributed the representation, the larger the "blast radius" of a single parameter modification, and the more impossible surgical updates become.

This contradiction implies that it is impossible to simultaneously optimize "storage efficiency" and "updateability" within a single parameter space.

### 2.3 Connection to the Three Essential Problems

The compression-writability contradiction directly affects the three problems:

- **Allocation**: High compression → Distributed knowledge storage → Inability to locate independently modifiable parameter subsets.
- **Capacity**: Low compression → Wasted capacity → Limited total knowledge storage.
- **Recall**: High compression → Retrieval paths highly entangled with storage paths → Writing new knowledge may destroy routing for retrieving old knowledge.

---

## 3. Correct Analogy of CLS Theory — Hippocampus is not RAG

### 3.1 Initial (Incorrect) Analogy

Initially, the discussion proposed that the biological brain solves the compression-writability conflict using two different systems: the hippocampus (low compression, fast writing) and the neocortex (high compression, slow integration). This is the Complementary Learning Systems (CLS) theory (McClelland, McNaughton & O'Reilly, 1995; updated version see Kumaran, Hassabis & McClelland, 2016).

This led to the inference: Since external memory systems (RAG, Agentic Search, File-based Memory) are excluded, can pure parametric schemes not simulate CLS?

### 3.2 Key Correction by the User

The user pointed out that this analogy is **fundamentally flawed**. The hippocampus should not be analogized to RAG/external memory for the following reasons:

**Essential Differences: Hippocampus vs. RAG**

| Property                      | Hippocampus                                                   | RAG/External Memory                 |
| ----------------------------- | ------------------------------------------------------------- | ----------------------------------- |
| Storage Medium                | **Synaptic weights (Parametric)**                             | Text/Vector DB (Non-parametric)     |
| Writing Method                | **Synaptic plasticity (Learning process)**                    | Copy-paste (No learning)            |
| Storage Duration              | **Temporary** (Days to weeks, then consolidated or forgotten) | Permanent (Unless manually deleted) |
| Relationship with Main System | **Rewrites cortical weights** via sleep consolidation         | **Never rewrites** model weights    |
| Encoding Method               | Sparse activation, compressed representation                  | Raw storage, no compression         |

**Core Difference:** The hippocampus is a **second parametric system** — it is itself a neural network with its own synaptic weights, only its architectural properties (learning rate, encoding method, capacity) are distinctly different from the neocortex. RAG is non-parametric external storage that will never drive updates to the model's weights.

### 3.3 Correct Correspondence of Sleep Consolidation

The sleep consolidation process in CLS theory:

1. During sleep, hippocampal memories (encoded in synaptic weights) are reactivated.
2. This reactivation generates replay signals.
3. Replay signals drive synaptic plasticity (gradient updates) in the neocortex.
4. Knowledge is gradually transferred from the "fast, low-interference, temporary" storage of the hippocampus to the "slow, high-capacity, persistent" storage of the neocortex.
5. Corresponding hippocampal memories gradually decay, releasing capacity for new memories.

**The entire process is purely parametric.** No database, no file system. Hippocampal "memory" is its weight state, and consolidation is using that weight state to generate training signals to update another set of weights.

This is equivalent to a process of **knowledge distillation across parameter regions**.

### 3.4 Correct LLM Correspondence

The correct correspondence of CLS in the LLM context is not "Model + RAG," but:

> **Two sets of parameters (or two regions in the same parameter space) with different learning dynamics, coupled with a consolidation process that transfers knowledge between them.**

- **"Hippocampal" Parameter Region**: High learning rate, sparse activation leading to pattern separation, low interference, limited capacity, temporary storage.
- **"Neocortical" Parameter Region**: Low learning rate, distributed encoding, high capacity, slow integration.
- **"Sleep Consolidation"**: Hippocampal region generates replay signals → Drives gradient updates in the neocortical region → Hippocampus gradually releases capacity.

**References:**

- McClelland, J.L., McNaughton, B.L., & O'Reilly, R.C. (1995). Why there are complementary learning systems in the hippocampus and neocortex. _Psychological Review_, 102(3), 419-457.
- Kumaran, D., Hassabis, D., & McClelland, J.L. (2016). What learning systems do intelligent agents need? Complementary learning systems theory updated. _Trends in Cognitive Sciences_, 20(7), 512-534.
- Early work on generative replay: Shin, H., Lee, J.K., Kim, J., & Kim, J. (2017). Continual learning with deep generative replay. _NeurIPS 2017_.
- Note: While the generative replay of Shin et al. has superficial similarities to the CLS consolidation process, it lacks the key architectural properties of the hippocampus (see next section).

---

## 4. Hippocampus is Inherently Sparse — The Criticality of Activation Sparsity

### 4.1 User's Inquiry

After establishing the correct analogy "Hippocampus = Second Parametric System," the user raised a key follow-up:

> "Is the hippocampus itself dense?"

### 4.2 Fact: The Hippocampus Is One of the Sparsest Regions in the Brain

The hippocampus is not dense. It exhibits extreme sparsity at multiple levels:

**Activation Sparsity:**

- **Dentate Gyrus (DG)**: At any given moment, only about **2-5%** of granule cells are active. The DG has about 1 million granule cells (rat data), but each memory activates only a tiny fraction of them.
- This extreme activation sparsity achieves **pattern separation** — similar patterns in input space are mapped to nearly non-overlapping sparse representations in activation space, thereby minimizing interference between different memories.

**Connectivity Sparsity:**

- **CA3 Recurrent Connections**: Each CA3 neuron forms synaptic connections with only about **2-4%** of other CA3 neurons.
- The entire brain is extremely sparse — each neuron connects to about 7,000 other neurons, representing approximately **0.00001%** of the total neuron count (~86 billion).

**References / Evidentiary Support:**

- Sparse coding in the Dentate Gyrus: Leutgeb, J.K., Leutgeb, S., Moser, M.B., & Moser, E.I. (2007). Pattern separation in the dentate gyrus and CA3 of the hippocampus. _Science_, 315(5814), 961-966.
- Activation rate estimation in the DG: Chawla, M.K., et al. (2005). Sparse, environmentally selective expression of Arc RNA in the upper blade of the rodent fascia dentata by brief spatial experience. _Hippocampus_, 15(5), 579-586.
- CA3 recurrent connectivity: Amaral, D.G., Ishizuka, N., & Claiborne, B. (1990). Neurons, numbers and the hippocampal network. _Progress in Brain Research_, 83, 1-11.

### 4.3 Neocortex Is Also Not Dense

Crucial fact: The neocortex is also not fully connected.

- Local connections (within the same column) are relatively dense.
- Long-range connections (across regions) are extremely sparse.
- The cortex is organized in columns/layers with structural sparsity.
- The activation rate of typical cortical neurons is about **10-20%**, much higher than the DG's 2-5%, but far lower than "dense."

Therefore, **dense networks do not exist in the brain**. Everything is sparse.

### 4.4 Fundamental Revision of the Previous Framework

The previous framework described the hippocampus as "low compression, fast writing," implying it was close to dense. This was **incorrect**.

Revised understanding:

- The hippocampus can write quickly and with low interference **not because it does not compress** or is close to dense.
- Rather, it achieves **pattern separation** through **extreme activation sparsity**.
- Two different memories activate nearly non-overlapping subsets of neurons → writing one does not overwrite the other.
- Sparsity is **not** the opposite of compression; it is **a specific compression strategy** that sacrifices per-neuron utilization in exchange for separability between memories.

### 4.5 Real Difference: Hippocampus vs. Neocortex

The real difference between the two systems is not sparse vs. dense, but the **degree of activation sparsity** and the **representation strategy**:

| Property                     | Hippocampus                                                                                  | Neocortex                                                                  |
| ---------------------------- | -------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------- |
| Connectivity Topology        | Sparse                                                                                       | Also sparse (but different pattern)                                        |
| Activation Sparsity          | **Extremely High** (~2-5%)                                                                   | Lower (~10-20%)                                                            |
| Learning Rate                | Fast (near one-shot, fast Hebbian plasticity)                                                | Slow (requires repeated exposure)                                          |
| Representation Strategy      | **Pattern separation** (Orthogonalization: similar inputs → non-overlapping representations) | **Distributed/overlapping** (Similar inputs → overlapping representations) |
| Interference Characteristics | Low interference between memories                                                            | New learning easily interferes with old memories                           |
| Capacity                     | Limited (constrained by extreme sparsity)                                                    | High (distributed encoding allows more information sharing)                |
| Storage Duration             | Temporary (Days to weeks)                                                                    | Long-term (Months to years)                                                |

**Key Insight:** The difference between the hippocampus and neocortex corresponds to a **continuum of activation sparsity** and the strategic choice between **pattern separation vs. pattern overlap**, rather than a binary division of weight topology into sparse vs. dense.

---

## 5. Fundamental Correction of Research Direction

### 5.1 From Weight Sparsity to Representation Separation

Based on the above discussion, the core correction of the research direction:

**Old Framework (Abandoned):**

> "Structural sparse topology (weight sparsity) is a necessary condition for continual learning."

**Problems:**

1. Trivially true under fixed architectures (since non-sparse = dense, and dense leads to forgetting).
2. Weight sparsity may just be a means to achieve activation sparsity, not the essence itself.

**New Core Question (Emerging from this discussion):**

> "For continual learning, the decisive factor might not be the sparsity of weight topology, but the **degree of pattern separation at the representation level** — whether different tasks/memories activate non-overlapping subsets of neurons."

That is:

- **Weight sparsity** (physical removal of connections) might just be a means to achieve **activation sparsity** (different memories activating non-overlapping subsets).
- The true essence is the **orthogonality/separability of representations**.
- If high activation sparsity and pattern separation can be achieved through other means (training objectives, regularization, architectural design), physical weight sparsity may not be required.

### 5.2 Three Research Directions Worth Exploring

Without pre-supposing solutions (no pre-supposition of sparsity or any specific method), starting from three open questions:

**Direction 1: Pareto Frontier of Compression-Writability**

- Does an architecture or training method exist that breaks the compression-writability tradeoff?
- Or is this a fundamental limit (similar to the rate-distortion bound in information theory)?
- If it is a fundamental limit, what is its form?

**Direction 2: Measurement and Management of Free Capacity**

- Can a "memory management theory for parameter space" be established?
- Similar to OS malloc/free, but operating in continuous parameter space.
- Can we measure the "residual writable capacity" of a trained model?

**Direction 3: Stability of Self-Routing**

- Under what conditions can the attention/routing mechanism of a network remain stable during continual learning?
- Can a routing mechanism be designed that is robust to the writing of new knowledge?
- This connects to the pattern separation function of the hippocampus in CLS: routing itself needs to ensure that different memories are routed to different representation subspaces.

### 5.3 Parametric Implementation of CLS as a Unified Framework

One possible unified research framework emerging from this discussion:

> **Achieving functional differentiation within a single model — two parameter regions with different learning dynamics, implementing knowledge transfer through an endogenous consolidation mechanism.**

This is not "Model + External Memory," but a **functional differentiation within the model architecture**:

1. A "Fast Learning Region" (high activation sparsity → pattern separation → low interference → limited capacity).
2. A "Slow Integration Region" (low activation sparsity → distributed coding → high capacity → requires consolidation).
3. A "Consolidation Mechanism" (fast region generates replay → drives slow region update → fast region releases capacity).

The key advantage of this framework is that it is fully parametric, does not depend on external storage, and has a clear biological correspondence.

---

## 6. Unresolved Key Questions

This round of discussion pauses here, with the following questions marked as starting points for future sessions:

1. **Causal Relationship Between Activation Sparsity and Weight Sparsity**: Is weight sparsity a sufficient/necessary/optimal means for achieving high activation sparsity? Or do methods exist that achieve pattern separation without weight sparsity?

2. **Specific Architecture for Parametric CLS Implementation**: How to differentiate "Hippocampal Region" and "Neocortical Region" within a Transformer architecture? Is it through different layers, different attention heads, or different parameter subsets within the same layer?

3. **Specific Implementation of Consolidation Mechanisms**: What operation does hippocampal replay correspond to in LLMs? Is it generating synthetic data from "fast region" parameters to fine-tune the "slow region"? Or is there a more direct way of weight transfer?

4. **Measurement of Pattern Separation**: How to quantify the degree of pattern separation in a network? Can a theoretical link be established between activation sparsity and forgetting bounds?

---

## 7. Logical Chain of Discussion Evolution (Chronological Record)

To maintain the integrity of the thinking process, the evolution path of the discussion is recorded as follows:

### Step 1: Formulating the Three Essential Problems

- Using "how to write this conversation into model weights" as a concrete example.
- Naturally decomposing into three sub-problems: allocation, capacity, and recall.
- Identifying that these three problems are isomorphic to the basic problems of any memory system (computer memory, biological memory).

### Step 2: Discovering Compression-Writability Tension

- Superposition is the source of LLM power but also an obstacle to continual learning.
- High compression (heavy superposition) → High capacity but unwritable.
- Low compression (light superposition) → Writable but low capacity.
- This is a fundamental contradiction at the information-theoretic level.

### Step 3: Introducing CLS Theory (Initial Incorrect Analogy)

- The biological brain uses a dual-system to solve this contradiction.
- Initially incorrectly analogized the hippocampus to RAG/external memory.
- Consequently thought "excluding external memory = excluding CLS."

### Step 4: User Correction — Hippocampus is a Parametric System

- The hippocampus stores using synaptic weights (parametric), not a database.
- Sleep consolidation is knowledge distillation from hippocampal parameters to neocortical parameters.
- RAG never updates model weights, fundamentally differing from the CLS consolidation process.
- Correct analogy: CLS = Two sets of parameters with different properties + cross-region consolidation mechanism.

### Step 5: User Inquiry — Is the Hippocampus Dense?

- Answer: No. The hippocampus is one of the brightest regions in the brain.
- DG: ~2-5% activation rate (extreme activation sparsity).
- CA3: ~2-4% connection rate (connectivity sparsity).
- The neocortex is also not dense, just with a lower degree of activation sparsity (~10-20%).

### Step 6: Framework Revision

- The difference between hippocampus and neocortex is not sparse vs. dense.
- Rather, it is the **degree** of activation sparsity and representation strategy (pattern separation vs. overlap).
- Weight sparsity might just be a means to achieve activation sparsity, not the essence.
- Research question shifts from "whether weight sparsity is needed" to "what kind of representation separation strategy is most effective."

---

## 8. Summary of Relevant References

### Foundations of Continual Learning

- Kirkpatrick, J., et al. (2017). Overcoming catastrophic forgetting in neural networks. _PNAS_, 114(13), 3521-3526. [EWC]
- Farajtabar, M., Azizan, N., Mott, A., & Li, A. (2020). Orthogonal gradient descent for continual learning. _AISTATS 2020_. [OGD]
- Saha, G., Garg, I., & Roy, K. (2021). Gradient projection memory for continual learning. _ICLR 2021_. [GPM]
- Knoblauch, J., Husain, H., & Diethe, T. (2020). Optimal continual learning has perfect memory and is NP-hard. _ICML 2020_.
- Shin, H., Lee, J.K., Kim, J., & Kim, J. (2017). Continual learning with deep generative replay. _NeurIPS 2017_.

### CLS Theory

- McClelland, J.L., McNaughton, B.L., & O'Reilly, R.C. (1995). Why there are complementary learning systems in the hippocampus and neocortex. _Psychological Review_, 102(3), 419-457.
- Kumaran, D., Hassabis, D., & McClelland, J.L. (2016). What learning systems do intelligent agents need? Complementary learning systems theory updated. _Trends in Cognitive Sciences_, 20(7), 512-534.

### Hippocampal Neuroscience

- Leutgeb, J.K., Leutgeb, S., Moser, M.B., & Moser, E.I. (2007). Pattern separation in the dentate gyrus and CA3 of the hippocampus. _Science_, 315(5814), 961-966.
- Chawla, M.K., et al. (2005). Sparse, environmentally selective expression of Arc RNA in the upper blade of the rodent fascia dentata. _Hippocampus_, 15(5), 579-586.
- Amaral, D.G., Ishizuka, N., & Claiborne, B. (1990). Neurons, numbers and the hippocampal network. _Progress in Brain Research_, 83, 1-11.
- Rolls, E.T. (2013). The mechanisms for pattern completion and pattern separation in the hippocampus. _Frontiers in Systems Neuroscience_, 7, 74.

### Sparsity and Network Capacity

- Frankle, J., & Carlin, M. (2019). The lottery ticket hypothesis: Finding sparse, trainable neural networks. _ICLR 2019_.
- Elhage, N., et al. (2022). Toy models of superposition. _Anthropic research_.

### Evolutionary Architecture

- Rusu, A.A., et al. (2016). Progressive neural networks. _arXiv:1606.04671_.

---

## 9. Document Meta-information

- **Creation Time**: 2026-02-24
- **Discussion Participants**: Alex (Xun Sun) + Claude
- **Preceding Document**: `docs/zh_CN/processing/20260224-0007.md`
- **Status**: Discussion paused at the causal relationship between activation sparsity and weight sparsity, pending future continues.
- **Next Steps**: Continue discussion along the four unresolved questions marked in Section 5.
