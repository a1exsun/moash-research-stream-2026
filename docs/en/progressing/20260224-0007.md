# Continual Learning Roadmap Progress Record (2026-02-24)

## Document Purpose

To record current progress, with a focus on:

1. Value assessment and structural revision of `proposal/research_proposal_draft.md`
2. Strategic judgment on whether "continuous complexity on dense models" is a dead end
3. Discussions on sufficient conditions for human-level long-term continual learning
4. Clarification of latest open-source sparse models (including Kimi/MiniMax)
5. Inventory of fixed-topology physically sparse models and dense->sparse algorithms

This record is intended to support future experimental decision-making and proposal iterations.

---

## 1. Proposal Value Assessment (Interim)

Target File:

- `/Users/alex/Alex/monash/continual-learning/proposal/research_proposal_draft.md`

Interim Conclusions:

1. The proposal has clear value, and the research questions have potential for publication.
2. The original version suffered from claims being stronger than the supporting evidence, primarily manifested in:
   - Absolute assertions (necessary condition, never done before, first time)
   - Extrapolation of idealized formulas to real Transformers
   - Potential capacity confounding in experimental comparisons
   - Insufficient extrapolation to real-world engineering scenarios for continual learning
3. Strengthening the persuasiveness of the proposal will require converging narrative intensity and enhancing the design of control variables.

---

## 2. Executed Proposal Revisions (Landed in This Round)

Corresponding Commit:

- commit: `a90e814`
- message: `refine proposal claims and strengthen experimental controls`

### 2.1 Refinement of Claim Intensity

1. Adjusted "Necessary Condition Hypothesis" to "Strong Constraint / Inductive Bias Hypothesis."
2. Changed "Root Cause Attribution" to "One of the Important Sources."
3. Changed "First Time / Never Done Before" to "Based on current search scope / operational judgment."

### 2.2 Theoretical Expression Refinement

1. Retained `rho^(L-1)` as an idealized approximation, valid only under specific premises.
2. Explicitly noted that global mixing of residuals and attention in real Transformers will deviate from this approximation.
3. Introduced the expression "Empirical estimation of effective interference radius required" to avoid theoretical overfitting.

### 2.3 Experimental Design Refinement

Added fairness and statistical protocols to core experiments:

1. Same-shape control
2. Matched-nonzero parameter control
3. Matched-FLOPs control
4. Multiple random seeds + Confidence Interval (CI) reports
5. Pre-registration of primary metrics (forgetting rate, forward transfer)

### 2.4 Enhancing Extrapolability

Added "Experiment 1B (Minimum Mechanism Baseline)":

1. Dense + LoRA
2. Dense + Replay (fixed small buffer)
3. Sparse-topology + LoRA

Objective: To check if topological effects persist and are additive within common engineering mechanisms.

---

## 3. Strategic Judgment: Is Continuous Complexity on Dense Models a Dead End?

Discussion points:

- Is there a fundamental lack of solution for continuously adding mechanisms to dense models?

Interim judgment:

1. If increasing complexity does not change the "global shared parameter writing physics," the long-term benefit is likely to diminish, leading to an engineering dead end.
2. If increased complexity directly changes the writing mechanism (localized writing, modular isolation, dynamic expansion, offline consolidation), it represents an effective transition path.
3. For the goal of "human-level long-term continual learning," a pure dense + patch system is likely insufficient.

Key Distinction:

- Dead-end complexity: Superimposing regularization/replay/scheduling while maintaining global coupling for writing.
- Non-dead-end complexity: Localization of writing and structural separation, gradually approaching a new paradigm.

---

## 4. Discussion on Sufficiency Under Human-Level Goals

User's objective is clear:

- Human-level, long-term, continual learning

Conclusions from this round:

1. `Sparse/Modular Topology + Selective Writing + Offline Consolidation` is one of the sets of necessary conditions, but likely remains insufficient.
2. Additional components might be required:
   - Online localized learning rules (to reduce dependence on frequent global backpropagation)
   - Long-term capacity management (Growth-Consolidation-Compression cycle)
3. It is not necessarily essential to "overthrow the Transformer operator," but it is likely necessary to overthrow the "pure Transformer + global SGD writing paradigm."

Converged expression:

- Do not presuppose the abandonment of Transformers.
- Presuppose the abandonment of the pure global dense writing paradigm.

---

## 5. Clarification of Open-Source Model Sparsity (New)

### 5.1 On "Whether New Models are Sparse"

Focus:

- Whether new models like Kimi 2.5 / MiniMax possess sparse network characteristics.

Interim Answer:

1. Kimi K2.5: Features MoE (Mixture of Experts) sparse activation.
2. MiniMax M2.5: Configuration reflects MoE expert routing (number of local experts and experts activated per token).

Key Clarification:

- These belong to "computational sparsity (activation sparsity)," which is not equivalent to "fixed-topology physical sparsity (weight physically absent)."

### 5.2 Representative Fixed-Topology Physically Sparse Models

Public representatives identified in this round (primarily from the RedHatAI/NeuralMagic ecosystem):

1. Sparse-Llama-3.1-8B-2of4
2. SparseLlama-3-8B-pruned_50.2of4
3. Llama-2-7b-pruned50-retrained / pruned70-retrained
4. OpenHermes-2.5-Mistral-7B-pruned2.4
5. Nous-Hermes-2-Yi-34B-pruned2.4
6. phi-2-pruned50
7. mpt-7b-gsm8k-pruned60-pt

Methodological Ecosystem Supplement:

- MaskLLM provides 2:4 sparse mask/export workflows and training support.

---

## 6. Inventory of Dense -> Sparse Algorithms (New)

Algorithmic framework provided in this round:

### 6.1 One-shot / Post-training Pruning

1. Magnitude pruning / GMP
2. OBC (Second-order approximation, OBS family)
3. SparseGPT
4. Wanda / Wanda++

### 6.2 Pruning and Then Recovery

1. Movement Pruning
2. L0 Regularization
3. LLM-Pruner
4. LoRAPrune

### 6.3 Hardware-Friendly Structural Sparsity

1. N:M / 2:4 Sparsity (e.g., MaskLLM related routes)
2. Practically easier to achieve real throughput gains.

### 6.4 Subnetwork Discovery Before/During Training

1. SNIP
2. Lottery Ticket / IMP

Engineering Conclusions:

1. Unstructured sparsity does not necessarily lead to inference acceleration (depends on kernels and runtime).
2. 2:4 / N:M is better suited for engineering implementation and deployment verification.
3. A strict distinction must be made between MoE activation sparsity and fixed weight topology sparsity.

---

## 7. Current Consensus (Directly Reusable)

1. The main research line in the near term should not be "continuing to pile patches on dense models," but rather "changing writing physics and scheduling mechanisms."
2. If the goal is human-level long-term continual learning, it is recommended to include "fixed topology sparsity" as a core empirical object, rather than just as an engineering acceleration trick.
3. The proposal has now converged from "strong narrative" to "strong experimental design," establishing a foundation for execution.

---

## 8. Recommendations for Next Steps

1. Initiate the first round of minimum experiments using the revised proposal: a budget-matched comparison of dense vs sparse.
2. Run Experiment 1B (Minimum Mechanism Baseline) in parallel to test extrapolability.
3. Prioritize selecting 1-2 reproducible fixed-topology sparse open-source weights as a starting point (2:4 preferred).

---

## 9. Related Files and Commits

1. Preceding Records:
   - `/Users/alex/Alex/monash/continual-learning/processing/20260222-0353.md`
   - `/Users/alex/Alex/monash/continual-learning/processing/20260223-2152.md`
   - `/Users/alex/Alex/monash/continual-learning/processing/20260223-2331.md`

2. Revised Proposal in this Round:
   - `/Users/alex/Alex/monash/continual-learning/proposal/research_proposal_draft.md`
   - commit: `a90e814`

3. This Document:
   - `/Users/alex/Alex/monash/continual-learning/processing/20260224-0007.md`
