# Comprehensive Record of Continual Learning Direction Discussions (2026-02-23)

## Document Purpose

This file is intended to fully document the progress of the current round of discussions regarding the "Continual Learning Technology Roadmap," covering:

- Clarification of objectives
- First-principles judgments
- Boundaries of Transformers and global gradient descent
- MVP (Minimum Viable Product) engineering strategies
- Identification of root causes of failure
- 2025-2026 research landscape assessment
- Directions for subsequent experimental design

This document is positioned as a project decision log, prioritizing traceability and executability over stylistic brevity.

---

## 0. Starting Point of Discussion

Initial Materials:

- `/Users/alex/Alex/monash/continual-learning/processing/20260222-0353.md`

Initial Assertions (Core from original):

1. Current continual learning follows two routes: "Engineering add-ons (RAG/Memory systems)" and "Direct parameter-level engagement."
2. Parameter-level continual learning is constrained by three missing mechanisms:
   - Lack of time coordinates/state dependency
   - Lack of dynamic topology generation
   - Lack of generative reshaping (sleep consolidation)
3. In the long term, we should step beyond the Backpropagation (BP) paradigm towards local learning rules and predictive coding.

---

## 1. First-Principles Review (Initial Evaluation of the Original)

### 1.1 First-Principles Decomposition Framework

The analytical framework adopted in this round:

1. Objective Function Level:
   - Continual learning = Simultaneously minimizing losses for new and old tasks within non-stationary data streams.

2. Constraint Level:
   - Finite parameter budget
   - Finite training/updating compute
   - Constraints on online service latency

3. Root Cause Level:
   - Catastrophic forgetting fundamentally stems from "gradient conflict/representation interference on shared parameters," rather than the absence of any single technique.

4. Falsifiability Level:
   - Any mechanical assertion should be translated into comparable metric improvements under a unified budget.

### 1.2 Judgments on the Original Three Puzzles

Conclusions (Interim):

1. Puzzle 1 (State dependency) is in the correct direction, but the "complete absence" phrasing is somewhat absolute.
2. Puzzle 2 (Dynamic topology) is in the correct direction, but the "inevitable collapse upon capacity saturation" phrasing is somewhat strong.
3. Puzzle 3 (Generative reshaping) is a valuable direction, but "generative replay is inherently superior to exemplar replay" is not an a priori truth.

### 1.3 Correction on "Stepping Beyond the BP Myth"

Interim Judgment:

- BP is not the uniquely correct path, but it has not been proven that it "must be abandoned."
- Reality bottlenecks are closer to:
  `Shared Parameters + Global Synchronous Writing + Single Time-scale Updates + Non-selective Writing`
- Emphasis should first fall on "changing the writing mechanism" rather than "replacing base operators."

---

## 2. Further Clarification of Objectives (User's Ultimate Goal)

User's explicit goals:

- Build a "small prototype system" to explore the correct direction.
- Do not strive for SOTA (State of the Art) general capabilities.
- Strive for "stronger learning capabilities compared to LLMs of the same class."
- Focus on the mechanism for "repeatedly fine-tuning context into weights while significantly alleviating forgetting."

Alignment conclusions on this objective:

1. This is a reasonable and executable research goal.
2. However, "repeatedly fine-tuning into weights" does not automatically equal "brain-like sleep consolidation."
3. To approach a "sleep mechanism," it should at least satisfy:
   - Selective writing (writing only high-value information)
   - Dual time-scales (fast online writing, slow offline integration)
   - Anti-interference constraints (parameter importance/modular isolation)
   - Replay recombination (beyond mechanical re-reading)

---

## 3. Core Controversy 1: Can Transformers Support Continual Learning?

Question:

- Is it possible for the existing Transformer architecture to achieve human-level continual learning?
- Is global gradient descent a fundamental obstacle?
- Must Transformers be replaced?

Conclusions from this round:

1. Within the Transformer framework, it is feasible to achieve "continual learning significantly superior to LLMs of the same class."
2. "Native Transformer + full-parameter global SGD" is unlikely to reach human-level continual learning.
3. It is not necessary to immediately replace Transformers; a more realistic approach is to use them as a "slow learning skeleton" while layering on new mechanisms.

Key Clarification:

- The obstacle is not the attention operator itself.
- The obstacle is the writing and scheduling mechanisms for globally shared parameters.
- In theory, it is not absolutely impossible; under realistic resource constraints, it remains a pragmatic root bottleneck.

---

## 4. Core Controversy 2: Is a New Pre-trained Architecture Essential?

Question:

- Should we first modify open-source models or first design sparse architectures and pre-train new models?

Conclusions from this round:

1. During the MVP phase, priority should be given to "modifying existing open-source models."
2. Starting with "new sparse architecture + pre-training" is not recommended.
3. Only if the modification route repeatedly hits a wall under clear metrics should we proceed to the stage of pre-training a new architecture.

Judgment Threshold (When to upgrade the base):

1. Forgetting rate cannot be reduced to less than half of the baseline (at the same budget).
2. Irreconcilable hard conflict between the speed of writing new knowledge and the retention of old knowledge occurs.
3. External modules result in non-viable operational costs for inference/storage.

---

## 5. Core Controversy 3: Do Dense Models Inherently Lack Continual Learning Potential?

Question:

- Do dense models naturally lack the potential for continual learning?

Conclusions from this round:

1. This concern has an important realistic basis, but the conclusion is overly strong.
2. The problem with dense models is not that they "necessarily fail," but that "full-parameter continuous writing easily causes interference."
3. The true bottleneck is the writing mechanism, not the dense identity itself.

More accurate bottleneck expression:

`Shared Parameters + Non-selective Writing + Single Time-scale Updates`

Hence, the priority strategy:

- First implement "sparse writing mechanisms" (LoRA/Adapter/Routing/Frozen Backbone) on a dense backbone.
- First verify if there is indeed a structural upper limit before deciding whether to change the base.

---

## 6. Consensus on MVP Positioning

Conclusion:

The MVP is not "building a new large model," but "developing a continual learning fine-tuning system."

Essential MVP Components (Minimum closed-loop):

1. Frozen backbone (dense backbone)
2. Writable incremental layers (LoRA/Adapter bank)
3. Writing gating (e.g., triggered by surprise/uncertainty)
4. Offline consolidation (sleep replay + merging/distillation)
5. Continual learning evaluation (learning speed, retention rate, BWT, unit update cost)

Compatible explanation of this positioning with the Bitter Lesson:

- Keep the base architecture simple and scalable.
- Limit complexity to "necessary learning scheduling mechanisms."
- Start with the simplest mechanism combination, then increase complexity according to evidence.

---

## 7. Why This Path Often Fails: Root Cause Checklist

Six fundamental bottlenecks summarized in this round:

1. Objective function misalignment: Optimizing only current loss, not directly optimizing long-term memory fidelity.
2. Stability-plasticity hard conflict: Inherent tension between writing fast and retaining knowledge.
3. Shared representation conflict: Contamination of new and old knowledge within the same representation subspace.
4. Agnosticism of writing decisions: Difficulty in judging "what is worth writing" in real streams.
5. Fragility of replay mechanisms: High cost of exemplar replay; drift-prone generative replay.
6. Evaluation distortion: Large gap between benchmarks and open-world distributions.

Key Conclusion:

- We cannot just build a "smarter fine-tuner."
- We must simultaneously cover: writing selection + anti-conflict + consolidation scheduling + realistic evaluation.

---

## 8. 2025-2026 Research Landscape Assessment (Interim)

Discussion Question:

- Is the continual learning fine-tuning system a "correct direction awaiting development" or a "wrong direction and false prosperity" in 2025-2026?

Interim Judgment:

- Closer to: `Correct direction, but still in the early-to-mid stage, and not yet fundamentally solved.`

### 8.1 Signals Supporting "Correct Direction" (Mentioned in session)

1. TiC-LM (ACL 2025)
   - Link: [https://aclanthology.org/2025.acl-long.1551/](https://aclanthology.org/2025.acl-long.1551/)
   - Clue: In temporal continuous pre-training, the combination of meta-schedule and replay approaches re-training performance while reducing compute.

2. Towards Effective and Efficient Continual Pre-training (ACL 2025)
   - Link: [https://aclanthology.org/2025.acl-long.289/](https://aclanthology.org/2025.acl-long.289/)
   - Clue: Emphasizes feasible paths for balancing the ability to learn new things and the ability to retain knowledge.

3. Multi-Stage LLM Fine-Tuning (NAACL Findings 2025)
   - Link: [https://aclanthology.org/2025.findings-naacl.303/](https://aclanthology.org/2025.findings-naacl.303/)
   - Clue: Significant gains observed in multi-stage continuous fine-tuning, beyond single-point improvements.

### 8.2 Signals Supporting "Root Problem Unsolved" (Mentioned in session)

1. Replay ratios are highly sensitive across different domains/stages (policies are not unified).
2. Large-scale post-training forgetting patterns are complex; merging strategies lack stability.
   - Clue Link: [https://arxiv.org/abs/2510.17776](https://arxiv.org/abs/2510.17776)

3. Multimodal continual learning evaluations show results are strongly dependent on model base capability, task order, and training paradigm.
   - Clue Link: [https://arxiv.org/abs/2508.08275](https://arxiv.org/abs/2508.08275)

Note:

- The above papers were used in this round to "position the research landscape," rather than for piece-by-piece reproduction of empirical conclusions.
- Subsequent entry into the experimental stage requires systematic verification of methods/settings for each paper.

---

## 9. Boundary Consensus on Biological Analogies

Points of Consensus:

1. "Repeatedly writing context into weights" can serve as an engineering approximation toward sleep consolidation.
2. However, it only resembles a sleep mechanism when features like "selectivity, slow consolidation, anti-interference, and recombinative replay" are met.
3. Arbitrary continuous fine-tuning should not be equated to "brain-like continual learning."

---

## 10. Project Phase Recommendations (Starting from current state)

### 10.1 Phase A (Immediate Execution)

Objective:

- Produce measurable evidence of continual learning gains on models of the same class.

Actions:

1. Select a fixed open-source backbone (small model for high-frequency iteration).
2. Establish a unified streaming evaluation protocol.
3. Build baseline: Full-parameter or standard LoRA continuous fine-tuning.
4. Implement minimum mechanism combination: Gated writing + replay + periodic consolidation.

### 10.2 Phase B (If A holds)

Objective:

- Improve anti-interference and scalability.

Actions:

1. Increase modular bank and routing.
2. Add controlled expansion and distillation compression.
3. Compare cost-benefit curves (performance vs. latency/VRAM/training duration).

### 10.3 Phase C (If A/B hit walls)

Objective:

- Verify if the upper limit of dense model modification has been reached.

Actions:

1. Conduct small-scale sparse new architecture experiments.
2. Explore paradigms such as local learning rules/predictive coding.

---

## 11. Decision Statements for Immediate Reuse

The following expressions can serve as core statements for subsequent plans, proposals, or READMEs:

1. This project prioritizes exploring `Context-to-Weight Consolidation (C2W)`:
   Continually consolidating high-value context into weights under a fixed budget while significantly suppressing forgetting.

2. We do not equate "continuous fine-tuning" directly with continual learning;
   It is only considered effective when both "writing speed" and "memory retention" are simultaneously improved under the same budget.

3. MVP focuses on the learning system rather than pre-training a new base:
   `Frozen Backbone + Selective Write + Sleep Consolidation + CL Evaluation`.

---

## 12. Output Files and Commit Records for this Round

1. Historical Analysis Input:
   - `/Users/alex/Alex/monash/continual-learning/processing/20260222-0353.md`

2. Generated prior to this round:
   - `/Users/alex/Alex/monash/continual-learning/processing/20260223-2152.md`

3. This Document:
   - `/Users/alex/Alex/monash/continual-learning/processing/20260223-2331.md`
