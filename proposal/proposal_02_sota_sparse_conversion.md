# Proposal 02: 基于开源 SOTA 模型的结构化稀疏化对比

## 目标

验证：将同一开源 SOTA dense LLM 转换为结构化稀疏权重后，是否在持续微调中直接降低灾难性遗忘。

这是路线判断实验，不是最终架构定论实验。

---

## 研究问题

1. Dense -> Structured Sparse 转换是否带来持续学习收益？
2. 收益是否超过稀疏化带来的初始性能损失？
3. 不同结构化稀疏模式（2:4、4:8、block）差异有多大？

---

## 假设

### H1
在等预算下，结构化稀疏版本的平均遗忘率低于 dense 对照。

### H2
在等 FLOPs 下，稀疏版本的“保持率/学习速度”效率更高。

### H3
存在可用的稀疏度区间，使长期收益超过初始损失（出现交叉点）。

---

## 实验设计

## 1. 模型检查点

以同一开源 SOTA 基座构建：

- `D0`：原始 dense
- `S0`：稀疏化后无恢复
- `S1`：S0 + 固定预算恢复训练
- `D1`：D0 + 与 S1 等预算额外训练

主比较：`D1 vs S1`

## 2. 稀疏化方法

至少两条路线：

1. 2:4 半结构化稀疏
2. block-sparse 或 4:8

算法建议：
- MaskLLM 路线 或 SparseGPT/Wanda++ + 恢复训练

要求：
- 持续微调阶段保持固定稀疏拓扑，不可回 dense。

## 3. 持续微调协议

- 固定域序列：代码 -> 数学 -> 医疗 -> 法律 -> 对话
- 固定训练预算：token/step/optimizer
- 主实验不叠加 replay/EWC（先隔离稀疏化价值）

## 4. 公平性对照

1. same-shape
2. matched-nonzero
3. matched-FLOPs

统计：
- >=3 seeds
- 95% CI

---

## 指标

主指标：
1. Forgetting
2. BWT
3. 平均保持率

次指标：
1. FWT
2. 训练吞吐
3. 推理吞吐/延迟（可选）

---

## 决策门槛（Go/No-Go）

Go（继续稀疏路线）：
- 在 matched-FLOPs 下，S1 的遗忘率相对 D1 下降 >= 20%，且学习速度不下降超过 10%。

No-Go（转向其他路线）：
- 多设置下 S1 无统计优势，或优势只来自额外恢复训练而非拓扑。

---

## 风险与应对

### 风险1：稀疏化初始损失过大
应对：分层测试稀疏度，先温和再激进。

### 风险2：硬件栈对稀疏不友好
应对：研究结论优先看持续学习指标，吞吐单独报告。

### 风险3：结果高度依赖单一模型
应对：至少在 2 个开源基座上复现关键结论。

