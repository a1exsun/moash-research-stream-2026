# 从 Dense 到结构化稀疏：持续微调抗遗忘价值的实证研究

## 研究提议

**研究者：** Alex  
**所属机构：** Monash University, Master of AI  
**日期：** 2026年2月

---

## 一、研究动机

当前持续学习研究存在一个反复出现的问题：
- 在 dense 基座上叠加越来越多机制（正则、回放、路由、外挂记忆）后，系统复杂度和成本持续上升；
- 但“灾难性遗忘”并未被稳定、可扩展地解决。

本提案聚焦一个更直接且可执行的问题：

> 如果把同一个开源高性能 dense LLM 转换为原生结构化稀疏权重（fixed structured sparse topology），再进行相同协议的持续微调，遗忘是否显著下降？

该问题的价值在于：
1. 它不依赖全新架构预训练，能快速给出方向性证据；
2. 它直接检验“结构化稀疏”对持续学习是否有独立贡献；
3. 它为后续是否投入“新稀疏架构预训练”提供决策门槛。

---

## 二、核心研究问题

### 2.1 主问题

在相同基座模型、相同持续微调数据流、相同训练预算下：

- Dense 模型与其结构化稀疏化版本相比，哪一个具有更低遗忘率和更好的长期保持能力？

### 2.2 子问题

1. 稀疏化收益是否来自“拓扑本身”，而非“训练预算差异”或“恢复训练额外收益”？
2. 不同结构化稀疏模式（如 2:4、4:8、块稀疏）在持续学习上的效果是否显著不同？
3. 稀疏化是否引入“初始性能损失 vs 长期保持收益”的交叉曲线？

---

## 三、核心假说

### H1（直接价值假说）

在等预算对照下，结构化稀疏模型在序列化持续微调中的平均遗忘率低于 dense 对照。

### H2（效率假说）

在等训练 FLOPs 或等 wall-clock 预算下，结构化稀疏模型的“保持率 / 学习速度”联合效率优于 dense 对照。

### H3（结构敏感假说）

在相同稀疏度下，不同结构化拓扑（2:4 vs 4:8 vs block）对遗忘曲线有统计显著差异。

### 可证伪条件

如果在等参数/等 FLOPs/等 token 的严格对照下，dense 在关键指标上持续达到或超过稀疏模型，则“稀疏化对持续学习有直接价值”的主张不成立。

---

## 四、实验总体设计

## 4.1 模型与检查点

选用一个开源高性能基础模型（建议 7B-8B 级，便于迭代），构建以下检查点：

1. `D0`：原始 dense 基座（参考点）
2. `S0`：由 `D0` 经结构化稀疏算法直接转换（无恢复训练）
3. `S1`：`S0` 经过固定预算恢复训练（recovery）
4. `D1`：`D0` 经过与 `S1` 完全等预算的额外训练（用于去除“多训练导致提升”的混淆）

核心比较：
- `D1 vs S1`（主结论）
- `D0 vs S0`（稀疏转换即时损失）

## 4.2 稀疏化方案

至少测试两种结构化稀疏配置：

1. 半结构化：`2:4`（硬件友好）
2. 更激进或替代结构：`4:8` 或 block-sparse（固定块大小）

可选算法池（按工程成熟度选1-2个，不贪多）：
- MaskLLM 路线（学习结构化 mask）
- SparseGPT/Wanda++ 路线（后训练剪枝 + 恢复）

原则：
- mask 固定后即为“物理拓扑约束”，持续微调阶段不允许回 dense。

## 4.3 持续微调协议

采用域序列持续学习（示例）：
- 代码 -> 数学 -> 医疗 -> 法律 -> 通用对话

训练规则：
1. 所有模型使用同一 token 化与同一数据序列。
2. 每阶段固定相同步数、batch、优化器设置。
3. 主实验不使用 replay/EWC/参数冻结（先隔离拓扑效应）。
4. 扩展实验可加“最小机制基线”（LoRA 或小缓存 replay）测试外推性。

## 4.4 公平性约束（关键）

同时报告三组对照，避免错误归因：

1. `same-shape`：同层数同隐藏维（直观对照）
2. `matched-nonzero`：dense 对照参数量匹配稀疏非零参数量
3. `matched-FLOPs`：训练计算预算匹配

统计协议：
- 每组至少 3 个随机种子
- 报告均值、标准差与 95% 置信区间
- 预注册主指标，避免事后挑指标

---

## 五、评估指标

### 5.1 持续学习核心指标

1. 平均遗忘率（Forgetting）
2. BWT（Backward Transfer）
3. FWT（Forward Transfer）
4. 各域保持率（Retention）
5. 交叉点轮次（若存在）

### 5.2 工程效率指标

1. 每单位训练 FLOPs 的保持率提升
2. 持续微调吞吐与显存占用
3. 推理延迟与吞吐（dense vs sparse 推理栈）

### 5.3 稳健性指标

1. 域序列打乱后的方差
2. 不同随机种子的结果稳定性
3. 不同稀疏结构之间的一致性

---

## 六、预期结果与解释框架

可能结果 A：`S1` 显著优于 `D1`
- 解释：结构化稀疏拓扑对持续学习具有直接正贡献。
- 含义：值得进一步投入稀疏原生架构与长期训练。

可能结果 B：`S1` 与 `D1` 接近
- 解释：稀疏化不是主导因素，或收益被恢复训练/任务设置抵消。
- 含义：应转向“写入机制与巩固调度”的联合改进。

可能结果 C：`S1` 明显弱于 `D1`
- 解释：当前稀疏模式破坏了表示能力，或稀疏恢复策略不足。
- 含义：先优化稀疏化质量与恢复策略，再谈持续学习收益。

---

## 七、主要风险与应对

### 风险1：稀疏化后初始损失过大

应对：
1. 先在较温和稀疏度做扫描（如 20%-50%）；
2. 严格配置恢复训练预算；
3. 使用 `D1` 对照去除“多训练带来提升”的混淆。

### 风险2：推理栈无法体现稀疏速度收益

应对：
1. 研究结论优先看持续学习指标；
2. 工程收益单独报告“理论稀疏收益”和“实际部署收益”的差距；
3. 优先使用硬件友好的 N:M 稀疏结构。

### 风险3：域数据构造引入偏差

应对：
1. 统一数据清洗与采样标准；
2. 做域顺序扰动实验；
3. 明确区分事实记忆任务与能力迁移任务。

---

## 八、最小可执行路线图（MVP）

### 阶段1：准备（1-2周）

1. 选定基础模型与稀疏算法
2. 生成 `D0/S0/S1/D1` 四个检查点
3. 搭建持续微调评测脚本

### 阶段2：核心实验（3-5周）

1. 执行主对照 `D1 vs S1`
2. 完成稀疏结构对比（至少两种）
3. 输出遗忘曲线与统计报告

### 阶段3：扩展验证（2-3周）

1. 加入最小机制基线（LoRA 或 replay）
2. 评估结果外推性
3. 汇总结论与下一阶段投入建议

---

## 九、预期贡献

1. 给出“dense->结构化稀疏转换”在持续微调场景中的首轮系统证据。
2. 将“稀疏化是否值得作为持续学习主线”从观点问题转化为可证伪实验问题。
3. 为后续路线决策提供明确门槛：
- 若有效，推进稀疏原生持续学习架构；
- 若无效，减少在纯拓扑方向的投入，转向写入规则与巩固机制。

---

## 十、结论

本提案不是直接回答“最终架构是什么”，而是回答一个更基础且必须先回答的问题：

> 在现实可执行条件下，将开源 SOTA dense LLM 转换为结构化稀疏权重，是否能直接提升持续学习抗遗忘能力？

只要这个问题被严格回答，后续路线选择（继续稀疏化、联合机制、还是架构重构）就会从猜测变成证据驱动。

